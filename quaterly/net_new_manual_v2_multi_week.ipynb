{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefee7f4-be9e-4b47-b4b7-775cce6f8290",
   "metadata": {},
   "source": [
    "# Manual Net New File Creation - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c98dfc-2ebe-4d2c-a273-0ab755ceb9b8",
   "metadata": {},
   "source": [
    "A net new file created using the exclusion lists available for the quarter that data is already frozen for must be made manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999daf0a-84aa-44a7-be28-f7c8e4d24f04",
   "metadata": {},
   "source": [
    "This process is to be done until the exponent date naturally crosses over to the next quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a327e0-7c92-456f-a367-867f29e7e1c5",
   "metadata": {},
   "source": [
    "The reason for creating this file is because processes like BIT dont adhere to the xponent date and follows the calendar instead,\n",
    "thus needing files relevant for the next quarter for processing much before the xponent date reaches the next quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbcff18e-a2c9-43fb-83be-3d3c21963233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries -\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo\n",
    "import gc\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7e85a4-725e-49d6-84f1-44a07fd50352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Weeks for which you have to prepare the file :\n",
    "# current and prior\n",
    "pair_weeks = {\n",
    "    '20240802' : '20240726',\n",
    "    '20240809' : '20240802',\n",
    "    '20240816' : '20240809',\n",
    "    '20240823' : '20240816',\n",
    "    '20240830' : '20240823',\n",
    "    '20240906' : '20240830',\n",
    "    '20240913' : '20240906'\n",
    "} # NOTE THIS IS OPTIONAL , FOR BULK RUN ONLY - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2a4cbe-b842-4f9a-9dc8-f74341831a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES -\n",
    "QTR_NET = 'Q4'                          # Next QUARTER\n",
    "QTR = '2024Q4'                          # Next QUARTER\n",
    "#CUR_WK = '20240802'                     # Frozen Xponent Data week+7\n",
    "#PRE_WK = '20240726'                     # Frozen Xponent Data week\n",
    "#WEEK1 = 0                               # PUT 1 if its the first week (the week after data frozne week) , else put 0\n",
    "CUT_OFF_net = pd.Timestamp('2025-1-1') #*THE 1ST DAY OF NEXT QUARTER, BASED QTR WE ARE GENERATING FILE FOR;\n",
    "\n",
    "bucket = 'vortex-staging-a65ced90'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe07248-2a26-4739-9a9b-4dc3e4f9632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies  - \n",
    "ZIP_TO_TERR = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/geography/zip_to_terr.parquet')\n",
    "ZIP_TO_TERR.rename(columns={'Territory':'Territory_IW1','Territory_Name':'Territory_Name_IW1'},inplace=True) #DOUBT not sure why we do this\n",
    "SPEC_INCL_LIN = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/reference/qtrspec_SPEC_INCL_LIN.parquet')\n",
    "#Extra\n",
    "SPEC_INCL_LIN.rename(columns={'SPECIALTY_CD':'SpecialtyCode'},inplace=True)\n",
    "SPEC_INCL_LIN.set_index('SpecialtyCode', inplace=True)\n",
    "# applying exclusions\n",
    "JAMI_INCLEXCL = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/reference/{QTR}/jami_inclexcl.parquet')\n",
    "jami_exclusions = pd.DataFrame()\n",
    "jami_exclusions = JAMI_INCLEXCL[JAMI_INCLEXCL['TYPE'].isin(['Legal Removals', 'Unknown Address'])]\n",
    "jami_exclusions = jami_exclusions[['IID']].copy()\n",
    "jami_exclusions.drop_duplicates(subset='IID', inplace=True)\n",
    "jami_exclusions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Importing IIDs from frozen IC universe , and deleting any occurances of those in net new as an added QC\n",
    "CUSTOMER_MASTER_IC_LIN = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/reference/CUSTOMER_MASTER_IC_LIN.parquet')\n",
    "CUSTOMER_MASTER_IC_LIN = pd.DataFrame(CUSTOMER_MASTER_IC_LIN['IID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6181283-e2ae-4e32-ae2d-8c3ed307750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 1300 Records for  Week :  20240802\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 2901 Records for  Week :  20240809\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 4404 Records for  Week :  20240816\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 5765 Records for  Week :  20240823\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 7436 Records for  Week :  20240830\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 9670 Records for  Week :  20240906\n",
      "Records Dropped from ic uni:  0\n",
      "Records Dropped {First Instance Check}:  0\n",
      "Exported for 13080 Records for  Week :  20240913\n"
     ]
    }
   ],
   "source": [
    "flag_list = [1 if i == 0 else 0 for i in range(len(pair_weeks))]\n",
    "for (CUR_WK, PRE_WK), WEEK1 in zip(pair_weeks.items(), flag_list):\n",
    "\n",
    "    # Current Week Full Master Profile -\n",
    "    CUR_PROFILE = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{CUR_WK}/reference/MASTER_PROFILE.parquet')\n",
    "    # Previous Week Master Profile -\n",
    "    PRE_PROFILE = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{PRE_WK}/reference/MASTER_PROFILE.parquet')\n",
    "\n",
    "    # Perform the merge using pd.merge\n",
    "    NET_NEW = pd.merge(CUR_PROFILE, PRE_PROFILE['IID'],on='IID', how='left', indicator=True)\n",
    "    NET_NEW = NET_NEW[NET_NEW['_merge'] == 'left_only']\n",
    "    NET_NEW = NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "    #adding week timestamp \n",
    "    NET_NEW['SOURCE'] = CUR_WK  #using cur_wk_xpn_date but in code  -\"&CUR_WK.\" is used.\n",
    "\n",
    "    # APPENDING NET NEW TO CREATE ROLLING FILE - \n",
    "    NET_NEW_ACCU = pd.DataFrame()\n",
    "    net_new_varname = \"NET_NEW_{}\".format(QTR_NET)\n",
    "    if WEEK1 == 1:\n",
    "        NET_NEW_ACCU = NET_NEW.copy()\n",
    "    elif WEEK1 == 0:\n",
    "        globals()[net_new_varname] = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{PRE_WK}/reference/{net_new_varname}.parquet')\n",
    "        columns_to_drop = ['Territory_IW1', 'Territory_Name_IW1','Region', \n",
    "                       'Region_Name', 'Area','Area_Name','SPECIALTY_DESCRIPTION',\n",
    "                       'SPEC_INCL_LIN','IC_INCL_LIN'] \n",
    "        globals()[net_new_varname] = globals()[net_new_varname].drop(columns=columns_to_drop)\n",
    "        NET_NEW_ACCU = pd.concat([NET_NEW, globals()[net_new_varname]], axis=0)\n",
    "    \n",
    "    #Assigning Terr information from Zip to Terr\n",
    "    \n",
    "    NET_NEW_FN = pd.merge(NET_NEW_ACCU,ZIP_TO_TERR,left_on='ZIP',right_on='Zip',how='left') #getting terr name, region name, area name from zip to terr\n",
    "    whitespace_values = {\n",
    "        'Territory_IW1': \"1111-99999-99\",\n",
    "        'Territory_Name_IW1': \"\",\n",
    "        'Region': \"1111-999\",\n",
    "        'Region_Name': \"\",\n",
    "        'Area': \"1111-9\",\n",
    "        'Area_Name': \"\"\n",
    "    }\n",
    "    # Fill whitespace_values with the default values\n",
    "    NET_NEW_FN.fillna(whitespace_values, inplace=True)\n",
    "\n",
    "    #getting speciality flags from spec incl lin quaterly file -[NOTE : BUG FIX SUMMARY - sas did not perform a left join, revised code to perfrom mapping strategy] \n",
    "    NET_NEW_FN['SPECIALTY_DESCRIPTION'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPECIALTY_DESCRIPTION'])\n",
    "    NET_NEW_FN['SPEC_INCL_LIN'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPEC_INCL'])\n",
    "    NET_NEW_FN.fillna({'SPECIALTY_DESCRIPTION':''},inplace=True)\n",
    "\n",
    "    #adding new column SPEC_INCL_LIN (this is based on match code, so ill have to wait for it to come.- NOTE  Condition is only applied on failed left matches\n",
    "    #NET_NEW_FN['SPEC_INCL_LIN'] = ['N' if x == '01' else 'Y' for x in NET_NEW_FN['MatchCode']] - This is wrong\n",
    "    NET_NEW_FN['SPEC_INCL_LIN'] = np.where(NET_NEW_FN['SPEC_INCL_LIN'].isna(),\n",
    "                                            np.where(NET_NEW_FN['MatchCode'] == '01', 'N', 'Y'),\n",
    "                                            NET_NEW_FN['SPEC_INCL_LIN'])\n",
    "    #dropping extra columns (different name , same data)\n",
    "    NET_NEW_FN.drop(['Zip'], axis=1, inplace=True)\n",
    "\n",
    "    #Filtering for any legal removal and unknown address HCPs\n",
    "    ref_week_NET_NEW = pd.merge(NET_NEW_FN,jami_exclusions,on = 'IID',how='left',indicator=True)\n",
    "    ref_week_NET_NEW = ref_week_NET_NEW[ref_week_NET_NEW['_merge'] == 'left_only']\n",
    "    ref_week_NET_NEW = ref_week_NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "    #IC flag creation\n",
    "    ref_week_NET_NEW['IC_INCL_LIN'] = 0  # Initialize the column with 0\n",
    "    \n",
    "    condition = (\n",
    "        (ref_week_NET_NEW['CustomerEffectiveStartDate'] < CUT_OFF_net) &\n",
    "        (ref_week_NET_NEW['SPEC_INCL_LIN'] == \"Y\") &\n",
    "        (ref_week_NET_NEW['CustomerStatusCode'] == \"Active\") &\n",
    "        (ref_week_NET_NEW['MatchCode'] != '01')\n",
    "    )\n",
    "    \n",
    "    ref_week_NET_NEW.loc[condition, 'IC_INCL_LIN'] = 1\n",
    "\n",
    "    qc_bef = len(ref_week_NET_NEW)\n",
    "    ref_week_NET_NEW = ref_week_NET_NEW[~ref_week_NET_NEW['IID'].isin(CUSTOMER_MASTER_IC_LIN['IID'])]\n",
    "    qc_af = len(ref_week_NET_NEW)\n",
    "    print(\"Records Dropped from ic uni: \",qc_bef-qc_af)\n",
    "\n",
    "    #Only Keep the first Instance of IID Source.\n",
    "    qc_bef = len(ref_week_NET_NEW)\n",
    "    ref_week_NET_NEW.sort_values(by=['IID', 'SOURCE'], inplace=True)\n",
    "    ref_week_NET_NEW.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "    qc_af = len(ref_week_NET_NEW)\n",
    "    print(\"Records Dropped {First Instance Check}: \",qc_bef-qc_af)\n",
    "    \n",
    "    ref_week_NET_NEW = ref_week_NET_NEW.sort_values(['IID'])\n",
    "    ref_week_NET_NEW.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # EXPORT - \n",
    "    ref_week_NET_NEW.to_parquet(f's3://vortex-staging-a65ced90/PYADM/weekly/archive/{CUR_WK}/reference/{net_new_varname}.parquet')\n",
    "    print(f'Exported for {ref_week_NET_NEW.shape[0]} Records for  Week : ',CUR_WK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ae63a-65d8-487f-b65a-380adc82f161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
