{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0380d1-bed3-4626-a412-6c0cd692e4e3",
   "metadata": {},
   "source": [
    "### Manual Net New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a99e7d9-c058-4baf-a92e-d6239f0586d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo\n",
    "import gc\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa4c35-e7cb-4c03-882e-a4ff89c15c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "QTR_NET = 'Q3'\n",
    "QTR = '2024Q3'\n",
    "\n",
    "# Frozen Xponent Data week+7\n",
    "CUR_WK = '20240802'\n",
    "# Frozen Xponent Data week\n",
    "PRE_WK = '20240726'\n",
    "# PUT 1 if its the first week (the week after data frozne week) , else put 0\n",
    "WEEK1 = 0\n",
    "bucket = 'vortex-staging-a65ced90'\n",
    "\n",
    "CUT_OFF_net = pd.Timestamp('2024-10-1')\n",
    "#*THE 1ST DAY OF NEXT QUARTER, BASED QTR WE ARE GENERATING FILE FOR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45c0552-be30-48b9-aa41-68f8e25e1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Week Full Master Profile -\n",
    "CUR_PROFILE = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{CUR_WK}/reference/MASTER_PROFILE.parquet')\n",
    "# Previous Week Master Profile -\n",
    "PRE_PROFILE = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{PRE_WK}/reference/MASTER_PROFILE.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "592e1c85-9dd4-44f0-9002-96d53edc5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge using pd.merge\n",
    "NET_NEW = pd.merge(CUR_PROFILE, PRE_PROFILE['IID'],on='IID', how='left', indicator=True)\n",
    "NET_NEW = NET_NEW[NET_NEW['_merge'] == 'left_only']\n",
    "NET_NEW = NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "#adding week timestamp \n",
    "NET_NEW['SOURCE'] = CUR_WK  #using cur_wk_xpn_date but in code  -\"&CUR_WK.\" is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec01428-8ed4-4ab3-912e-4d7c08a5b73f",
   "metadata": {},
   "source": [
    "### Appending net new to create rolling file\n",
    "- the week after frozen week , the net new file is reset\n",
    "- otherwise, net new files gets accumilated by using previous week net new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65f84859-7987-4a38-ad35-0dbdd09609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_NEW_ACCU = pd.DataFrame()\n",
    "\n",
    "net_new_varname = \"NET_NEW_{}\".format(QTR_NET)\n",
    "\n",
    "if WEEK1 == 1:\n",
    "    NET_NEW_ACCU = NET_NEW.copy()\n",
    "elif WEEK1 == 0:\n",
    "    globals()[net_new_varname] = pd.read_parquet(f's3://{bucket}/PYADM/weekly/archive/{PRE_WK}/reference/{net_new_varname}.parquet')\n",
    "    columns_to_drop = ['Territory_IW1', 'Territory_Name_IW1','Region', \n",
    "                   'Region_Name', 'Area','Area_Name','SPECIALTY_DESCRIPTION',\n",
    "                   'SPEC_INCL_LIN','IC_INCL_LIN'] \n",
    "    globals()[net_new_varname] = globals()[net_new_varname].drop(columns=columns_to_drop)\n",
    "    NET_NEW_ACCU = pd.concat([NET_NEW, globals()[net_new_varname]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dac27e-c4d4-454d-adb7-f28d03530ae5",
   "metadata": {},
   "source": [
    "#### Assigning Terr information from Zip to Terr\n",
    "- Addtionally, using the quarterly Speciality Inclusion Exclusion list to get inclusion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1178e5b7-63a3-4189-b3d6-5bf4ab8bf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_TO_TERR = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/geography/zip_to_terr.parquet')\n",
    "ZIP_TO_TERR.rename(columns={'Territory':'Territory_IW1','Territory_Name':'Territory_Name_IW1'},inplace=True) #DOUBT not sure why we do this\n",
    "SPEC_INCL_LIN = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/reference/qtrspec_SPEC_INCL_LIN.parquet')\n",
    "#Extra\n",
    "SPEC_INCL_LIN.rename(columns={'SPECIALTY_CD':'SpecialtyCode'},inplace=True)\n",
    "SPEC_INCL_LIN.set_index('SpecialtyCode', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a56d5ad-fe05-4d1d-8590-caf707822bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting terr name, region name, area name from zip to terr\n",
    "NET_NEW_FN = pd.merge(NET_NEW_ACCU,ZIP_TO_TERR,left_on='ZIP',right_on='Zip',how='left')\n",
    "whitespace_values = {\n",
    "    'Territory_IW1': \"1111-99999-99\",\n",
    "    'Territory_Name_IW1': \"\",\n",
    "    'Region': \"1111-999\",\n",
    "    'Region_Name': \"\",\n",
    "    'Area': \"1111-9\",\n",
    "    'Area_Name': \"\"\n",
    "}\n",
    "# Fill whitespace_values with the default values\n",
    "NET_NEW_FN.fillna(whitespace_values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad7091a8-9b59-44ed-9a2e-1bd49dd1f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting speciality flags from spec incl lin quaterly file -[NOTE : BUG FIX SUMMARY - sas did not perform a left join, revised code to perfrom mapping strategy] \n",
    "NET_NEW_FN['SPECIALTY_DESCRIPTION'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPECIALTY_DESCRIPTION'])\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPEC_INCL'])\n",
    "NET_NEW_FN.fillna({'SPECIALTY_DESCRIPTION':''},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ec6542b-250b-495f-9899-85e7a405429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column SPEC_INCL_LIN (this is based on match code, so ill have to wait for it to come.- NOTE  Condition is only applied on failed left matches\n",
    "#NET_NEW_FN['SPEC_INCL_LIN'] = ['N' if x == '01' else 'Y' for x in NET_NEW_FN['MatchCode']] - This is wrong\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = np.where(NET_NEW_FN['SPEC_INCL_LIN'].isna(),\n",
    "                                        np.where(NET_NEW_FN['MatchCode'] == '01', 'N', 'Y'),\n",
    "                                        NET_NEW_FN['SPEC_INCL_LIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "205e48ff-8dec-492c-8a81-6388a7f9f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping extra columns (different name , same data)\n",
    "NET_NEW_FN.drop(['Zip'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa96a7-622f-4c5e-ad73-1deed8937988",
   "metadata": {},
   "source": [
    "### Filtering for any legal removal and unknown address HCPs\n",
    "- 'Jami' file is used here (double check import location and check for file updates on quarter change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c8071fb-bd80-49e6-ad94-eabdc0bda9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying exclusions\n",
    "JAMI_INCLEXCL = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/reference/{QTR}/jami_inclexcl.parquet')\n",
    "jami_exclusions = pd.DataFrame()\n",
    "jami_exclusions = JAMI_INCLEXCL[JAMI_INCLEXCL['TYPE'].isin(['Legal Removals', 'Unknown Address'])]\n",
    "jami_exclusions = jami_exclusions[['IID']].copy()\n",
    "jami_exclusions.drop_duplicates(subset='IID', inplace=True)\n",
    "jami_exclusions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ref_week_NET_NEW = pd.merge(NET_NEW_FN,jami_exclusions,on = 'IID',how='left',indicator=True)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[ref_week_NET_NEW['_merge'] == 'left_only']\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250864ed-8ab9-4919-9a01-34436867649e",
   "metadata": {},
   "source": [
    "#### IC flag creation\n",
    "- IC_INCL_LIN is a an important flag which decides if a hcp should be considered for IC or not\n",
    "- CustomerEffectiveStartDate , SPEC_INCL_LIN, CustomerStatusCode, MatchCode decide its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "699db4b7-1e86-4c57-bfa5-3a08fc5b1420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12607, 82)\n"
     ]
    }
   ],
   "source": [
    "ref_week_NET_NEW['IC_INCL_LIN'] = 0  # Initialize the column with 0\n",
    "\n",
    "condition = (\n",
    "    (ref_week_NET_NEW['CustomerEffectiveStartDate'] < CUT_OFF_net) &\n",
    "    (ref_week_NET_NEW['SPEC_INCL_LIN'] == \"Y\") &\n",
    "    (ref_week_NET_NEW['CustomerStatusCode'] == \"Active\") &\n",
    "    (ref_week_NET_NEW['MatchCode'] != '01')\n",
    ")\n",
    "\n",
    "ref_week_NET_NEW.loc[condition, 'IC_INCL_LIN'] = 1\n",
    "print(ref_week_NET_NEW.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b7ca5-401c-49f8-8387-c8a989343144",
   "metadata": {},
   "source": [
    "Importing IIDs from frozen IC universe , and deleting any occurances of those in net new as an added QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edd893a3-7e86-45f7-9f0c-6b3255d1cb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped from ic uni:  0\n"
     ]
    }
   ],
   "source": [
    "CUSTOMER_MASTER_IC_LIN = pd.read_parquet(f's3://vortex-staging-a65ced90/PYADM/quaterly/{QTR}/reference/CUSTOMER_MASTER_IC_LIN.parquet')\n",
    "CUSTOMER_MASTER_IC_LIN = pd.DataFrame(CUSTOMER_MASTER_IC_LIN['IID'])\n",
    "#Faulty Approach - needs checking into : Try using - ref_week_NET_NEW = merged_df[merged_df['_merge'] == 'left_only']\n",
    "# incase of memory issues ?\n",
    "# merged_df = pd.merge(ref_week_NET_NEW, CUSTOMER_MASTER_IC_LIN[['IID']], on='IID', how='inner')\n",
    "# ref_week_NET_NEW = ref_week_NET_NEW.drop(merged_df.index)\n",
    "#updated approach - \n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[~ref_week_NET_NEW['IID'].isin(CUSTOMER_MASTER_IC_LIN['IID'])]\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped from ic uni: \",qc_bef-qc_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "702b845a-fafb-4b32-8c9c-510278c60dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped :  0\n"
     ]
    }
   ],
   "source": [
    "#Only Keep the first Instance of IID Source.\n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW.sort_values(by=['IID', 'SOURCE'], inplace=True)\n",
    "ref_week_NET_NEW.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped : \",qc_bef-qc_af)\n",
    "\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.sort_values(['IID'])\n",
    "ref_week_NET_NEW.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c933cf9d-620f-4efc-817d-2cc2da5f759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT - \n",
    "ref_week_NET_NEW.to_parquet(f's3://vortex-staging-a65ced90/PYADM/weekly/archive/{CUR_WK}/reference/{net_new_varname}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01bdeb-b8cd-43da-a6fc-500f2d44d645",
   "metadata": {},
   "source": [
    "# QC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea4b9c4d-f85a-4ce1-9c18-2ace3c8a2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SOURCE  COUNT\n",
      "6  20240503   1503\n",
      "4  20240510   1523\n",
      "1  20240517   1650\n",
      "2  20240524   1626\n",
      "0  20240531   1895\n",
      "7  20240607   1330\n",
      "3  20240614   1561\n",
      "5  20240621   1519\n"
     ]
    }
   ],
   "source": [
    "#17.1: Check current week's net_new source\n",
    "# Perform the frequency count\n",
    "freq_counts = ref_week_NET_NEW['SOURCE'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['SOURCE', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('SOURCE')\n",
    "\n",
    "# Print the frequency counts\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61873ea4-3de6-4101-b54f-71e285bb0bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IC_INCL_LIN  COUNT\n",
      "0            1   8059\n",
      "1            0   4548\n"
     ]
    }
   ],
   "source": [
    "#17.2: Check IC_INCL_LIN - this wont work yet because flag is not there (match code not avilable)\n",
    "freq_counts = ref_week_NET_NEW['IC_INCL_LIN'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['IC_INCL_LIN', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('COUNT', ascending=False)\n",
    "\n",
    "# Print the frequency counts #NOTE : last check on 27-12-2023 , 16 people with IC flag 0 are lesser in python compared to sas presumably Jami ppl\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c304f16-8768-4991-9dc0-19079334d58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcp_count 12607\n",
      "distinct_hcp_count 12607\n"
     ]
    }
   ],
   "source": [
    "# 17.3: Check Net New for Dups\n",
    "print(\"hcp_count\",ref_week_NET_NEW['IID'].count())\n",
    "print(\"distinct_hcp_count\",ref_week_NET_NEW['IID'].nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
