{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6438b02d-b34e-433a-bb50-e048c55dfd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7d99cf-6a2a-4cc9-9e59-5e1d35b89477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables -\n",
    "this_day = datetime.today()\n",
    "### FOR TESTING - REMOVE LATER ### \n",
    "this_day = this_day - timedelta(days=7)\n",
    "####\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")\n",
    "\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b10b40-d38b-4b52-b911-9ff48a62596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'vortex-staging-a65ced90'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34aa03c2-cc5b-4a5d-8434-87fcccee0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBNAMES\n",
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "ptk = f'PYADM/weekly/archive/{CUR_WK}/plantrak/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75628f7-9193-4181-8dd7-f80bd330e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking Up Data from Plantrak_2\n",
    "# weekly - (NMLLAX stands for Normalized Lax)\n",
    "#NMLLAX= pl.read_parquet(ptk+'\\\\LAX_N.parquet') #Source for this is subject to change , may add full version in future?\n",
    "NMLLAX= pl.read_parquet(f's3://{bucket}/{ptk}LAX_N.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37f88f5c-1610-49d3-bfb0-9cb34bcd5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows where product is null\n",
    "NMLLAX = NMLLAX.filter(pl.col('PROD_CD') != \"\") \n",
    "# This statement should be redundant as we kept inner join with seg def instead of left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1548a13a-0ead-4dd9-be28-c09a0671428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking up Date Parm from previous code to get week number and merging it rx data\n",
    "#date_parm_wk = pl.read_parquet(curwk+\"\\\\curwk_DATE_PARM_WK.parquet\")\n",
    "date_parm_wk = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_WK.parquet')\n",
    "date_parm_wk = date_parm_wk.with_columns(pl.col('WK_END_DATE').dt.date()) \n",
    "\n",
    "#new fix-\n",
    "NMLLAX = NMLLAX.with_columns(pl.col('WK_END_DATE').cast(pl.Date))\n",
    "##\n",
    "\n",
    "NMLLAX = NMLLAX.join(date_parm_wk,on='WK_END_DATE',how='left') # SHOULD I CHANGE THIS TO INNER ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b989593b-5f65-4af5-91f6-940815a62fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['TRX','NRX','TUN','NUN','TUF','NUF']\n",
    "# Adding a new column called PROD_WK , will contain PROD_CD and the week number of transaction\n",
    "# this the column on which we transpose the data  \n",
    "NMLLAX = NMLLAX.with_columns([(pl.col(\"PROD_CD\").cast(pl.Utf8) + \"P_\" + pl.col(\"I\").cast(pl.Utf8)).alias(\"PROD_WK\")])\n",
    "\n",
    "# Only Keeping columns pertinent to LAX_DN\n",
    "NMLLAX = NMLLAX.select(pl.col(['IID', 'PlanID', 'PlanName', 'PayerID', 'PayerName', 'PBMID' ,'PBMName', 'TRX', 'NRX','TUN', 'NUN', 'TUF','NUF','PROD_WK']))\n",
    "\n",
    "# Sorting data at IID level to chunk and filter effectively\n",
    "NMLLAX = NMLLAX.sort('IID')\n",
    "\n",
    "#converting column to cat to save space -\n",
    "NMLLAX = NMLLAX.with_columns(pl.col(\"PROD_WK\").cast(pl.Utf8).cast(pl.Categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ace026d-dc4d-426f-8b3f-1ec4baf3dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_unique_vals' contains a list of all possible column names after transposing data\n",
    "# we will use it to standardize the shape of every chunk, as it will help us concat / stack them each iteration\n",
    "\n",
    "unique_vals = list(NMLLAX['PROD_WK'].unique()) #using full data to get all unique values here\n",
    "# NOTE : It might very well be possible that some weeks of data may be missing, we might have to add those columns manually at some point\n",
    "\n",
    "full_unique_vals = []\n",
    "def unique_vals_prod_wk(col_name):   #this function breaks down PROD_WK to your regular column names like LI1PTUF\n",
    "    parts = col_name.split('_')\n",
    "    for m in metrics:\n",
    "        full = parts[0]+m+parts[-1]\n",
    "        full_unique_vals.append(full)\n",
    "    \n",
    "for i in unique_vals:\n",
    "    unique_vals_prod_wk(i)\n",
    "\n",
    "# Could add a modifier here to check and have full 105 weeks of data ?\n",
    "    \n",
    "full_unique_vals.sort()\n",
    "full_unique_vals =['IID','PlanID', 'PlanName', 'PayerID', 'PayerName', 'PBMID' ,'PBMName'] + full_unique_vals\n",
    "#Adding IID and others because i will also use this list to standardize the order of columns in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e64062e-91e6-4a00-9974-4949f9199bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products in data which do not have 105 weeks of data :  2\n",
      "['MRB', 'ZEL']\n",
      "shape: (2, 2)\n",
      "┌──────┬────────────┐\n",
      "│ prod ┆ num_of_wks │\n",
      "│ ---  ┆ ---        │\n",
      "│ str  ┆ u32        │\n",
      "╞══════╪════════════╡\n",
      "│ MRB  ┆ 83         │\n",
      "│ ZEL  ┆ 55         │\n",
      "└──────┴────────────┘\n",
      "BUT !  - \n",
      "MRB  has gaps in weeks\n",
      "ZEL  has gaps in weeks\n"
     ]
    }
   ],
   "source": [
    "wk_tst = pl.DataFrame()\n",
    "wk_tst = wk_tst.with_columns(pl.Series(name='col_names_raw',values=full_unique_vals[7:]))\n",
    "\n",
    "def split_col_names(value):\n",
    "    prod = value[:3]\n",
    "    metric = value[4:7]\n",
    "    wknum = value[7:]\n",
    "    return prod, metric, wknum\n",
    "\n",
    "wk_tst = wk_tst.with_columns([pl.col(\"col_names_raw\").map_elements(split_col_names, return_dtype=pl.Object).alias(\"split_values\")])\n",
    "\n",
    "wk_tst = wk_tst.with_columns([\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[0], return_dtype=pl.Utf8).alias(\"prod\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[1], return_dtype=pl.Utf8).alias(\"metric\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[2], return_dtype=pl.Utf8).alias(\"wknum\"),\n",
    "])\n",
    "wk_tst = wk_tst.drop([\"split_values\",\"col_names_raw\"])\n",
    "\n",
    "res = wk_tst.group_by(['prod','metric']).agg([pl.col('wknum').n_unique().alias('num_of_wks')])\n",
    "missing_wknum = res.filter(pl.col('num_of_wks') != 105)\n",
    "missing_wknum = missing_wknum.sort(by='prod')\n",
    "print(\"Number of products in data which do not have 105 weeks of data : \",len(missing_wknum['prod'].unique()))\n",
    "print(list((missing_wknum['prod'].unique())))\n",
    "missmps = list((missing_wknum['prod'].unique()))\n",
    "missing_wknum_print = missing_wknum.select(pl.col(['prod','num_of_wks']))\n",
    "missing_wknum_print = missing_wknum_print.unique(subset=['prod','num_of_wks'])\n",
    "print(missing_wknum_print)\n",
    "\n",
    "wk_conti = pl.DataFrame()\n",
    "wk_conti = wk_tst.filter(pl.col('prod').is_in(missmps))\n",
    "wk_conti = wk_conti.drop('metric')\n",
    "wk_conti = wk_conti.unique(subset=['prod','wknum'])\n",
    "wk_conti = wk_conti.with_columns(pl.col(\"wknum\").cast(pl.Int32))\n",
    "\n",
    "print('BUT !  - ')\n",
    "for prod in missmps:\n",
    "    f1 = wk_conti.filter(pl.col('prod')== prod )\n",
    "    if (len(f1['wknum'].unique()) != f1['wknum'].max()):\n",
    "        print(prod,\" has gaps in weeks\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a9ab0-efa7-4956-88e6-71b299a764f7",
   "metadata": {},
   "source": [
    "# Transpose Data By 3000 HCP chunks-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6f0e60-f9a0-46b3-b869-f625e148bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iids = NMLLAX['IID'].unique() \n",
    "chunk_size = 5000 #Each chunk will contain 5000 HCPs worth of transactions (NOT ROWS, they may differ each chunk)\n",
    "\n",
    "iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]\n",
    "#So IID_chunks is a list of lists, each list contains 5000 HCPs and number of lists is our number of chunks\n",
    "\n",
    "prod_family_market_buckets = {\n",
    "    \"MRXF\" : [\"MRGP\",\"MRBP\",\"GLYP\"],\n",
    "    \"LINF\" : [\"LI1P\",\"LI2P\",\"LI3P\"],\n",
    "    \"LUBF\" : [\"AMTP\",\"LUBP\"],\n",
    "    \"GENM\" : [\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"GLYP\"],\n",
    "    \"BRDM\" : [\"AMTP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"MOTP\",\"ZELP\",\"IRLP\"],\n",
    "    \"LAXM\" : [\"AMTP\",\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"GLYP\",\"MOTP\",\"ZELP\",\"IRLP\"]\n",
    "}\n",
    "\n",
    "writer = None\n",
    "df_final = pl.DataFrame()\n",
    "loop_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c99896-5282-4623-8c83-c5902124f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [44:40<00:00, 25.53s/it]\n"
     ]
    }
   ],
   "source": [
    "for iid_chunk in tqdm(iid_chunks):\n",
    "    \n",
    "    # if loop_counter == 11:\n",
    "    #     break\n",
    "    \n",
    "    df_chunk = NMLLAX.filter(pl.col('IID').is_in(iid_chunk))\n",
    "    \n",
    "    df_pivot_chunk = df_chunk.pivot(\n",
    "        values=metrics,index=['IID','PlanID','PlanName','PayerID','PayerName','PBMID','PBMName'],\n",
    "        columns='PROD_WK',\n",
    "        maintain_order=True,\n",
    "        sort_columns=True\n",
    "    )\n",
    "    \n",
    "    del df_chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    df_pivot_chunk = df_pivot_chunk.select(\n",
    "        pl.all().name.map(\n",
    "            lambda col_name: col_name.split('_')[3] + col_name.split('_')[0] + col_name.split('_')[-1] if 'PROD_WK_' in col_name else col_name\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    missing_cols = set(full_unique_vals) - set(df_pivot_chunk.columns)\n",
    "\n",
    "    if missing_cols:\n",
    "        df_missing = pl.DataFrame({col: pl.Series([None]*len(df_pivot_chunk), dtype=pl.Float64) for col in missing_cols})\n",
    "        df_pivot_chunk = pl.concat([df_pivot_chunk, df_missing], how='horizontal')\n",
    "    \n",
    "    del df_missing\n",
    "    gc.collect()\n",
    "\n",
    "    df_pivot_chunk = df_pivot_chunk.select(full_unique_vals)\n",
    "    \n",
    "    for prod_family, prod_codes in prod_family_market_buckets.items():\n",
    "        for metric in metrics:\n",
    "            prod_metric_combinations = {prod_code + metric for prod_code in prod_codes}\n",
    "            relevant_columns = [col for col in full_unique_vals if any(comb  in col for comb in prod_metric_combinations)]\n",
    "            relevant_columns = [(col, col.split(metric)) for col in relevant_columns]\n",
    "            week_numbers = sorted(set(int(parts[-1]) for col, parts in relevant_columns))\n",
    "            for week_number in week_numbers:\n",
    "                new_column = prod_family + metric + str(week_number)\n",
    "                week_columns = [col for col, parts in relevant_columns if parts[-1] == str(week_number)]\n",
    "                #df_pivot_chunk = df_pivot_chunk.with_columns(sum(pl.col(c) for c in week_columns).alias(new_column))\n",
    "                df_pivot_chunk = df_pivot_chunk.with_columns(pl.sum_horizontal(week_columns).alias(new_column))\n",
    "\n",
    "    df_final = df_final.vstack(df_pivot_chunk)\n",
    "    \n",
    "    loop_counter += 1\n",
    "\n",
    "    if loop_counter % 5 == 0 and loop_counter != 0: # This takes about 25 seconds ? TTT Should be ~ 30 Secs\n",
    "        table = df_final.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(ptk+'\\\\LAX_DN.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{ptk}LAX_DN.parquet', table.schema)\n",
    "        writer.write_table(table)\n",
    "        del table\n",
    "        gc.collect()\n",
    "        df_final = pl.DataFrame() # Reset df_final after writing to file\n",
    "\n",
    "    \n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(df_final) > 0:\n",
    "    table = df_final.to_arrow()\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{ptk}LAX_DN.parquet', table.schema)\n",
    "    writer.write_table(table)\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7981a6-93d3-45a6-a757-75e5d27441fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYADM/weekly/archive/20240531/plantrak/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "306962ba-1e59-4951-b524-efb63bdf5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_iids = NMLLAX['IID'].unique() \n",
    "# chunk_size = 3000\n",
    "# iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]\n",
    "\n",
    "# writer = None\n",
    "# df_final = pl.DataFrame()\n",
    "# loop_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b724581-ca4e-4ba8-9f9a-10a5d5d6a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iid_chunk in tqdm(iid_chunks): #4 is the max limit\n",
    "\n",
    "#     if loop_counter == 5:\n",
    "#         break\n",
    "    \n",
    "#     df_chunk = NMLLAX.filter(pl.col('IID').is_in(iid_chunk))\n",
    "    \n",
    "#     df_pivot_chunk = df_chunk.pivot(\n",
    "#         values=metrics,index=['IID','PlanID','PlanName','PayerID','PayerName','PBMID','PBMName'],\n",
    "#         columns='PROD_WK',\n",
    "#         maintain_order=True,\n",
    "#         sort_columns=True\n",
    "#     )\n",
    "    \n",
    "#     df_pivot_chunk = df_pivot_chunk.select(\n",
    "#         pl.all().name.map(\n",
    "#             lambda col_name: col_name.split('_')[3] + col_name.split('_')[0] + col_name.split('_')[-1] if 'PROD_WK_' in col_name else col_name\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     del df_chunk\n",
    "#     gc.collect()\n",
    "    \n",
    "#     missing_cols = set(full_unique_vals) - set(df_pivot_chunk.columns)\n",
    "\n",
    "#     if missing_cols:\n",
    "#         df_missing = pl.DataFrame({col: pl.Series([None]*len(df_pivot_chunk), dtype=pl.Float64) for col in missing_cols})\n",
    "#         df_pivot_chunk = pl.concat([df_pivot_chunk, df_missing], how='horizontal')  \n",
    "        \n",
    "#     del df_missing\n",
    "#     gc.collect()\n",
    "\n",
    "#     df_pivot_chunk = df_pivot_chunk.select(full_unique_vals)\n",
    "\n",
    "#     df_final = df_final.vstack(df_pivot_chunk)\n",
    "    \n",
    "#     loop_counter += 1\n",
    "\n",
    "#     if loop_counter % 5 == 0 and loop_counter != 0: # This takes about 25 seconds ? TTT Should be ~ 30 Secs\n",
    "#         table = df_final.to_arrow()\n",
    "#         if writer is None:\n",
    "#             writer = pq.ParquetWriter(ptk+'\\\\plan_df1.parquet', table.schema)\n",
    "#         writer.write_table(table)\n",
    "#         df_final = pl.DataFrame() # Reset df_final after writing to file\n",
    "\n",
    "    \n",
    "\n",
    "# # Write any remaining chunks to the Parquet file\n",
    "# if len(df_final) > 0:\n",
    "#     table = df_final.to_arrow()\n",
    "#     if writer is None:\n",
    "#         writer = pq.ParquetWriter(ptk+'\\\\plan_df1.parquet', table.schema)\n",
    "#     writer.write_table(table)\n",
    "\n",
    "# # Close the ParquetWriter\n",
    "# if writer is not None:\n",
    "#     writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470550e8-8423-4d3d-9427-cb419512afd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
