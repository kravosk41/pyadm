{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc2eb7c-377d-4868-ba8b-d9c8e59eed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18bc4e95-c9f2-496c-8c9d-300388a1d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables -\n",
    "this_day = datetime.today()\n",
    "### FOR TESTING - REMOVE LATER ### \n",
    "this_day = this_day - timedelta(days=7)\n",
    "####\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")\n",
    "\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29e983d-e7ef-491c-b8af-f7784efdc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing IW Release File \n",
    "s3 = s3fs.S3FileSystem()\n",
    "bucket_list = s3.listdir(f'vortex-staging-a65ced90/PYADM/raw/{CUR_PROC_WK}/inbound/')\n",
    "for file in bucket_list:\n",
    "    if (file['Key'].__contains__(f\"IRWD_RELEASE_WKLY_{CUR_PROC_WK}\")):\n",
    "        release_file = file['Key']\n",
    "\n",
    "# Reading Release File -\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = release_file.split('/')[0]\n",
    "file_key = release_file.split('/',1)[1]\n",
    "\n",
    "adm = pd.read_csv(f's3://{bucket}/{file_key}',sep='|')\n",
    "adm1 = adm[CUR_PROC_WK].tolist()\n",
    "\n",
    "for item in adm1:\n",
    "    if item.startswith('P_OUT_110_WKLY_PLANTRAK_'):\n",
    "        PLANTRAK_TIMESTAMP = item.split('_')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e0d3af-d09c-4f40-98bb-701a1fc575f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBNAMES\n",
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19105cb-8322-43bb-8967-1bfd2d78f7f2",
   "metadata": {},
   "source": [
    "##### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a430d5-80e4-442f-a489-5bf0fb1b7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Period File to get Week_End_Date, will help in trimming data \n",
    "curwk_period = pl.read_parquet(f's3://{bucket}/{curwk}curwk_period.parquet')\n",
    "curwk_DATE_PARM_WK = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_WK.parquet')\n",
    "\n",
    "curwk_DATE_PARM_WK = curwk_DATE_PARM_WK.with_columns(pl.col('WK_END_DATE').cast(pl.Date))\n",
    "curwk_period = curwk_period.with_columns(pl.col('WK_END_DATE').cast(pl.Date))\n",
    "\n",
    "\n",
    "curwk_period= curwk_period.join(curwk_DATE_PARM_WK,on='WK_END_DATE',how='inner')\n",
    "curwk_period = curwk_period.sort(by = [\"WK_END_DATE\",\"PeriodKey\"])\n",
    "curwk_period= curwk_period.select(pl.col(['WK_END_DATE','PeriodKey'])) # Removing 'I' from here because i didnt end up using I in the firest iteration?\n",
    "min_period_key = pd.to_datetime((curwk_period['PeriodKey'].min()), format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908fea51-7a1b-4cf7-946a-7ca400570ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file = raw_path + 'P_OUT_110_WKLY_PLANTRAK_' + str(PLANTRAK_TIMESTAMP) + \".TXT\"\n",
    "input_file = f's3://{bucket}/{raw_path}P_OUT_110_WKLY_PLANTRAK_{PLANTRAK_TIMESTAMP}.TXT'\n",
    "collist = [\n",
    "    'ChannelID', 'IID', 'ProductID', 'ProductName', \n",
    "    'PlanID', 'PlanName', 'PayerID', 'PayerName', 'PBMID', 'PBMName', 'WeekKey', \n",
    "    'TotalRx', 'NewRx', 'TotalQuantity', 'NewQuantity', 'TotalFactoredQuantity', 'NewFactoredQuantity'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac055c13-7abc-41cd-ad67-4683c353a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(\n",
    "    input_file,delimiter='|',\n",
    "    dtype={12:str} # that column had mixed dtypes\n",
    "    ,parse_dates=[10],\n",
    "    usecols=collist,\n",
    "    #nrows=1000000, #for testing , leaving it commented [ caps max amount of lines read]\n",
    "    chunksize=1000000, #7 mins with 5k , 5 mins with 50k , 4 mins with 100k, test more if like\n",
    "    iterator=True\n",
    ")\n",
    "# sum(1 for row in open(input_file, 'r')) - can be used to get total number of rows for better display of tqdm progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107340de-3218-4459-9d94-283da428a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [06:29, 13.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28123953, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_RX_PLANTRAK_WEEKLY = pl.DataFrame()\n",
    "for chunk in tqdm(reader): #Note : if a better progress tracker is req , add (nrows//chunksize) as second arg in tqdm\n",
    "    \n",
    "    chunk_df = (\n",
    "        pl.DataFrame(chunk)\n",
    "        .lazy()\n",
    "        .filter(pl.col('WeekKey') >= pl.datetime(min_period_key.year,min_period_key.month,min_period_key.day)) #This conrols the broad filter date pl.datetime(2021,1,1)\n",
    "        .with_columns(pl.col('WeekKey').dt.strftime('%Y%m%d').alias('PeriodKey'))\n",
    "        .collect()\n",
    "    )\n",
    "    #.with_columns(pl.col('WeekKey').cast(pl.Date))\n",
    "    RAW_RX_PLANTRAK_WEEKLY.vstack(chunk_df,in_place=True)\n",
    "\n",
    "# RAW_RX_PLANTRAK_WEEKLY.estimated_size(unit='gb') ----> 7.31 GB , (39026959, 17) 730it [04:01,  3.02it/s]\n",
    "# # 1 is RTL , 2 is MO\n",
    "RAW_RX_PLANTRAK_WEEKLY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7090b749-c431-4061-ae79-7b0d3c2d5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cat_PK = list(curwk_period['PeriodKey'].unique())\n",
    "with pl.StringCache():\n",
    "    #Pre Filing the global string cache - \n",
    "    pl.Series(list_of_cat_PK).cast(pl.Categorical)\n",
    "    #Assinging Cats-\n",
    "    RAW_RX_PLANTRAK_WEEKLY = RAW_RX_PLANTRAK_WEEKLY.with_columns([pl.col(\"PeriodKey\").cast(pl.Utf8).cast(pl.Categorical)])\n",
    "    curwk_period = curwk_period.with_columns([pl.col(\"PeriodKey\").cast(pl.Utf8).cast(pl.Categorical)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de105c76-6dc5-4a25-b829-b950c0dcca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffea1197-372b-4ee5-8e57-2b108083f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_RX_PLANTRAK_WEEKLY = RAW_RX_PLANTRAK_WEEKLY.with_columns([\n",
    "    pl.col(\"ChannelID\").cast(pl.Utf8).cast(pl.Categorical),\n",
    "    pl.col(\"ProductID\").cast(pl.Utf8).cast(pl.Categorical),\n",
    "    pl.col(\"ProductName\").cast(pl.Utf8).cast(pl.Categorical)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94267cd-4500-4d71-9e69-5ea4f1207799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.069560786709189"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_RX_PLANTRAK_WEEKLY.estimated_size(unit='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7e2906-49fe-41a6-98a8-4b8c9cd968bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Too Memory Intensive ? - May have to impliment chunking in future\n",
    "RAW_RX_PLANTRAK_WEEKLY = RAW_RX_PLANTRAK_WEEKLY.join(curwk_period,on='PeriodKey',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6acc3da-9297-483c-810b-6f56823e8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_RX_PLANTRAK_WEEKLY = RAW_RX_PLANTRAK_WEEKLY.drop(['WeekKey','PeriodKey'])\n",
    "#Converting pulled column to cat as well\n",
    "#RAW_RX_PLANTRAK_WEEKLY = RAW_RX_PLANTRAK_WEEKLY.with_columns([pl.col(\"I\").cast(pl.Utf8).cast(pl.Categorical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf104f1-fe19-49ce-b7e8-403ee0d035b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.860018400475383"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_RX_PLANTRAK_WEEKLY.estimated_size(unit='gb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a445e8-7fba-4c84-9e18-adc8585be72e",
   "metadata": {},
   "source": [
    "Note -\n",
    "Ive removed Market Code and Market name \n",
    "but the SAS code has a frequncy check for Market Name , Need to add that\n",
    "column back if this QC is deemed necessary \n",
    "My justiciation for its removal is that we only get data for one Market and 2 columns will hog space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e99879b-3ec6-4dd4-9ac0-640d912eee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Consider Rewriting or removing this , takes too long , provides unsorted info- \n",
    "# print(\"Period Key Frequency Check :\")\n",
    "# RAW_RX_PLANTRAK_WEEKLY.group_by(\"PeriodKey\").count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a0316a-e9a7-4a24-ad04-d14b86ddfbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Result 0  :  0\n"
     ]
    }
   ],
   "source": [
    "## Reading Mergecases -\n",
    "#MERGECASES = pl.read_parquet(curwk+\"//curwk_MERGES.parquet\", columns = ['IID','IronwoodWinnerID'])\n",
    "MERGECASES = pl.read_parquet(f's3://{bucket}/{curwk}curwk_MERGES.parquet')\n",
    "\n",
    "# Make sure there is no issues wih MERGECASES QC's from Main adm should pass, if not request clean file\n",
    "#Quick Check -\n",
    "pre = MERGECASES.shape[0]\n",
    "MERGECASES.unique(subset=['IID','IronwoodWinnerID'])\n",
    "post = MERGECASES.shape[0]\n",
    "print(\"Expected Result 0  : \",pre-post)\n",
    "\n",
    "# tdf = MERGECASES.filter(\n",
    "#     pl.col('IID') == pl.col('IronwoodWinnerID')\n",
    "# )\n",
    "# tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae938a34-e63c-4207-87d5-484666fc3d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LooserID records found in raw data :  83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_MERGECASES_ids  = list(MERGECASES['IID'].unique())\n",
    "LOOSER_RX = RAW_RX_PLANTRAK_WEEKLY.filter(pl.col('IID').is_in(temp_MERGECASES_ids)) \n",
    "WINNER_RX = RAW_RX_PLANTRAK_WEEKLY.filter(~pl.col('IID').is_in(temp_MERGECASES_ids))\n",
    "print(\"Number of LooserID records found in raw data : \",LOOSER_RX.shape[0])\n",
    "del RAW_RX_PLANTRAK_WEEKLY\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daebe5c5-5ec9-47f9-9659-e9969fc609da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fixing Looser ID records\n",
    "LOOSER_RX = LOOSER_RX.join(MERGECASES,on='IID',how='left')\n",
    "group_cols = ['IronwoodWinnerID','ChannelID','ProductID','ProductName',\n",
    "             'PlanID','PlanName','PayerID','PayerName','PBMID','PBMName','WK_END_DATE'] #removed I from here\n",
    "sum_cols = ['TotalRx','TotalQuantity','TotalFactoredQuantity','NewRx','NewQuantity','NewFactoredQuantity']\n",
    "\n",
    "#perform groupby and calc function \n",
    "LOOSER_RX1 = LOOSER_RX.group_by(group_cols).agg([pl.col(c).sum() for c in sum_cols])\n",
    "LOOSER_RX1= LOOSER_RX1.rename({'IronwoodWinnerID':'IID'})\n",
    "del LOOSER_RX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "633dd2cb-d97a-48c0-93cf-c734f987ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28123953, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining back with winner (note , im trying not to make new dataframes because of large sizes)\n",
    "LOOSER_RX1 = LOOSER_RX1.select(WINNER_RX.columns)\n",
    "WINNER_RX.vstack(LOOSER_RX1,in_place=True)\n",
    "WINNER_RX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd635548-8c7d-475a-a194-fd232952c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.860018400475383"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINNER_RX.estimated_size(unit='gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8947ddba-432a-4506-b5b2-c0c43e8d1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summerize Winner Raw Data. # HEAVY OPERATION - Make sure memory is stable\n",
    "group_cols = ['IID','ChannelID','ProductID','ProductName',\n",
    "             'PlanID','PlanName','PayerID','PayerName','PBMID','PBMName','WK_END_DATE'] #removed I from here\n",
    "\n",
    "WINNER_RX = WINNER_RX.group_by(group_cols).agg([pl.col(c).sum() for c in sum_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fb8ee7b-6329-4e73-981e-612268a4cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WINNER_RX.write_parquet(curwk+'\\\\RAW_RX_PLANTRAK_CUR_WK.parquet',compression='zstd',compression_level=10,use_pyarrow=True)\n",
    "\n",
    "WINNER_RX.to_pandas().to_parquet(f's3://{bucket}/{curwk}RAW_RX_PLANTRAK_CUR_WK.parquet',compression='snappy')\n",
    "\n",
    "#End of 2nd Code , calls other codes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
