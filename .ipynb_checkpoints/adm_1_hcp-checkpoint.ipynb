{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00728955-138f-45fa-9601-f0492f185973",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e91dd63-6045-45e9-abd2-26219186b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06706ef-b62a-4050-9c84-07fa355044bf",
   "metadata": {},
   "source": [
    "Calculate Date Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "010f3eba-02e9-482b-8ec3-7afef22cc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Calcuation Section :\n",
    "\n",
    "#Since data is received on Monday\n",
    "#Getting this weeks monday from today's date.\n",
    "\n",
    "##############################\n",
    "#FOR CUR_PROC_WK, TODAY, PRE_PROC_WK,CUR_WK,PRE_WK\n",
    "this_day = datetime.today()\n",
    "\n",
    "# #####\n",
    "# # for testing :\n",
    "# this_day = this_day - timedelta(days=7) # REMOVE ON REGULAR RUNS\n",
    "# #####\n",
    "\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")  #Converting to desired string format\n",
    "TODAY = monday  #Keeping datetime format\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "#xponent data week, even in month closing week, still use FRIDAY date;\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "DATA_DATE_WEEKLY = f\"{CUR_WK0.year}-{CUR_WK0.month:02d}-{CUR_WK0.day:02d}\"\n",
    "YR = CUR_WK0.year\n",
    "\n",
    "#weekly events and calls data date - NO two weeks delay(FRIDAY);\n",
    "EVTS_DATE_WEEKLY0 = monday - timedelta(days=3)\n",
    "EVTS_DATE_WEEKLY = f\"{EVTS_DATE_WEEKLY0.year}-{EVTS_DATE_WEEKLY0.month:02d}-{EVTS_DATE_WEEKLY0.day:02d}\"\n",
    "\n",
    "#previous data week - FRIDAY\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")\n",
    "\n",
    "DATA_DATE_CALENDAR = f\"{monday.year}-{monday.month:02d}-{monday.day:02d}\"\n",
    "RECORD_END_DATE = pd.PeriodIndex([this_day], freq='Q').end_time[0].strftime('%Y-%m-%d')\n",
    "##############################\n",
    "QTR = f'{CUR_WK0.year}Q{(CUR_WK0.month-1)//3+1}'\n",
    "QTR_NET = f\"Q{((CUR_WK0.month - 1) // 3 + 1)}\"\n",
    "CUT_OFF_net = ((pd.PeriodIndex([CUR_WK0], freq='Q')) + 1).start_time[0].strftime('%Y-%m-%d')\n",
    "##############################\n",
    "month_end_date = CUR_WK0.replace(day=1) - timedelta(days=1) #This hold the month ending date for the month before current xpn date\n",
    "# Month closing week\n",
    "if 0 <= (CUR_WK0 - month_end_date).days < 7:\n",
    "    # Xponent data month, in month closing week\n",
    "    # Xponent data date or last day of the previous month (if month closing) - FRIDAY\n",
    "    DATA_DATE_MONTHLY = str(month_end_date.year) + '-' + str(month_end_date.month).zfill(2) + '-' + str(month_end_date.day).zfill(2)\n",
    "else:\n",
    "    # Xponent data month, not in month closing week\n",
    "    # Xponent data date or last day of the previous month (if month closing) - FRIDAY\n",
    "    DATA_DATE_MONTHLY = DATA_DATE_WEEKLY\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d0504f6-d9df-4242-b476-2cf969899e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUR_PROC_WK2 = '20240702'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c338b83-4721-4d58-b004-25d6baf0aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing IW Release File \n",
    "s3 = s3fs.S3FileSystem()\n",
    "bucket_list = s3.listdir(f'vortex-staging-a65ced90/PYADM/raw/{CUR_PROC_WK}/inbound/')\n",
    "for file in bucket_list:\n",
    "    if (file['Key'].__contains__(f\"IRWD_RELEASE_WKLY_{CUR_PROC_WK}\")):\n",
    "        release_file = file['Key']\n",
    "\n",
    "# Reading Release File -\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = release_file.split('/')[0]\n",
    "file_key = release_file.split('/',1)[1]\n",
    "\n",
    "adm = pd.read_csv(f's3://{bucket}/{file_key}',sep='|')\n",
    "adm1 = adm[CUR_PROC_WK].tolist()\n",
    "\n",
    "for item in adm1:\n",
    "    if item.startswith('P_OUT_110_HCP-ZIP-DEFAULT_'):\n",
    "        HCPZD_TIMESTAMP = item.split('_')[-1].split('.')[0]\n",
    "    elif item.startswith('P_OUT_110_HCP_'):\n",
    "        HCP_TIMESTAMP = item.split('_')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d32fa-3a28-49f2-bb3e-2e12c976a9be",
   "metadata": {},
   "source": [
    "Library Names and File Paths :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f02764d-c641-48ad-b4fd-3f982c6c8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "qtrspec = f'PYADM/quaterly/{QTR}/reference/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "ref_pwek = f'PYADM/weekly/archive/{PRE_WK}/reference/'\n",
    "ref_week = 'PYADM/weekly/staging/reference/'\n",
    "ref_month = 'PYADM/monthly/staging/reference/'\n",
    "ZIP = f'PYADM/quaterly/{QTR}/geography/'\n",
    "jami = f'PYADM/reference/{QTR}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632229c-2833-47b1-a3e6-b539ba85981a",
   "metadata": {},
   "source": [
    "Importing ZIP Default Files-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "717d517a-4fab-460b-9cc8-d87486cf3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 178 ms, sys: 4.98 ms, total: 183 ms\n",
      "Wall time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_CUSTOMER_ZD = pd.read_csv(\n",
    "    f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT_{HCPZD_TIMESTAMP}.TXT',\n",
    "    delimiter='|',\n",
    "    dtype={1:'Int64',37:'Int64',38:'Int64',32:str,17:str,29:str,20:str,18:str,26:str,21:str,15:str,23:str,25:str,24:str,22:str,14:str},\n",
    "    parse_dates=[31]\n",
    ")\n",
    "temp_CUSTOMER_ZD.rename(columns={'win_i_id': 'IronwoodWinnerID'}, inplace=True)\n",
    "\n",
    "temp_CUSTOMERFLAG_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-FLG_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_ADDRESS_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-ADDR_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype={1:str,11:str,10: str,6:str,12:str})\n",
    "\n",
    "temp_ADDRESSFLAG_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-ADDR-FLG_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "temp_ADDRESSFLAG_ZD.rename(columns={'AddressSourceID': 'AddressSourceId'}, inplace=True)\n",
    "\n",
    "temp_STATELICENSE_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-STATE-LIC_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_COMMUNICATION_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-COMM_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_SPECIALTY_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-SPECIALTIES_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype = {4:str,5:str,1:str})\n",
    "\n",
    "temp_DEA_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-DEA_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_IDENTIFIER_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-IDENTIFIERS_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype={'IID':'Int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b2473-9a54-4af3-ad3a-65fad03e175a",
   "metadata": {},
   "source": [
    "Import HCP Files-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68935b50-c129-4ac0-98be-9d6e32e43a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropped Rows from Dedup of Customer Component :  0\n",
      "CPU times: user 9.58 s, sys: 747 ms, total: 10.3 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GET S3 OBJ function was faster by 10 seconds!!!\n",
    "temp_CUSTOMER1 = pd.read_csv(\n",
    "    f's3://{bucket}/{raw_path}P_OUT_110_HCP_{HCP_TIMESTAMP}.TXT',\n",
    "    delimiter='|',\n",
    "    dtype={1:'Int64',37:'Int64',38:'Int64',\n",
    "          32:str,6:str,17:str,29:str,20:str,18:str,26:str,21:str,15:str,23:str,25:str,24:str,22:str,14:str},\n",
    "    parse_dates=[31]\n",
    ")\n",
    "\n",
    "temp_CUSTOMER1.rename(columns={'win_i_id': 'IronwoodWinnerID'}, inplace=True)\n",
    "qc_before = len(temp_CUSTOMER1)\n",
    "# DEDUP on 'IID'\n",
    "temp_CUSTOMER1.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "qc_after = len(temp_CUSTOMER1)\n",
    "print(\" Dropped Rows from Dedup of Customer Component : \",qc_before-qc_after)\n",
    "\n",
    "temp_CUSTOMERFLAG1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-FLG_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_ADDRESS1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ADDR_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype={1:str,11:str,10: str,6:str,12:str}, on_bad_lines='warn')\n",
    "\n",
    "temp_ADDRESSFLAG1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ADDR-FLG_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "temp_ADDRESSFLAG1.rename(columns={'AddressSourceID': 'AddressSourceId'}, inplace=True)\n",
    "\n",
    "temp_STATELICENSE1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-STATE-LIC_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_COMMUNICATION1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-COMM_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_SPECIALTY1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-SPECIALTIES_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype = {4:str,5:str,1:str})\n",
    "\n",
    "temp_DEA1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-DEA_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_IDENTIFIER1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-IDENTIFIERS_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype={'IID':'Int64',1:str,2:str},\n",
    "                               converters={'IdentifierValue': lambda x: str(x) if x != 'nan' else ''}\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869398e-b84f-477c-8964-55bd7e8ac7fc",
   "metadata": {},
   "source": [
    "##### FIX FOR MERGING ADDRESS AND FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f46b17c-caf3-45cb-902e-2fc58d4f8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " temp_ADDRESS_ZD dropped :  0\n",
      " temp_ADDRESSFLAG_ZD dropped :  0\n",
      " temp_ADDRESS1 dropped :  0\n",
      " temp_ADDRESSFLAG1 dropped :  0\n",
      "CPU times: user 3.54 s, sys: 45 ms, total: 3.58 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# *Dedup Address Components first -;\n",
    "# THIS SHOULD HAVE ZERO DUPS\n",
    "\n",
    "qc_before = len(temp_ADDRESS_ZD)\n",
    "temp_ADDRESS_ZD.drop_duplicates(subset=['IID', 'AddressSourceId'], keep='first', inplace=True)\n",
    "# Adding Sorting Step\n",
    "temp_ADDRESS_ZD.sort_values('AddressSourceId',inplace=True,ignore_index=True)\n",
    "qc_after = len(temp_ADDRESS_ZD)\n",
    "print(\" temp_ADDRESS_ZD dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESSFLAG_ZD)\n",
    "temp_ADDRESSFLAG_ZD.drop_duplicates(subset=['IID', 'AddressSourceId','AddressFlagType'], keep='first', inplace=True)\n",
    "qc_after = len(temp_ADDRESSFLAG_ZD)\n",
    "print(\" temp_ADDRESSFLAG_ZD dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESS1)\n",
    "temp_ADDRESS1.drop_duplicates(subset=['IID', 'AddressSourceId'], keep='first', inplace=True)\n",
    "# Adding Sorting Step\n",
    "temp_ADDRESS1.sort_values('AddressSourceId',inplace=True,ignore_index=True)\n",
    "qc_after = len(temp_ADDRESS1)\n",
    "print(\" temp_ADDRESS1 dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESSFLAG1)\n",
    "temp_ADDRESSFLAG1.drop_duplicates(subset=['IID', 'AddressSourceId','AddressFlagType'], keep='first', inplace=True)\n",
    "qc_after = len(temp_ADDRESSFLAG1)\n",
    "print(\" temp_ADDRESSFLAG1 dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c79f626-b2dc-4140-bb1b-112349799523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Addr with Addr Flag ;\n",
    "temp_ADDRESS_ZD_F = pd.merge(temp_ADDRESS_ZD, temp_ADDRESSFLAG_ZD[['IID', 'AddressSourceId','AddressFlagType']],\n",
    "                             how = 'left',\n",
    "                             left_on = ['IID', 'AddressSourceId'],\n",
    "                             right_on = ['IID', 'AddressSourceId'])\n",
    "temp_ADDRESS_ZD=temp_ADDRESS_ZD_F.copy()\n",
    "\n",
    "temp_ADDRESS1_F = pd.merge(temp_ADDRESS1, temp_ADDRESSFLAG1[['IID', 'AddressSourceId','AddressFlagType']],\n",
    "                             how = 'left',\n",
    "                             left_on = ['IID', 'AddressSourceId'],\n",
    "                             right_on = ['IID', 'AddressSourceId'])\n",
    "temp_ADDRESS1=temp_ADDRESS1_F.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b7a78-d1e5-49be-a1a5-191586bbf787",
   "metadata": {},
   "source": [
    "##### JOIN ALL PARTITIONS AND ZIP DEFAULT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ce29ec1-5d18-45c4-972b-0588d31162c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 452 ms, sys: 4.12 ms, total: 456 ms\n",
      "Wall time: 559 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def comb_hcp(DF_HCP,DF_ZD):\n",
    "    combined_df = pd.concat([DF_HCP, DF_ZD], ignore_index=True)\n",
    "    return(combined_df)\n",
    "\n",
    "curwk_CUSTOMER = comb_hcp(temp_CUSTOMER1,temp_CUSTOMER_ZD) \n",
    "curwk_CUSTOMERFLAG = comb_hcp(temp_CUSTOMERFLAG1,temp_CUSTOMERFLAG_ZD) \n",
    "curwk_ADDRESS = comb_hcp(temp_ADDRESS1,temp_ADDRESS_ZD) \n",
    "curwk_STATELICENSE = comb_hcp(temp_STATELICENSE1,temp_STATELICENSE_ZD)\n",
    "curwk_COMMUNICATION = comb_hcp(temp_COMMUNICATION1,temp_COMMUNICATION_ZD) \n",
    "curwk_SPECIALTY = comb_hcp(temp_SPECIALTY1,temp_SPECIALTY_ZD) \n",
    "curwk_DEA = comb_hcp(temp_DEA1,temp_DEA_ZD) \n",
    "curwk_IDENTIFIER = comb_hcp(temp_IDENTIFIER1,temp_IDENTIFIER_ZD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98bb26-63bc-4a61-a785-4a744e4ed6ef",
   "metadata": {},
   "source": [
    "##### CREATE CURRENT WEEK CUSTOMER_MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca5cc2bb-246d-4b98-a2ba-d6fd0c90154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date related columns from Customer Component need to be changed from text to datetime format so that filtering can be done later\n",
    "# Added this because 9999-12-31 is not supported by the datetime converter\n",
    "# So active Customer will now be designated by 2262-04-11 instead as its winthin supported date range\n",
    "\n",
    "curwk_CUSTOMER.loc[curwk_CUSTOMER['CustomerEffectiveEndDate'] == '9999-12-31', 'CustomerEffectiveEndDate'] = '2262-04-11'\n",
    "\n",
    "# Date Column type change for Identifier -\n",
    "curwk_CUSTOMER['CustomerEffectiveStartDate'] = pd.to_datetime(curwk_CUSTOMER['CustomerEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_CUSTOMER['CustomerEffectiveEndDate'] = pd.to_datetime(curwk_CUSTOMER['CustomerEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4fec4dd4-6326-4045-a359-21dea21935eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " curwk_CUSTOMER dropped :  0\n"
     ]
    }
   ],
   "source": [
    "# Dedup CUSTOMER component\n",
    "qc_before = len(curwk_CUSTOMER)\n",
    "temp_CUSTOMER = curwk_CUSTOMER.sort_values(by='IID').drop_duplicates(subset='IID', keep='first')\n",
    "temp_CUSTDUP = curwk_CUSTOMER[curwk_CUSTOMER.duplicated(subset='IID', keep=False)]\n",
    "qc_after = len(temp_CUSTOMER)\n",
    "print(\" curwk_CUSTOMER dropped : \",qc_before-qc_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83ef3736-1d2f-425d-86a2-e8fdf3bd1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date related columns from Address Component need to be changed from text to datetime format so that filtering can be done\n",
    "# Added this because 9999-12-31 is not supported by the datetime converter\n",
    "# So active addresses will now be designated by 2262-04-11 instead as its winthin supported date range\n",
    "curwk_ADDRESS.loc[curwk_ADDRESS['AddressEffectiveEndDate'] == '9999-12-31', 'AddressEffectiveEndDate'] = '2262-04-11'\n",
    "\n",
    "# Date Column type change for address - \n",
    "curwk_ADDRESS['AddressEffectiveStartDate'] = pd.to_datetime(curwk_ADDRESS['AddressEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_ADDRESS['AddressEffectiveEndDate'] = pd.to_datetime(curwk_ADDRESS['AddressEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c30060-62cd-4559-b517-51b5465fa33c",
   "metadata": {},
   "source": [
    "##### Dedup and CLean ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5e8987d-b41e-47fd-9893-c4e55bb74479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  916545 Records in curwk_ADDRESS\n",
      "There ARE  915394 Records in temp_ADDRESS\n",
      "There ARE  1151 Records in temp_ADDRESSEND\n"
     ]
    }
   ],
   "source": [
    "# creating empty dataframes first\n",
    "temp_ADDRESSEND = pd.DataFrame() #this will store all the addresses that failed the filters\n",
    "temp_ADDRESS = pd.DataFrame() #final filterted result\n",
    "\n",
    "# Defining the conditions -\n",
    "condition1 = curwk_ADDRESS['AddressEffectiveEndDate'] < TODAY\n",
    "condition2 = curwk_ADDRESS['AddressStatusCode'] == 'Inactive'\n",
    "condition3 = curwk_ADDRESS['AddressEffectiveEndDate'].isna()  # Additional condition for NaT values\n",
    "\n",
    "# applying filter - \n",
    "temp_ADDRESSEND = curwk_ADDRESS[(condition1 | condition2 | condition3)].copy()\n",
    "temp_ADDRESS = curwk_ADDRESS[~(condition1 | condition2 | condition3)].copy() #ive simply used negagation '~' here to flip the condtions here\n",
    "\n",
    "#qc \n",
    "print(\"There were \",len(curwk_ADDRESS),\"Records in curwk_ADDRESS\")\n",
    "print(\"There ARE \",len(temp_ADDRESS),\"Records in temp_ADDRESS\")\n",
    "print(\"There ARE \",len(temp_ADDRESSEND),\"Records in temp_ADDRESSEND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d6987-9f6f-4baf-9de9-f76d1bfcec73",
   "metadata": {},
   "source": [
    "#### 2.1: Check each IID only has one BOB address -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90ba4765-448e-4019-a97c-218a834ef363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Table for BOB Count:\n",
      "HCPs\n",
      "0      8887\n",
      "1    131716\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # QC Count the number of addresses with AddressFlagType = 'BOB' for each hcp\n",
    "# this approach is no longer working - \n",
    "# bob_count = temp_ADDRESS.groupby('IID')['AddressFlagType'].apply(lambda x: (x == 'BOB').sum()).reset_index()\n",
    "# bob_count.rename(columns={'AddressFlagType': 'BOB Count'}, inplace=True)\n",
    "\n",
    "# bob_count_freq_table = bob_count['BOB Count'].value_counts().sort_index().reset_index()\n",
    "# bob_count_freq_table.rename(columns={'index': 'BOB Count', 'BOB Count': 'count'}, inplace=True)\n",
    "# bob_count_freq_table = bob_count_freq_table.reindex(range(len(bob_count_freq_table)), fill_value=0)\n",
    "\n",
    "# bob_count_freq_table.columns.values[0] = 'BOB_Count'\n",
    "# bob_count_freq_table.columns.values[1] = 'No_of_HCPs'\n",
    "\n",
    "# print(\"Frequency Table for BOB Count:\")\n",
    "# print(bob_count_freq_table.to_string(index=False))\n",
    "\n",
    "# this code is calculating the number of times ‘BOB’ appears in the AddressFlagType column for each unique IID, and then creating a \n",
    "# frequency table of these counts. The frequency table shows how many IIDs have each possible count of ‘BOB’.\n",
    "\n",
    "bob_count = temp_ADDRESS[['IID','AddressFlagType']].copy()\n",
    "bob_count['HCPs'] = bob_count['AddressFlagType'].apply(lambda x: 1 if x == 'BOB' else 0)\n",
    "bob_count_grouped = bob_count.groupby('IID')['HCPs'].sum()\n",
    "\n",
    "# Frequency table\n",
    "frequency_table = bob_count_grouped.value_counts().sort_index()\n",
    "print(\"Frequency Table for BOB Count:\")\n",
    "print(frequency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3dc68c-83fc-43a1-98ac-cbb9812dbac8",
   "metadata": {},
   "source": [
    "### BOB filtering notes : \n",
    "#### from my understanding of the sas codes we have 5 cases to consider - \n",
    "* case 1- Only 1 Address, and Count of BOB flag = 1\n",
    "- case 2- More than 1 Address, and Count of BOB flag = 1 [in this case we only keep the addr with bob flag]\n",
    "- case 3- More than 1 Address, and Count of BOB flag = 0 / Null [I.e no BOB flag] , [ In this case we keep the first obs]\n",
    "- case 4- Only 1 Address, and Count of BOB flag = 0 / Null [I.e no BOB flag], [In this case the assumption is the only avaialble address is the best address so we keep it]\n",
    "- case 5- Count of BOB flag grater than 1 , Outlier case , this implys there are multiple addresses with BOB flag Droppinging this case for now\n",
    "Line  \n",
    "*We were NOT assinging BOB to cases with only one addr and no bob flag , contrary to previous understanding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a0119d2-b9e6-4abc-891b-17f14c991abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOB Clean and Dedup by IID\n",
    "temp_test1 = temp_ADDRESS.copy() #creating a new df to do processing for BOB flag filtering\n",
    "\n",
    "#adding 2 flags to temp_test1 for calculation and filtering - \n",
    "Co_Addr = temp_test1.groupby('IID').size()\n",
    "Co_Addr.name = 'Co_Addr' #count of addresses [ we can use hub_id to count number of addresses for each hub_id]\n",
    "\n",
    "Co_BOB = temp_test1[temp_test1['AddressFlagType'] == 'BOB'].groupby('IID').size()\n",
    "Co_BOB.name = 'Co_BOB'\n",
    "\n",
    "temp_test1 = temp_test1.merge(Co_Addr, on='IID', how='left')\n",
    "temp_test1 = temp_test1.merge(Co_BOB, on='IID', how='left')\n",
    "#qc\n",
    "#temp_test1.shape\n",
    "\n",
    "# following are dataframes made by filtering temp_test1\n",
    "# we filter it to create sub components to cater each condition , then we merge them back together\n",
    "\n",
    "temp_test2 = temp_test1[temp_test1['Co_Addr'] == 1].copy() #case 1 , 4\n",
    "temp_test2.reset_index(drop=True, inplace=True) #as long as there is one address keeping it , BOB or not\n",
    "#print(temp_test2.shape)\n",
    "\n",
    "temp_test3 = temp_test1[(temp_test1['Co_Addr'] > 1) & (temp_test1['Co_BOB'].isnull() | (temp_test1['Co_BOB'] == 0))].copy() #case 3\n",
    "temp_test3.reset_index(drop=True, inplace=True)\n",
    "temp_test3.drop_duplicates(subset='IID', keep='first', inplace=True) #only keeping first occurance\n",
    "#print(temp_test3.shape)\n",
    "\n",
    "temp_test4 = temp_test1[(temp_test1['Co_Addr'] > 1) & (temp_test1['Co_BOB'] == 1)].copy() #case 2\n",
    "temp_test4 = temp_test4[temp_test4['AddressFlagType'] == 'BOB'].copy()\n",
    "#print(temp_test4.shape) # only keeping instances with BOB\n",
    "\n",
    "temp_test5 = temp_test1[temp_test1['Co_BOB'] > 1].copy() #case 5 [outlier scenario]\n",
    "#print(temp_test5.shape)\n",
    "\n",
    "# Combining Components here - \n",
    "# Concatenate temp_test2, temp_test3, and temp_test4 into temp_Addr_all\n",
    "temp_Addr_all = pd.concat([temp_test2, temp_test3, temp_test4], ignore_index=True)\n",
    "#print(temp_Addr_all.shape)\n",
    "\n",
    "#Deduping final Filtered Dataset by IID [or IID in our case] -\n",
    "\n",
    "# Keep only the first occurrence of each IID and drop the duplicates\n",
    "temp_ADDRESSNODUP = temp_Addr_all.drop_duplicates(subset='IID', keep='first')\n",
    "temp_ADDRESSNODUP = temp_ADDRESSNODUP.drop(columns=['Co_Addr', 'Co_BOB']) # FINAL DATASET READY\n",
    "\n",
    "# Save the duplicates in a separate DataFrame [This may be a redundant practice carried over from sas]\n",
    "temp_AddressDup = temp_Addr_all[temp_Addr_all.duplicated(subset='IID')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af71127-982b-4d42-bee6-755e1a1ce729",
   "metadata": {},
   "source": [
    "#### Dedup and Clean SPECIALTY -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5cced605-90f6-4070-9bc4-10220cbcfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "curwk_SPECIALTY.loc[curwk_SPECIALTY['SpecialtyEffectiveEndDate'] == '9999-12-31', 'SpecialtyEffectiveEndDate'] = '2262-04-11'\n",
    "# Date Column type change for address -\n",
    "curwk_SPECIALTY['SpecialtyEffectiveStartDate'] = pd.to_datetime(curwk_SPECIALTY['SpecialtyEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_SPECIALTY['SpecialtyEffectiveEndDate'] = pd.to_datetime(curwk_SPECIALTY['SpecialtyEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ce87530-6aee-485b-b767-8a66aea40881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IID dropped :  0\n"
     ]
    }
   ],
   "source": [
    "temp_SPECIALTY = curwk_SPECIALTY.copy()\n",
    "qc_before = len(temp_SPECIALTY)\n",
    "temp_SPECSORT = temp_SPECIALTY.sort_values(by = 'IID').drop_duplicates(subset='IID', keep='first')\n",
    "temp_SPECDUP = temp_SPECIALTY[temp_SPECIALTY.duplicated(subset='IID', keep=False)]\n",
    "qc_after = len(temp_SPECSORT)\n",
    "print(\" IID dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae1a2eb5-03db-4d5f-a3ea-9ae3a8df4afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SpecialtyCode dropped :  0\n"
     ]
    }
   ],
   "source": [
    "# new Specialty Check\n",
    "qc_before = len(temp_SPECIALTY)\n",
    "temp_SPECIALTYSORT = temp_SPECIALTY.sort_values(by = 'SpecialtyCode').drop_duplicates(subset='SpecialtyCode', keep='first')\n",
    "temp_SPECIALTYDUP = temp_SPECIALTY[temp_SPECIALTY.duplicated(subset='SpecialtyCode', keep=False)]\n",
    "qc_after = len(temp_SPECSORT)\n",
    "print(\" SpecialtyCode dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9f85e02a-cfea-43f2-b6ec-38042411ac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 List of Spec_Cds not avialable in qtrspec_SPEC_INCL_LIN : \n",
      "['ASO', 'DAP', 'GLAU', 'OID', 'TY', 'URPS', 'VEC', 'VEN', 'VER', 'VES']\n"
     ]
    }
   ],
   "source": [
    "# new spec check\n",
    "# Fetching Data from stored location - \n",
    "qtrspec_SPEC_INCL_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}qtrspec_SPEC_INCL_LIN.parquet')\n",
    "\n",
    "# Merging with qaterly file to find new specs -\n",
    "temp_CHECK_SPECIALTY = pd.merge(temp_SPECIALTYSORT, qtrspec_SPEC_INCL_LIN, how='left', left_on='SpecialtyCode', right_on='SPECIALTY_CD', indicator=True)\n",
    "# Filter the merged_data using conditions A AND NOT B\n",
    "temp_CHECK_SPECIALTY = temp_CHECK_SPECIALTY[(temp_CHECK_SPECIALTY['_merge'] == 'left_only')]\n",
    "# Drop the _merge column as its not needed\n",
    "temp_CHECK_SPECIALTY.drop('_merge', axis=1, inplace=True)\n",
    "temp_CHECK_SPECIALTY = temp_CHECK_SPECIALTY[['SpecialtyCode']]\n",
    "\n",
    "#Print Results -\n",
    "if temp_CHECK_SPECIALTY.empty:\n",
    "    print(\"No new specialty\")\n",
    "else:\n",
    "    print(\"2.3 List of Spec_Cds not avialable in qtrspec_SPEC_INCL_LIN : \")\n",
    "    print(temp_CHECK_SPECIALTY['SpecialtyCode'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1c15b-19a8-4770-8a77-f60776402005",
   "metadata": {},
   "source": [
    "##### process IDENTIFIERS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b45e911-a055-43af-a62e-fcd2e0b194c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix max end date -\n",
    "curwk_IDENTIFIER.loc[curwk_IDENTIFIER['IdentifierEffectiveEndDate'] == '9999-12-31', 'IdentifierEffectiveEndDate'] = '2262-04-11'\n",
    "curwk_IDENTIFIER['IdentifierEffectiveStartDate'] = pd.to_datetime(curwk_IDENTIFIER['IdentifierEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_IDENTIFIER['IdentifierEffectiveEndDate'] = pd.to_datetime(curwk_IDENTIFIER['IdentifierEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "874f298c-3070-48a6-8bc3-48827050ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPI\n",
    "NPI_CUR_PROC_WK = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'NPI'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierValue'] != \"\"\n",
    "condition4 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "\n",
    "NPI_CUR_PROC_WK = curwk_IDENTIFIER[(condition1 & condition2 & condition3 & ~(condition4))].copy()\n",
    "NPI_CUR_PROC_WK.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "\n",
    "NPI_CUR_PROC_WK.rename(columns={'IdentifierValue': 'NPI_ID','IdentifierEffectiveStartDate':'NPI_StartDate','IdentifierEffectiveEndDate':'NPI_EndDate'}, inplace=True)\n",
    "NPI_CUR_PROC_WK = NPI_CUR_PROC_WK[['IID','NPI_ID','NPI_StartDate','NPI_EndDate']]\n",
    "\n",
    "#Note - Historically Npi Datasets from all weeks are combined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c99643e7-c107-4ec3-a8f0-274f0fcfa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ME\n",
    "temp_IDENTIFIER_ME = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'ME'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "temp_IDENTIFIER_ME = curwk_IDENTIFIER[(condition1 & condition2 & ~(condition3))].copy()\n",
    "\n",
    "temp_IDENTIFIERSORT = temp_IDENTIFIER_ME.copy()\n",
    "temp_IDENTIFIERSORT.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "temp_IDENTIFIERSORT.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c56c1820-5113-4bf5-a80b-304b3bd78f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saw Cases of Records where both ME ID and NPI ID where aviailable for the HCP\n",
    "temp_ME_NPI = temp_IDENTIFIERSORT.merge(NPI_CUR_PROC_WK, how='outer', on='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "57eebb56-73cf-417d-bdfe-b6940232b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taxonomy\n",
    "temp_IDENTIFIER_TAXONOMY = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'TAXONOMY'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "temp_IDENTIFIER_TAXONOMY = curwk_IDENTIFIER[(condition1 & condition2 & ~(condition3))].copy()\n",
    "\n",
    "temp_IDENTIFIER2SORT = temp_IDENTIFIER_TAXONOMY.copy()\n",
    "temp_IDENTIFIER2SORT.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "temp_IDENTIFIER2SORT.reset_index(drop=True, inplace=True)\n",
    "\n",
    "temp_IDENTIFIER2SORT.rename(columns={'IdentifierValue': 'TAXONOMYID'}, inplace=True)\n",
    "temp_IDENTIFIER2SORT = temp_IDENTIFIER2SORT[['IID','TAXONOMYID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7925ffa6-adbb-4129-b3b9-9c44e587de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ME_NPI_TAX = temp_ME_NPI.merge(temp_IDENTIFIER2SORT, how='outer', on='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09475048-1e13-4b1f-af77-1e7bbfe15baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_IDENTIFIERS = pd.concat([NPI_CUR_PROC_WK,temp_IDENTIFIERSORT,temp_IDENTIFIER2SORT],axis = 0)\n",
    "#temp_IDENTIFIERS = pd.concat([temp_IDENTIFIERSORT,temp_IDENTIFIER2SORT],axis = 0)\n",
    "temp_IDENTIFIERS=temp_ME_NPI_TAX.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7553211-3a4b-490e-a897-fa03c8722351",
   "metadata": {},
   "source": [
    "### Creating Final Weekly Master Profile Delta -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee06c668-7122-4c1a-b632-6aa86c3b6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Weekly Delta Customer Master\n",
    "curwk_CUSTOMER_MASTER = pd.DataFrame()\n",
    "\n",
    "# I am assuming that for the case of overlapping columns , we want to keep stuff from Customer Component only\n",
    "# Perform the merge operation ( second line is drop the overlapping columns from right dataset)\n",
    "curwk_CUSTOMER_MASTER = pd.merge(temp_CUSTOMER, temp_ADDRESSNODUP, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]\n",
    "\n",
    "curwk_CUSTOMER_MASTER = pd.merge(curwk_CUSTOMER_MASTER, temp_SPECSORT, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]\n",
    "\n",
    "curwk_CUSTOMER_MASTER = pd.merge(curwk_CUSTOMER_MASTER, temp_IDENTIFIERS, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a09c2867-438c-4a0b-bb87-11b03354e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Changes - \n",
    "\n",
    "# Rename columns and change data types\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER.rename(columns={'PostalCode': 'ZIP', 'SecondaryID': 'IMSID'})\n",
    "curwk_CUSTOMER_MASTER['IMSID'] = curwk_CUSTOMER_MASTER['IMSID'].astype(str)\n",
    "\n",
    "#Coluns that need to be dropped ? :\n",
    "curwk_CUSTOMER_MASTER.drop(columns={'TransactionID' , 'PartnerID' , 'PartnerLoserID' ,'PartnerWinnerID', 'PartnerName'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99ddbb88-02b4-4f94-bf9d-72d9d6f7406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141739, 70)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curwk_CUSTOMER_MASTER.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9161509-7dae-4b3f-9d88-03c9cd75aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp fix for Identifer \n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER.astype({'IdentifierValue': str})\n",
    "#print(curwk_CUSTOMER_MASTER['IdentifierValue'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4efe3adb-acd0-45e5-a7df-322f609d01b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141739, 70)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curwk_CUSTOMER_MASTER.to_parquet(f's3://{bucket}/{curwk}curwk_CUSTOMER_MASTER.parquet')\n",
    "curwk_CUSTOMER_MASTER.shape\n",
    "# #weekly master profile delta ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3e72a-ad4e-48d0-974a-d0d37c13cd91",
   "metadata": {},
   "source": [
    "### 3 - Profiles Delta- Create HCP Universe, Adds, Updates, Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "439acde1-4444-429e-ab9f-fb1ea4d28557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ZIPL QC : ---\n",
      "Total Number of Records : 141739\n",
      "Number of Records with Invalid Zip :  1166\n",
      "NUmber of Records with Invalid Zip and Active Status : 1023\n",
      "Threshhold check 0.721749130443985  This value tells us what percentage of active HCPs in the delta are going to whitespace\n"
     ]
    }
   ],
   "source": [
    "#ZIP length check\n",
    "temp_ZIPL_CHECK = curwk_CUSTOMER_MASTER.copy()\n",
    "temp_ZIPL_CHECK['ZIPL'] = temp_ZIPL_CHECK['ZIP'].str.len() #adding new column for zip length\n",
    "all_rec = len(temp_ZIPL_CHECK)\n",
    "temp_ZIPL_CHECK = temp_ZIPL_CHECK[temp_ZIPL_CHECK['ZIPL'] != 5].copy() #filtering records where zip length is not 5 [non compliant zip code]\n",
    "invalid_zip = len(temp_ZIPL_CHECK)\n",
    "temp_ZIPL_CHECK = temp_ZIPL_CHECK[temp_ZIPL_CHECK['CustomerStatusCode'] == 'Active'].copy() #[then filtering out how many of them are for active hcps]\n",
    "act_invalid_zip = len(temp_ZIPL_CHECK)\n",
    "print(\"--- ZIPL QC : ---\")\n",
    "print(\"Total Number of Records :\",all_rec)\n",
    "print(\"Number of Records with Invalid Zip : \",invalid_zip)\n",
    "print(\"NUmber of Records with Invalid Zip and Active Status :\",act_invalid_zip)\n",
    "print(\"Threshhold check\",(act_invalid_zip/all_rec)*100,\" This value tells us what percentage of active HCPs in the delta are going to whitespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e36c1965-2fc7-4f3c-adbb-350ea5d18c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1134, 70)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATE ADDS, UPDATES, AND MERGES DATASETS -\n",
    "temp_ADDS_UPDATES = pd.DataFrame() #This will hold the New HPCs and Updates\n",
    "curwk_MERGES = pd.DataFrame() #This will hold the Winner / Looser ID HCPs\n",
    "enddate_CUST_ENDDATE_CUR_PROC_WK = pd.DataFrame() #This will hold the end date crossed HCPs\n",
    "\n",
    "#defining conditions - \n",
    "cond1 = curwk_CUSTOMER_MASTER['IronwoodWinnerID'].isna() \n",
    "cond2 = curwk_CUSTOMER_MASTER['IronwoodLoserID'].isna()\n",
    "cond3 = curwk_CUSTOMER_MASTER['CustomerEffectiveEndDate'] < TODAY\n",
    "\n",
    "#applying conditions -\n",
    "curwk_MERGES = curwk_CUSTOMER_MASTER[(~(cond1) & ~(cond2) & cond3)].copy()\n",
    "enddate_CUST_ENDDATE_CUR_PROC_WK = curwk_CUSTOMER_MASTER[((cond1 | cond2) & cond3)].copy()\n",
    "temp_ADDS_UPDATES = curwk_CUSTOMER_MASTER[cond2 & ~(cond3)].copy() #No Value for Winner and Non Endate HCPs\n",
    "\n",
    "#storing\n",
    "curwk_MERGES.to_parquet(f's3://{bucket}/{curwk}curwk_MERGES.parquet')\n",
    "curwk_MERGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c813abae-f13d-48f0-b24d-d648142d9fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139042, 70)\n",
      "(1692333, 70)\n",
      "(1561, 70)\n"
     ]
    }
   ],
   "source": [
    "#Separating Addtions and Updates\n",
    "\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = pd.DataFrame() #this will hold updates\n",
    "temp_HCP_ADDS_CUR_PROC_WK = pd.DataFrame() #this will hold new hcps\n",
    "temp_b =pd.DataFrame()\n",
    "\n",
    "MASTER_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet') # fetching previous week's full master profile\n",
    "ref_pwek_MASTER_PROFILE = pd.DataFrame(MASTER_PROFILE['IID'].copy())\n",
    "#only keeping IIDs (reason being, if they are update cases , we want all the columns to be repalced)\n",
    "\n",
    "#merging the datasets , will filter based on indicator (_merge column)\n",
    "merged_df = pd.merge(ref_pwek_MASTER_PROFILE,temp_ADDS_UPDATES,on='IID',how='outer',indicator=True)\n",
    "\n",
    "#separating\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = merged_df[merged_df['_merge'] == 'both']\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = temp_HCP_UPDATES_CUR_PROC_WK.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "temp_b = merged_df[merged_df['_merge'] == 'left_only']\n",
    "temp_b = temp_b.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "temp_HCP_ADDS_CUR_PROC_WK = merged_df[merged_df['_merge'] == 'right_only']\n",
    "temp_HCP_ADDS_CUR_PROC_WK = temp_HCP_ADDS_CUR_PROC_WK.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "#qc -\n",
    "print(temp_HCP_UPDATES_CUR_PROC_WK.shape)\n",
    "print(temp_b.shape)\n",
    "print(temp_HCP_ADDS_CUR_PROC_WK.shape)\n",
    "\n",
    "#Adding Columns -\n",
    "adds_HCP_ADDS_CUR_PROC_WK = temp_HCP_ADDS_CUR_PROC_WK.copy() \\\n",
    ".assign(RECORD_START_DATE=DATA_DATE_CALENDAR, RECORD_END_DATE=RECORD_END_DATE)\n",
    "\n",
    "updates_HCP_UPDATES_CUR_PROC_WK = temp_HCP_UPDATES_CUR_PROC_WK.copy() \\\n",
    ".assign(RECORD_START_DATE=DATA_DATE_CALENDAR, RECORD_END_DATE=RECORD_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a08ca6-8d69-4fbd-b9c4-57148bfac469",
   "metadata": {},
   "source": [
    "#### CREATE WEEKLY FULL UNIVERSE CUSTOMER MASTER    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "18f87fc5-e036-46a8-863c-81315e78932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_CUSTOMER_MASTER_PRE_PROC_WK = pd.DataFrame() #intermediate dataset[ this has all the exclusions applied -END, UPDATE, AND MERGED IID ]\n",
    "\n",
    "ref_pwek_MASTER_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet') # fetching previous week's full master profile\n",
    "\n",
    "all_exclusions = pd.DataFrame({\n",
    "    'IID': pd.concat([\n",
    "        updates_HCP_UPDATES_CUR_PROC_WK['IID'],\n",
    "        curwk_MERGES['IID'],\n",
    "        enddate_CUST_ENDDATE_CUR_PROC_WK['IID']\n",
    "    ], ignore_index=True).unique()\n",
    "})\n",
    "\n",
    "#ALternative Approach , useing isin() method, but it may not be vaible for very large datasets [DOUBT]\n",
    "#ref_pwek_MASTER_PROFILE = ref_pwek_MASTER_PROFILE[~ref_pwek_MASTER_PROFILE['hub_id'].isin(all_exclusions['hub_id'])]\n",
    "\n",
    "#going with merge + filter for now as it may be more efficient - \n",
    "\n",
    "#merging the datasets , will filter based on indicator (_merge column)\n",
    "merged_df = pd.merge(ref_pwek_MASTER_PROFILE,all_exclusions,on='IID',how='left',indicator=True)\n",
    "temp_CUSTOMER_MASTER_PRE_PROC_WK = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d1bef0be-7364-40c8-b852-7c154df3296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of losers found in the previous week master profile :  1094\n",
      "Instances inisde mergecass where IID = WinnerID\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IID</th>\n",
       "      <th>CustomerContactType</th>\n",
       "      <th>CustomerStatusCode</th>\n",
       "      <th>IMSID</th>\n",
       "      <th>SecondaryIDSourceName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>MiddleName</th>\n",
       "      <th>SuffixName</th>\n",
       "      <th>PrefixName</th>\n",
       "      <th>...</th>\n",
       "      <th>IdentifierType</th>\n",
       "      <th>IdentifierValue</th>\n",
       "      <th>IdentifierStateProvinceCode</th>\n",
       "      <th>IdentifierStatusCode</th>\n",
       "      <th>IdentifierEffectiveStartDate</th>\n",
       "      <th>IdentifierEffectiveEndDate</th>\n",
       "      <th>NPI_ID</th>\n",
       "      <th>NPI_StartDate</th>\n",
       "      <th>NPI_EndDate</th>\n",
       "      <th>TAXONOMYID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [IID, CustomerContactType, CustomerStatusCode, IMSID, SecondaryIDSourceName, FirstName, LastName, MiddleName, SuffixName, PrefixName, DisplayName, PreferredName, FormerName, Gender, BirthYear, DeceasedYear, ProfessionalDesignation, CompanyName, DepartmentName, JobTitle, GraduateSchoolName, GraduationYear, GraduationStateCode, DegreeName, AccountType, AccountSubType, ClassofTradeCode, PDRPOptOutFlag, PDRPOptOutDate, MatchCode, CustomerEffectiveStartDate, CustomerEffectiveEndDate, IronwoodWinnerID, IronwoodLoserID, AddressSourceId, AddressType, AddressLine1, AddressLine2, AddressLine3, AddressLine4, CityName, StateCode, CountryCode, ZIP, AddressSitePhone, AddressSiteEmail, LatitudeNumber, LongitudeNumber, AddressStatusCode, AddressEffectiveStartDate, AddressEffectiveEndDate, AddressFlagType, AddressFlagType_x, SpecialtyRank, SpecialtyCode, SpecialtyDescription, SpecialtyGroupCode, SpecialtyGroupDescription, SpecialtyEffectiveStartDate, SpecialtyEffectiveEndDate, IdentifierType, IdentifierValue, IdentifierStateProvinceCode, IdentifierStatusCode, IdentifierEffectiveStartDate, IdentifierEffectiveEndDate, NPI_ID, NPI_StartDate, NPI_EndDate, TAXONOMYID]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 70 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qc -\n",
    "temp_CHECK_MERGE = pd.DataFrame()\n",
    "ref_pwek_MASTER_PROFILE = pd.DataFrame(ref_pwek_MASTER_PROFILE['IID'].copy())\n",
    "\n",
    "temp_CHECK_MERGE = pd.merge(ref_pwek_MASTER_PROFILE,(pd.DataFrame(curwk_MERGES['IID'])),on='IID',how='outer',indicator=True)\n",
    "temp_CHECK_MERGE = temp_CHECK_MERGE[temp_CHECK_MERGE['_merge'] == 'right_only']\n",
    "print(\"# of losers found in the previous week master profile : \",len(temp_CHECK_MERGE))\n",
    "print(\"Instances inisde mergecass where IID = WinnerID\")\n",
    "df_equal  = curwk_MERGES[curwk_MERGES['IID'] == curwk_MERGES['IronwoodWinnerID']]\n",
    "df_equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f34e0b76-4a06-4766-9d90-d52d01c86604",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = pd.DataFrame()\n",
    "\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = pd.concat([adds_HCP_ADDS_CUR_PROC_WK,\n",
    "                                              updates_HCP_UPDATES_CUR_PROC_WK,\n",
    "                                              temp_CUSTOMER_MASTER_PRE_PROC_WK])\n",
    "\n",
    "# Reset the index\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = temp_CUSTOMER_MASTER_CUR_PROC_WK.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e42c5a28-5134-4dbd-b52c-383a964340a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records dropped: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1832896, 72)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort | Drop Dups | Store Data\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK.sort_values('IID',inplace=True)\n",
    "ref_week_MASTER_PROFILE = temp_CUSTOMER_MASTER_CUR_PROC_WK.drop_duplicates('IID')\n",
    "num_duplicates_dropped = len(temp_CUSTOMER_MASTER_CUR_PROC_WK) - len(ref_week_MASTER_PROFILE)\n",
    "print(\"Number of duplicate records dropped:\", num_duplicates_dropped)\n",
    "\n",
    "\n",
    "#File ready -\n",
    "MASTER_PROFILE = ref_week_MASTER_PROFILE.copy()\n",
    "MASTER_PROFILE.to_parquet(f's3://{bucket}/{ref_week}MASTER_PROFILE.parquet')\n",
    "MASTER_PROFILE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9e035-8616-4060-9ab8-f8a562c5918a",
   "metadata": {},
   "source": [
    "#### Duplicate Specialty Descriptions Check (3.2) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4425c8f9-50d3-4c9f-9e29-69b62971dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Specialties: Index(['ABDOMINAL', 'CARDIOVASC', 'CERTIFIED', 'INTERNAL M', 'PEDIATRIC'], dtype='object', name='SpecialtyDescription')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecialtyCode</th>\n",
       "      <th>SpecialtyDescription</th>\n",
       "      <th>SPECIALTY_CD</th>\n",
       "      <th>SPECIALTY_DESCRIPTION</th>\n",
       "      <th>SPEC_INCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS</td>\n",
       "      <td>ABDOMINAL</td>\n",
       "      <td>AS</td>\n",
       "      <td>ABDOMINAL SURGERY</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>AR</td>\n",
       "      <td>ABDOMINAL</td>\n",
       "      <td>AR</td>\n",
       "      <td>ABDOMINAL RADIOLOGY</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CD</td>\n",
       "      <td>CARDIOVASC</td>\n",
       "      <td>CD</td>\n",
       "      <td>CARDIOVASCULAR DISEASE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>CDS</td>\n",
       "      <td>CARDIOVASC</td>\n",
       "      <td>CDS</td>\n",
       "      <td>CARDIOVASCULAR SURGERY</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>CNM</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>CNM</td>\n",
       "      <td>CERTIFIED NURSE MIDWIFE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>CNA</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>CNA</td>\n",
       "      <td>CERTIFIED NURSE ANESTHETIST</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MP</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MP</td>\n",
       "      <td>INTERNAL MEDICINE-PSYCHIATRY</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>IFP</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IFP</td>\n",
       "      <td>INTERNAL MEDICINE/FAMILY PRACTICE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>MPM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MPM</td>\n",
       "      <td>INTERNAL MEDICINE/REHABILITAION</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>IPM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IPM</td>\n",
       "      <td>INTERNAL MEDICINE-PREVENTIVE MEDICINE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>MEM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MEM</td>\n",
       "      <td>INTERNAL MED-EMERGENCY MED</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>IEC</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IEC</td>\n",
       "      <td>INTERNAL MED/EMERGENCY MED/CRITICAL CARE MED (...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>IMD</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IMD</td>\n",
       "      <td>INTERNAL MEDICINE/DERMATOLOGY (RESIDENTS ONLY)</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCP</td>\n",
       "      <td>PEDIATRIC</td>\n",
       "      <td>CCP</td>\n",
       "      <td>PEDIATRIC CRITICAL CARE MEDICINE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>PDS</td>\n",
       "      <td>PEDIATRIC</td>\n",
       "      <td>PDS</td>\n",
       "      <td>PEDIATRIC SURGERY (SURGERY)</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SpecialtyCode SpecialtyDescription SPECIALTY_CD  \\\n",
       "1              AS            ABDOMINAL           AS   \n",
       "357            AR            ABDOMINAL           AR   \n",
       "6              CD           CARDIOVASC           CD   \n",
       "246           CDS           CARDIOVASC          CDS   \n",
       "316           CNM            CERTIFIED          CNM   \n",
       "317           CNA            CERTIFIED          CNA   \n",
       "247            MP           INTERNAL M           MP   \n",
       "248           IFP           INTERNAL M          IFP   \n",
       "288           MPM           INTERNAL M          MPM   \n",
       "342           IPM           INTERNAL M          IPM   \n",
       "343           MEM           INTERNAL M          MEM   \n",
       "365           IEC           INTERNAL M          IEC   \n",
       "371           IMD           INTERNAL M          IMD   \n",
       "4             CCP            PEDIATRIC          CCP   \n",
       "42            PDS            PEDIATRIC          PDS   \n",
       "\n",
       "                                 SPECIALTY_DESCRIPTION SPEC_INCL  \n",
       "1                                    ABDOMINAL SURGERY         N  \n",
       "357                                ABDOMINAL RADIOLOGY         Y  \n",
       "6                               CARDIOVASCULAR DISEASE         Y  \n",
       "246                             CARDIOVASCULAR SURGERY         N  \n",
       "316                            CERTIFIED NURSE MIDWIFE         Y  \n",
       "317                        CERTIFIED NURSE ANESTHETIST         N  \n",
       "247                       INTERNAL MEDICINE-PSYCHIATRY         Y  \n",
       "248                  INTERNAL MEDICINE/FAMILY PRACTICE         Y  \n",
       "288                    INTERNAL MEDICINE/REHABILITAION         N  \n",
       "342              INTERNAL MEDICINE-PREVENTIVE MEDICINE         Y  \n",
       "343                         INTERNAL MED-EMERGENCY MED         Y  \n",
       "365  INTERNAL MED/EMERGENCY MED/CRITICAL CARE MED (...         Y  \n",
       "371     INTERNAL MEDICINE/DERMATOLOGY (RESIDENTS ONLY)         Y  \n",
       "4                     PEDIATRIC CRITICAL CARE MEDICINE         Y  \n",
       "42                         PEDIATRIC SURGERY (SURGERY)         N  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_spec_cds = ref_week_MASTER_PROFILE[['SpecialtyCode', 'SpecialtyDescription']].copy()#get the columns i need for master profile\n",
    "unique_spec_cds.drop_duplicates(subset='SpecialtyCode', keep='first', inplace=True)#drop drop all duplicates so i only have unique speciality codes\n",
    "unique_spec_cds.reset_index(drop=True, inplace=True) #reset index\n",
    "unique_spec_cds_incl = pd.merge(unique_spec_cds,qtrspec_SPEC_INCL_LIN,left_on ='SpecialtyCode',right_on='SPECIALTY_CD',how='left') # joining so i get the spec_incl  flag\n",
    "# Group by SPECIALTY_DESCRIPTION and count unique spec_incl values\n",
    "specialty_counts = unique_spec_cds_incl.groupby('SpecialtyDescription')['SPEC_INCL'].nunique()\n",
    "invalid_specialties = specialty_counts[specialty_counts > 1].index\n",
    "# Display the invalid specialties\n",
    "print(\"Invalid Specialties:\",invalid_specialties)\n",
    "qc_df_print = unique_spec_cds_incl[unique_spec_cds_incl['SpecialtyDescription'].isin(invalid_specialties)].copy()\n",
    "qc_df_print.sort_values(by='SpecialtyDescription',inplace=True)\n",
    "qc_df_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2097701-d3d7-4f16-b255-7ca3f3c3b677",
   "metadata": {},
   "source": [
    "#### REFERENCE DATE PARAMETERS DATA SETS CREATION  -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bb1ef857-6439-4190-b708-d22277021fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>CALENDAR</th>\n",
       "      <th>DATADATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XPONENT</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALLS</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENTS</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MASTER PROFILE</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATA    CALENDAR    DATADATE\n",
       "0         XPONENT  2024-07-01  2024-06-14\n",
       "1           CALLS  2024-07-01  2024-06-28\n",
       "2          EVENTS  2024-07-01  2024-06-28\n",
       "3  MASTER PROFILE  2024-07-01  2024-07-01"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'DATA': ['XPONENT', 'CALLS', 'EVENTS', 'MASTER PROFILE'],\n",
    "    'CALENDAR': [DATA_DATE_CALENDAR] * 4,\n",
    "    'DATADATE': [DATA_DATE_WEEKLY,EVTS_DATE_WEEKLY,EVTS_DATE_WEEKLY,DATA_DATE_CALENDAR]\n",
    "}\n",
    "\n",
    "ref_week_DATE_PARM = pd.DataFrame(data)\n",
    "ref_week_DATE_PARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fec62394-a950-4ed8-848f-9d1a294b758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>CALENDAR</th>\n",
       "      <th>DATADATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XPONENT</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALLS</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENTS</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MASTER PROFILE</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATA    CALENDAR    DATADATE\n",
       "0         XPONENT  2024-07-01  2024-06-14\n",
       "1           CALLS  2024-07-01  2024-06-14\n",
       "2          EVENTS  2024-07-01  2024-06-14\n",
       "3  MASTER PROFILE  2024-07-01  2024-07-01"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'DATA': ['XPONENT', 'CALLS', 'EVENTS', 'MASTER PROFILE'],\n",
    "    'CALENDAR': [DATA_DATE_CALENDAR] * 4,\n",
    "    'DATADATE': [DATA_DATE_MONTHLY,DATA_DATE_MONTHLY,DATA_DATE_MONTHLY,DATA_DATE_CALENDAR]\n",
    "}\n",
    "\n",
    "ref_mnth_DATE_PARM = pd.DataFrame(data)\n",
    "ref_mnth_DATE_PARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3bda311a-c8f0-4f5c-8cb4-184300b064d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_week_DATE_PARM.to_parquet(f's3://{bucket}/{ref_week}DATE_PARM.parquet')\n",
    "ref_mnth_DATE_PARM.to_parquet(f's3://{bucket}/{ref_month}DATE_PARM.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fc6c2097-1961-4d33-a5dc-619c2e7a5ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15004"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DELETEING non used Dfs?\n",
    "del temp_CUSTOMER_MASTER_CUR_PROC_WK\n",
    "del ref_week_MASTER_PROFILE\n",
    "del merged_df\n",
    "del temp_CUSTOMER_MASTER_PRE_PROC_WK\n",
    "del temp_ADDRESS1_F\n",
    "del temp_ADDRESS1\n",
    "del temp_test1\n",
    "del temp_ADDRESS\n",
    "del curwk_ADDRESS\n",
    "del temp_b\n",
    "del curwk_CUSTOMER_MASTER\n",
    "del updates_HCP_UPDATES_CUR_PROC_WK\n",
    "del temp_ADDS_UPDATES\n",
    "del temp_HCP_UPDATES_CUR_PROC_WK\n",
    "del temp_ADDRESSFLAG1\n",
    "del curwk_COMMUNICATION\n",
    "del temp_COMMUNICATION1\n",
    "del temp_CUSTOMER1\n",
    "del temp_CUSTOMER\n",
    "del curwk_CUSTOMER\n",
    "del curwk_DEA\n",
    "del temp_Addr_all\n",
    "del temp_DEA1\n",
    "del temp_ADDRESSNODUP\n",
    "del temp_test4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cda2b-9f13-414e-9c4f-349c5ef84c6e",
   "metadata": {},
   "source": [
    "### Net New HCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4be4011b-f2a2-4905-be6f-182adab2c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Week Full Master Profile - MASTER_PROFILE (Already Loaded in memory)\n",
    "CUR_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_week}MASTER_PROFILE.parquet')\n",
    "# Previous Week Master Profile\n",
    "PRE_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet')\n",
    "\n",
    "# Perform the merge using pd.merge\n",
    "NET_NEW = pd.merge(CUR_PROFILE, PRE_PROFILE['IID'],on='IID', how='left', indicator=True)\n",
    "NET_NEW = NET_NEW[NET_NEW['_merge'] == 'left_only']\n",
    "NET_NEW = NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "#adding week timestamp \n",
    "NET_NEW['SOURCE'] = CUR_WK  #using cur_wk_xpn_date but in code  -\"&CUR_WK.\" is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f1325-3509-4600-b3fe-86499933f2e9",
   "metadata": {},
   "source": [
    "### Appending net new to create rolling file\n",
    "- the week after frozen week , the net new file is reset\n",
    "- otherwise, net new files gets accumilated by using previous week net new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "490850d8-0fe1-4c31-b66a-9c448f9bd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if WEEK1 == 1 then dont append to previous week net new , else append.\n",
    "WEEK1 = 0 #DONT CHANGE THIS  , You have to manually reset net net for a new Qtr in a separate code , reuse the code there\n",
    "NET_NEW_ACCU = pd.DataFrame()\n",
    "\n",
    "net_new_varname = \"NET_NEW_{}\".format(QTR_NET)\n",
    "\n",
    "if WEEK1 == 1:\n",
    "    NET_NEW_ACCU = NET_NEW.copy()\n",
    "elif WEEK1 == 0:\n",
    "    globals()[net_new_varname] = pd.read_parquet(f's3://{bucket}/{ref_pwek}{net_new_varname}.parquet')\n",
    "    columns_to_drop = ['Territory_IW1', 'Territory_Name_IW1','Region', \n",
    "                   'Region_Name', 'Area','Area_Name','SPECIALTY_DESCRIPTION',\n",
    "                   'SPEC_INCL_LIN','IC_INCL_LIN'] \n",
    "    globals()[net_new_varname] = globals()[net_new_varname].drop(columns=columns_to_drop)\n",
    "    NET_NEW_ACCU = pd.concat([NET_NEW, globals()[net_new_varname]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbee48-4bb7-462f-8c33-5e79291ae73b",
   "metadata": {},
   "source": [
    "#### Assigning Terr information from Zip to Terr\n",
    "- Addtionally, using the quarterly Speciality Inclusion Exclusion list to get inclusion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "075f8b69-6025-4986-a2e1-2d14f8f37b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_TO_TERR = pd.read_parquet(f's3://{bucket}/{ZIP}zip_to_terr.parquet')\n",
    "ZIP_TO_TERR.rename(columns={'Territory':'Territory_IW1','Territory_Name':'Territory_Name_IW1'},inplace=True) #DOUBT not sure why we do this\n",
    "SPEC_INCL_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}qtrspec_SPEC_INCL_LIN.parquet')\n",
    "#Extra\n",
    "SPEC_INCL_LIN.rename(columns={'SPECIALTY_CD':'SpecialtyCode'},inplace=True)\n",
    "SPEC_INCL_LIN.set_index('SpecialtyCode', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4da13148-887e-43ac-b66b-b8b16e610b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting terr name, region name, area name from zip to terr\n",
    "NET_NEW_FN = pd.merge(NET_NEW_ACCU,ZIP_TO_TERR,left_on='ZIP',right_on='Zip',how='left')\n",
    "whitespace_values = {\n",
    "    'Territory_IW1': \"1111-99999-99\",\n",
    "    'Territory_Name_IW1': \"\",\n",
    "    'Region': \"1111-999\",\n",
    "    'Region_Name': \"\",\n",
    "    'Area': \"1111-9\",\n",
    "    'Area_Name': \"\"\n",
    "}\n",
    "# Fill whitespace_values with the default values\n",
    "NET_NEW_FN.fillna(whitespace_values, inplace=True)\n",
    "\n",
    "#getting speciality flags from spec incl lin quaterly file -[NOTE : BUG FIX SUMMARY - sas did not perform a left join, revised code to perfrom mapping strategy] \n",
    "NET_NEW_FN['SPECIALTY_DESCRIPTION'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPECIALTY_DESCRIPTION'])\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPEC_INCL'])\n",
    "NET_NEW_FN.fillna({'SPECIALTY_DESCRIPTION':''},inplace=True)\n",
    "\n",
    "#adding new column SPEC_INCL_LIN (this is based on match code, so ill have to wait for it to come.- NOTE  Condition is only applied on failed left matches\n",
    "#NET_NEW_FN['SPEC_INCL_LIN'] = ['N' if x == '01' else 'Y' for x in NET_NEW_FN['MatchCode']] - This is wrong\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = np.where(NET_NEW_FN['SPEC_INCL_LIN'].isna(),\n",
    "                                        np.where(NET_NEW_FN['MatchCode'] == '01', 'N', 'Y'),\n",
    "                                        NET_NEW_FN['SPEC_INCL_LIN'])\n",
    "\n",
    "#dropping extra columns (different name , same data)\n",
    "NET_NEW_FN.drop(['Zip'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198694e-b48a-4ebb-b4ce-32e015a3d847",
   "metadata": {},
   "source": [
    "### Filtering for any legal removal and unknown address HCPs\n",
    "- 'Jami' file is used here (double check import location and check for file updates on quarter change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3329662e-3c26-4f48-b174-24ed14c8d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying exclusions\n",
    "JAMI_INCLEXCL = pd.read_parquet(f's3://{bucket}/{jami}jami_inclexcl.parquet')\n",
    "jami_exclusions = pd.DataFrame()\n",
    "jami_exclusions = JAMI_INCLEXCL[JAMI_INCLEXCL['TYPE'].isin(['Legal Removals', 'Unknown Address'])]\n",
    "jami_exclusions = jami_exclusions[['IID']].copy()\n",
    "jami_exclusions.drop_duplicates(subset='IID', inplace=True)\n",
    "jami_exclusions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ref_week_NET_NEW = pd.merge(NET_NEW_FN,jami_exclusions,on = 'IID',how='left',indicator=True)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[ref_week_NET_NEW['_merge'] == 'left_only']\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bab82-b747-496f-beac-59527f08776a",
   "metadata": {},
   "source": [
    "#### IC flag creation\n",
    "- IC_INCL_LIN is a an important flag which decides if a hcp should be considered for IC or not\n",
    "- CustomerEffectiveStartDate , SPEC_INCL_LIN, CustomerStatusCode, MatchCode decide its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2b5cbb81-20ff-4057-8f4b-76644bf9fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43433, 82)\n"
     ]
    }
   ],
   "source": [
    "ref_week_NET_NEW['IC_INCL_LIN'] = 0  # Initialize the column with 0\n",
    "\n",
    "condition = (\n",
    "    (ref_week_NET_NEW['CustomerEffectiveStartDate'] < CUT_OFF_net) &\n",
    "    (ref_week_NET_NEW['SPEC_INCL_LIN'] == \"Y\") &\n",
    "    (ref_week_NET_NEW['CustomerStatusCode'] == \"Active\") &\n",
    "    (ref_week_NET_NEW['MatchCode'] != '01')\n",
    ")\n",
    "\n",
    "ref_week_NET_NEW.loc[condition, 'IC_INCL_LIN'] = 1\n",
    "print(ref_week_NET_NEW.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0f319-f684-4bdb-8fa9-9c1c381d1bd3",
   "metadata": {},
   "source": [
    "#### Importing IIDs from frozen IC universe , and deleting any occurances of those in net new as an added QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0b019038-d6a8-4ae1-acd0-a4f0e87d4806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped from ic uni:  0\n"
     ]
    }
   ],
   "source": [
    "CUSTOMER_MASTER_IC_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}CUSTOMER_MASTER_IC_LIN.parquet')\n",
    "CUSTOMER_MASTER_IC_LIN = pd.DataFrame(CUSTOMER_MASTER_IC_LIN['IID'])\n",
    "#Faulty Approach - needs checking into : Try using - ref_week_NET_NEW = merged_df[merged_df['_merge'] == 'left_only']\n",
    "# incase of memory issues ?\n",
    "# merged_df = pd.merge(ref_week_NET_NEW, CUSTOMER_MASTER_IC_LIN[['IID']], on='IID', how='inner')\n",
    "# ref_week_NET_NEW = ref_week_NET_NEW.drop(merged_df.index)\n",
    "#updated approach - \n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[~ref_week_NET_NEW['IID'].isin(CUSTOMER_MASTER_IC_LIN['IID'])]\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped from ic uni: \",qc_bef-qc_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "63813f12-3928-42c9-accb-2f3e65a89b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped :  0\n"
     ]
    }
   ],
   "source": [
    "#Only Keep the first Instance of IID Source.\n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW.sort_values(by=['IID', 'SOURCE'], inplace=True)\n",
    "ref_week_NET_NEW.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped : \",qc_bef-qc_af)\n",
    "\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.sort_values(['IID'])\n",
    "ref_week_NET_NEW.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#sdap here\n",
    "ref_week_NET_NEW.to_parquet(f's3://{bucket}/{ref_week}{net_new_varname}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e223e3b-7a8a-470c-b247-38b1615ac869",
   "metadata": {},
   "source": [
    "#### QC | Frequency Table to QC number of records , OC flags , date of addition etc\n",
    "- Carry over from sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69c5fa8d-7ff1-48f7-a7d9-5de4ba1b2250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SOURCE  COUNT\n",
      "3   20240202   2496\n",
      "10  20240209   1914\n",
      "8   20240216   2041\n",
      "12  20240223   1868\n",
      "5   20240301   2236\n",
      "6   20240308   2184\n",
      "9   20240315   1927\n",
      "7   20240322   2155\n",
      "1   20240329   3148\n",
      "0   20240405   5682\n",
      "4   20240412   2297\n",
      "13  20240419   1654\n",
      "2   20240426   2743\n",
      "18  20240503   1503\n",
      "17  20240510   1523\n",
      "14  20240517   1650\n",
      "15  20240524   1626\n",
      "11  20240531   1895\n",
      "19  20240607   1330\n",
      "16  20240614   1561\n"
     ]
    }
   ],
   "source": [
    "#17.1: Check current week's net_new source\n",
    "# Perform the frequency count\n",
    "freq_counts = ref_week_NET_NEW['SOURCE'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['SOURCE', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('SOURCE')\n",
    "\n",
    "# Print the frequency counts\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ab9f8092-623a-4d41-b2e6-adc297d977ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IC_INCL_LIN  COUNT\n",
      "0            1  27092\n",
      "1            0  16341\n"
     ]
    }
   ],
   "source": [
    "#17.2: Check IC_INCL_LIN - this wont work yet because flag is not there (match code not avilable)\n",
    "freq_counts = ref_week_NET_NEW['IC_INCL_LIN'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['IC_INCL_LIN', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('COUNT', ascending=False)\n",
    "\n",
    "# Print the frequency counts #NOTE : last check on 27-12-2023 , 16 people with IC flag 0 are lesser in python compared to sas presumably Jami ppl\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ff87fc41-674b-4b44-adb7-6d8f80b565db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcp_count 43433\n",
      "distinct_hcp_count 43433\n"
     ]
    }
   ],
   "source": [
    "# 17.3: Check Net New for Dups\n",
    "print(\"hcp_count\",ref_week_NET_NEW['IID'].count())\n",
    "print(\"distinct_hcp_count\",ref_week_NET_NEW['IID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "176c1f15-af1d-417f-89d0-3bb61ecc4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO/\n",
    "# Check for Net New NOBS COUNT - DONE\n",
    "# Impliment SAS method to only keep Oldest Record - DONE\n",
    "# Apply QC that checks against frozen MP - DONE\n",
    "# Export Dataframe - DONE\n",
    "# Remove Redundant Cells and Time - DONE\n",
    "# check why speciality rank has 2 columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
