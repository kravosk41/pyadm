{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad754703-5d84-4384-b7f2-9536c3f27787",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f5dcc9-bdfb-4aa5-b601-67cf30ba7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8d7e0-8015-4481-865b-543d4c596d63",
   "metadata": {},
   "source": [
    "Date Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "618ccfdd-c773-4cf4-b282-048facfd3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables -\n",
    "this_day = datetime.today()\n",
    "# ### FOR TESTING - REMOVE LATER ### \n",
    "# this_day = this_day - timedelta(days=7)\n",
    "# ####\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")\n",
    "\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")\n",
    "\n",
    "month_end_date = CUR_WK0.replace(day=1) - timedelta(days=1)\n",
    "if 0 <= (CUR_WK0 - month_end_date).days < 7: # When its Month Ending (we use MONTH OFF to start subtracting)\n",
    "    CCYYMM_OFF = CUR_WK0.replace(day=1).strftime(\"%Y%m\")\n",
    "\n",
    "else: #Not Month ending\n",
    "    CCYYMM_OFF = (CUR_WK0.replace(day=1) + relativedelta(months=1)).strftime(\"%Y%m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd29e09-04ab-49f4-a013-413f10fa335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'vortex-staging-a65ced90'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936a099-93c2-4a12-9a35-8d6402624d70",
   "metadata": {},
   "source": [
    "Library names and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f139ac5e-56f3-475d-b273-a2817bed7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "prewk = f'PYADM/raw/{PRE_PROC_WK}/dataframes/'\n",
    "wkxpn = 'PYADM/weekly/staging/xponent/'\n",
    "mthxpn = 'PYADM/monthly/staging/xponent/'\n",
    "pwkxpn = f'PYADM/weekly/archive/{PRE_WK}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bb72ce-64a8-46bb-8166-7276f906b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking Up Data from Rx_2\n",
    "# monthly -\n",
    "mthxpn_LAX_N = pl.read_parquet(f's3://{bucket}/{curwk}mthxpn_LAX_N.parquet') #Source for this is subject to change , may add full version in future?\n",
    "mthxpn_LAX_N = mthxpn_LAX_N.filter(mthxpn_LAX_N['CCYYMM'] != pl.lit(CCYYMM_OFF))\n",
    "#Dropping rows where product is null\n",
    "mthxpn_LAX_N = mthxpn_LAX_N.filter(mthxpn_LAX_N['PROD_CD'] != \"\")\n",
    "date_parm_mth = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_MTH.parquet')\n",
    "date_parm_mth = date_parm_mth.drop('DATE_AS_OF')\n",
    "LAX_N_1 = mthxpn_LAX_N.join(date_parm_mth,on='CCYYMM',how='inner')\n",
    "LAX_N_1 = LAX_N_1.drop(['MKT_CD','MarketName','G_B','PFAM_NAME','PROD_NAME','WK_END_DATE','RO_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ab83fe-cba5-4669-b837-51271ffdbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATE - 3% data had a period key outside date range , WHY ?\n",
    "#             - Number of HCPs same or not? [ Chunks were 100 instead of 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd5a89a-7c4a-4d68-9a85-615f83bdeaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['TRX','NRX','TUN','NUN','TUF','NUF']\n",
    "# Adding a new column called PROD_WK , will contain PROD_CD and the week number of transaction\n",
    "# this the column on which we transpose the data  \n",
    "df_with_new_cols = LAX_N_1.with_columns([(pl.col(\"PROD_CD\").cast(pl.Utf8) + \"P_\" + pl.col(\"I\").cast(pl.Utf8)).alias(\"PROD_MT\")])\n",
    "\n",
    "# Since PROD_WK now has prod and week info, we dont these columns\n",
    "df_dropped = df_with_new_cols.drop(['PFAM_CD','PROD_CD','I'])\n",
    "\n",
    "# Sorting data at IID level to chunk and filter effectively\n",
    "df_sorted = df_dropped.sort('IID')\n",
    "\n",
    "#For memory protection\n",
    "del LAX_N_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01251a36-7952-47c6-899e-8ee3b45c37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_unique_vals' contains a list of all possible column names after transposing data\n",
    "# we will use it to standardize the shape of every chunk, as it will help us concat / stack them each iteration\n",
    "\n",
    "unique_vals = list(df_sorted['PROD_MT'].unique()) #using full data to get all unique values here\n",
    "# NOTE : It might very well be possible that some weeks of data may be missing, we might have to add those columns manually at some point\n",
    "\n",
    "full_unique_vals = []\n",
    "def unique_vals_prod_wk(col_name):   #this function breaks down PROD_WK to your regular column names like LI1PTUF\n",
    "    parts = col_name.split('_')\n",
    "    for m in metrics:\n",
    "        full = parts[0]+m+parts[-1]\n",
    "        full_unique_vals.append(full)\n",
    "    \n",
    "for i in unique_vals:\n",
    "    unique_vals_prod_wk(i)\n",
    "\n",
    "# Could add a modifier here to check and have full 105 weeks of data ?\n",
    "    \n",
    "full_unique_vals.sort()\n",
    "full_unique_vals.insert(0,'IID') #Adding IID because i will also use this list to standardize the order of columns in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e508e303-0ffe-46ab-af7d-a5b61c2958fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products in data which do not have 24 months of data :  1\n",
      "['ZEL']\n",
      "shape: (1, 2)\n",
      "┌──────┬─────────────┐\n",
      "│ prod ┆ num_of_mnts │\n",
      "│ ---  ┆ ---         │\n",
      "│ str  ┆ u32         │\n",
      "╞══════╪═════════════╡\n",
      "│ ZEL  ┆ 14          │\n",
      "└──────┴─────────────┘\n",
      "BUT !  - \n",
      "ZEL  has gaps in weeks\n"
     ]
    }
   ],
   "source": [
    "mt_tst = pl.DataFrame()\n",
    "mt_tst = mt_tst.with_columns(pl.Series(name='col_names_raw',values=full_unique_vals[1:]))\n",
    "\n",
    "def split_col_names(value):\n",
    "    prod = value[:3]\n",
    "    metric = value[4:7]\n",
    "    mtnum = value[7:]\n",
    "    return prod, metric, mtnum\n",
    "\n",
    "mt_tst = mt_tst.with_columns([pl.col(\"col_names_raw\").map_elements(split_col_names, return_dtype=pl.Object).alias(\"split_values\")])\n",
    "\n",
    "mt_tst = mt_tst.with_columns([\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[0], return_dtype=pl.Utf8).alias(\"prod\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[1], return_dtype=pl.Utf8).alias(\"metric\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[2], return_dtype=pl.Utf8).alias(\"mtnum\"),\n",
    "])\n",
    "mt_tst = mt_tst.drop([\"split_values\",\"col_names_raw\"])\n",
    "\n",
    "res = mt_tst.group_by(['prod','metric']).agg([pl.col('mtnum').n_unique().alias('num_of_mnts')])\n",
    "missing_mtnum = res.filter(pl.col('num_of_mnts') != 24)\n",
    "missing_mtnum = missing_mtnum.sort(by='prod')\n",
    "print(\"Number of products in data which do not have 24 months of data : \",len(missing_mtnum['prod'].unique()))\n",
    "print(list((missing_mtnum['prod'].unique())))\n",
    "missmps = list((missing_mtnum['prod'].unique()))\n",
    "missing_mtnum_print = missing_mtnum.select(pl.col(['prod','num_of_mnts']))\n",
    "missing_mtnum_print = missing_mtnum_print.unique(subset=['prod','num_of_mnts'])\n",
    "print(missing_mtnum_print)\n",
    "\n",
    "wk_conti = pl.DataFrame()\n",
    "wk_conti = mt_tst.filter(pl.col('prod').is_in(missmps))\n",
    "wk_conti = wk_conti.drop('metric')\n",
    "wk_conti = wk_conti.unique(subset=['prod','mtnum'])\n",
    "wk_conti = wk_conti.with_columns(pl.col(\"mtnum\").cast(pl.Int32))\n",
    "\n",
    "print('BUT !  - ')\n",
    "for prod in missmps:\n",
    "    f1 = wk_conti.filter(pl.col('prod')== prod )\n",
    "    if (len(f1['mtnum'].unique()) != f1['mtnum'].max()):\n",
    "        print(prod,\" has gaps in weeks\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164789f0-bff8-4b57-b627-d473b5784792",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iids = df_sorted['IID'].unique() #(This will be the number of rows in final result)\n",
    "chunk_size = 30000 #Each chunk will contain 5000 HCPs worth of transactions (NOT ROWS, they may differ each chunk)\n",
    "\n",
    "iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]\n",
    "#So IID_chunks is a list of lists, each list contains 5000 HCPs and number of lists is our number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6e09483-8abc-4885-9f21-17c003dbe803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:32<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "writer = None  #this is used by pyarrow to write data in chunks to external file\n",
    "df_final2 = pl.DataFrame()  #so, df_final2 will be the object holding final data and being used for exporting\n",
    "loop_counter = 0 #just for utility\n",
    "\n",
    "for iid_chunk in tqdm(iid_chunks):\n",
    "\n",
    "    df_chunk = df_sorted.filter(pl.col('IID').is_in(iid_chunk))\n",
    "    df_pivot_chunk = df_chunk.pivot(values=metrics,index='IID',columns='PROD_MT',maintain_order=True,sort_columns=True)\n",
    "    df_pivot_chunk = df_pivot_chunk.select(pl.all().name.map(lambda col_name: col_name.split('_')[3] + col_name.split('_')[0] + col_name.split('_')[-1] if 'PROD_MT_' in col_name else col_name))\n",
    "\n",
    "    missing_cols = [col for col in full_unique_vals if col not in df_pivot_chunk.columns]\n",
    "    for col in missing_cols: # This Takes 5 Seconds\n",
    "        null_series = pl.Series(col, [None]*len(df_pivot_chunk), dtype=pl.Float64)\n",
    "        df_pivot_chunk = df_pivot_chunk.with_columns(null_series)\n",
    "\n",
    "    df_pivot_chunk = df_pivot_chunk.select(full_unique_vals)\n",
    "\n",
    "    df_final2 = df_final2.vstack(df_pivot_chunk)\n",
    "\n",
    "    if loop_counter % 10 == 0 and loop_counter != 0: # THIS CONTROLLS HOW MANY CHUNKS TO APPEND BEFORE WRITING\n",
    "        table = df_final2.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(curwk+'\\\\df_final2.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{curwk}df_final2.parquet',table.schema)\n",
    "        writer.write_table(table)\n",
    "        df_final2 = pl.DataFrame() # Reset df_final2 after writing to file\n",
    "\n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(df_final2) > 0:\n",
    "    table = df_final2.to_arrow()\n",
    "    if writer is None:\n",
    "        #writer = pq.ParquetWriter(curwk+'\\\\df_final2.parquet', table.schema)\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{curwk}df_final2.parquet',table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b708e9d-9d72-49f6-a15e-2315f2dfcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_family_market_buckets = {\n",
    "    \"MRXF\" : [\"MRGP\",\"MRBP\",\"GLYP\"],\n",
    "    \"LINF\" : [\"LI1P\",\"LI2P\",\"LI3P\"],\n",
    "    \"LUBF\" : [\"AMTP\",\"LUBP\"],\n",
    "    \"GENM\" : [\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"GLYP\"],\n",
    "    \"BRDM\" : [\"AMTP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"MOTP\",\"ZELP\",\"IRLP\"],\n",
    "    \"LAXM\" : [\"AMTP\",\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"GLYP\",\"MOTP\",\"ZELP\",\"IRLP\"]\n",
    "}\n",
    "metrics = ['TRX','NRX','TUN','NUN','TUF','NUF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51b3b737-8e23-486d-a590-b68d16827026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [01:37,  5.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#parquet_file = pq.ParquetFile(curwk+'\\\\df_final2.parquet')\n",
    "parquet_file = pq.ParquetFile(f's3://{bucket}/{curwk}df_final2.parquet')\n",
    "\n",
    "writer = None\n",
    "mthxpn_LAX_DN = pl.DataFrame()\n",
    "loop_counter = 0\n",
    "\n",
    "for batch in tqdm(parquet_file.iter_batches(batch_size=30000)):\n",
    "    \n",
    "    pl_batch = pl.from_arrow(batch)\n",
    "    all_columns = pl_batch.columns \n",
    "    \n",
    "    for prod_family, prod_codes in prod_family_market_buckets.items():\n",
    "        for metric in metrics:\n",
    "            relevant_columns = [col for col in all_columns if any(prod_code + metric in col for prod_code in prod_codes)]\n",
    "            month_numbers = sorted(set(int(col.split(metric)[-1]) for col in relevant_columns))\n",
    "            for month_number in month_numbers:\n",
    "                new_column = prod_family + metric + str(month_number)\n",
    "                month_columns = [col for col in relevant_columns if col.endswith(metric + str(month_number))]\n",
    "                #pl_batch = pl_batch.with_columns(sum(pl.col(c) for c in month_columns).alias(new_column))\n",
    "                pl_batch = pl_batch.with_columns(pl.sum_horizontal(month_columns).alias(new_column))\n",
    "\n",
    "    mthxpn_LAX_DN = mthxpn_LAX_DN.vstack(pl_batch)\n",
    "    \n",
    "    if loop_counter % 40 == 0 and loop_counter != 0:\n",
    "        table = mthxpn_LAX_DN.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(mthxpn+'\\\\LAX_DN.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{mthxpn}LAX_DN.parquet', table.schema)\n",
    "        writer.write_table(table)\n",
    "        mthxpn_LAX_DN = pl.DataFrame()\n",
    "    \n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(mthxpn_LAX_DN) > 0:\n",
    "    table = mthxpn_LAX_DN.to_arrow()\n",
    "    if writer is None:\n",
    "        #writer = pq.ParquetWriter(mthxpn+'\\\\LAX_DN.parquet', table.schema)\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{mthxpn}LAX_DN.parquet', table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "#LAX_DN FOR MONTHLY level is complete !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
