{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2f464b-3619-490f-bcca-afd63b8f5acb",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd12954-8f95-463a-a60f-98ba5d1018d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cf3ce2-c66e-40ff-899b-515875edd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables -\n",
    "this_day = datetime.today()\n",
    "# ### FOR TESTING - REMOVE LATER ### \n",
    "# this_day = this_day - timedelta(days=7)\n",
    "# ####\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")\n",
    "\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20f33d4-f752-4305-876f-453a048e0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'vortex-staging-a65ced90'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea009045-69e8-4a48-869f-b2dc4e8c1f7f",
   "metadata": {},
   "source": [
    "Library names and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee9545a-c7bc-43c1-a416-a704c6f79dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "prewk = f'PYADM/raw/{PRE_PROC_WK}/dataframes/'\n",
    "wkxpn = 'PYADM/weekly/staging/xponent/'\n",
    "mthxpn = 'PYADM/monthly/staging/xponent/'\n",
    "pwkxpn = f'PYADM/weekly/archive/{PRE_WK}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc526b9c-e04c-4f90-97cd-254a62d7f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking Up Data from Rx_2\n",
    "# weekly -\n",
    "wkxpn_LAX_N = pl.read_parquet(f's3://{bucket}/{curwk}wkxpn_LAX_N.parquet') #Source for this is subject to change , may add full version in future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1107cb4f-8ae5-4039-b682-114927d52287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows where product is null\n",
    "wkxpn_LAX_N = wkxpn_LAX_N.filter(wkxpn_LAX_N['PROD_CD'] != \"\")\n",
    "\n",
    "#Picking up Date Parm from previous code to get week number and merging it rx data\n",
    "date_parm_wk = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_WK.parquet')\n",
    "date_parm_wk = date_parm_wk.with_columns(pl.col('WK_END_DATE').dt.date()) \n",
    "\n",
    "#fix for dtype fix | join not working | NEW\n",
    "wkxpn_LAX_N = wkxpn_LAX_N.with_columns(pl.col('WK_END_DATE').cast(pl.Date))\n",
    "\n",
    "LAX_N_1 = wkxpn_LAX_N.join(date_parm_wk,on='WK_END_DATE',how='left') # SHOULD I CHANGE THIS TO INNER ? \n",
    "\n",
    "#Dropping redundant columns , not pertinet to creation of lax_dn\n",
    "LAX_N_1 = LAX_N_1.drop(['MKT_CD','MarketName','G_B','PFAM_NAME','PROD_NAME','WK_END_DATE','RO_TYPE'])\n",
    "\n",
    "#This might come in use when creating buckets , CAN ALSO USE SEG DEF DIRECTLY OR HARD CODE\n",
    "#fam_prod_mapping = LAX_N_1.select(['PFAM_CD', 'PROD_CD']).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64af5c4-67fa-4826-a876-cde3f8514f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['TRX','NRX','TUN','NUN','TUF','NUF']\n",
    "# Adding a new column called PROD_WK , will contain PROD_CD and the week number of transaction\n",
    "# this the column on which we transpose the data  \n",
    "df_with_new_cols = LAX_N_1.with_columns([(pl.col(\"PROD_CD\").cast(pl.Utf8) + \"P_\" + pl.col(\"I\").cast(pl.Utf8)).alias(\"PROD_WK\")])\n",
    "\n",
    "# Since PROD_WK now has prod and week info, we dont these columns\n",
    "df_dropped = df_with_new_cols.drop(['PFAM_CD','PROD_CD','I'])\n",
    "\n",
    "# Sorting data at IID level to chunk and filter effectively\n",
    "df_sorted = df_dropped.sort('IID')\n",
    "\n",
    "#For memory protection\n",
    "del LAX_N_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ed8124-bc67-4dc7-9b16-c66b5e233cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_unique_vals' contains a list of all possible column names after transposing data\n",
    "# we will use it to standardize the shape of every chunk, as it will help us concat / stack them each iteration\n",
    "\n",
    "unique_vals = list(df_sorted['PROD_WK'].unique()) #using full data to get all unique values here\n",
    "# NOTE : It might very well be possible that some weeks of data may be missing, we might have to add those columns manually at some point\n",
    "\n",
    "full_unique_vals = []\n",
    "def unique_vals_prod_wk(col_name):   #this function breaks down PROD_WK to your regular column names like LI1PTUF\n",
    "    parts = col_name.split('_')\n",
    "    for m in metrics:\n",
    "        full = parts[0]+m+parts[-1]\n",
    "        full_unique_vals.append(full)\n",
    "    \n",
    "for i in unique_vals:\n",
    "    unique_vals_prod_wk(i)\n",
    "\n",
    "# Could add a modifier here to check and have full 105 weeks of data ?\n",
    "    \n",
    "full_unique_vals.sort()\n",
    "full_unique_vals.insert(0,'IID') #Adding IID because i will also use this list to standardize the order of columns in each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a142c8-7a35-4f85-b24b-4a780a38e6bb",
   "metadata": {},
   "source": [
    "###### QC only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5441fc84-836c-4238-9f78-abd1c8ab22ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products in data which do not have 105 weeks of data :  2\n",
      "['MRB', 'ZEL']\n",
      "shape: (2, 2)\n",
      "┌──────┬────────────┐\n",
      "│ prod ┆ num_of_wks │\n",
      "│ ---  ┆ ---        │\n",
      "│ str  ┆ u32        │\n",
      "╞══════╪════════════╡\n",
      "│ MRB  ┆ 82         │\n",
      "│ ZEL  ┆ 58         │\n",
      "└──────┴────────────┘\n",
      "BUT !  - \n",
      "MRB  has gaps in weeks\n",
      "ZEL  has gaps in weeks\n"
     ]
    }
   ],
   "source": [
    "wk_tst = pl.DataFrame()\n",
    "wk_tst = wk_tst.with_columns(pl.Series(name='col_names_raw',values=full_unique_vals[1:]))\n",
    "\n",
    "def split_col_names(value):\n",
    "    prod = value[:3]\n",
    "    metric = value[4:7]\n",
    "    wknum = value[7:]\n",
    "    return prod, metric, wknum\n",
    "\n",
    "wk_tst = wk_tst.with_columns([pl.col(\"col_names_raw\").map_elements(split_col_names, return_dtype=pl.Object).alias(\"split_values\")])\n",
    "\n",
    "wk_tst = wk_tst.with_columns([\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[0], return_dtype=pl.Utf8).alias(\"prod\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[1], return_dtype=pl.Utf8).alias(\"metric\"),\n",
    "    pl.col(\"split_values\").map_elements(lambda x: x[2], return_dtype=pl.Utf8).alias(\"wknum\"),\n",
    "])\n",
    "wk_tst = wk_tst.drop([\"split_values\",\"col_names_raw\"])\n",
    "\n",
    "res = wk_tst.group_by(['prod','metric']).agg([pl.col('wknum').n_unique().alias('num_of_wks')])\n",
    "missing_wknum = res.filter(pl.col('num_of_wks') != 105)\n",
    "missing_wknum = missing_wknum.sort(by='prod')\n",
    "print(\"Number of products in data which do not have 105 weeks of data : \",len(missing_wknum['prod'].unique()))\n",
    "print(list((missing_wknum['prod'].unique())))\n",
    "missmps = list((missing_wknum['prod'].unique()))\n",
    "missing_wknum_print = missing_wknum.select(pl.col(['prod','num_of_wks']))\n",
    "missing_wknum_print = missing_wknum_print.unique(subset=['prod','num_of_wks'])\n",
    "print(missing_wknum_print)\n",
    "\n",
    "wk_conti = pl.DataFrame()\n",
    "wk_conti = wk_tst.filter(pl.col('prod').is_in(missmps))\n",
    "wk_conti = wk_conti.drop('metric')\n",
    "wk_conti = wk_conti.unique(subset=['prod','wknum'])\n",
    "wk_conti = wk_conti.with_columns(pl.col(\"wknum\").cast(pl.Int32))\n",
    "\n",
    "print('BUT !  - ')\n",
    "for prod in missmps:\n",
    "    f1 = wk_conti.filter(pl.col('prod')== prod )\n",
    "    if (len(f1['wknum'].unique()) != f1['wknum'].max()):\n",
    "        print(prod,\" has gaps in weeks\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da770c87-384a-43db-b8e5-7f4b3968fa45",
   "metadata": {},
   "source": [
    "#### Following Transposes data by chunks worth 5000 HCPS and exports them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864ae5cb-1876-4659-8aaa-0fb1c40dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iids = df_sorted['IID'].unique() #(This will be the number of rows in final result)\n",
    "chunk_size = 50000 #Each chunk will contain 5000 HCPs worth of transactions (NOT ROWS, they may differ each chunk)\n",
    "\n",
    "iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]\n",
    "#So IID_chunks is a list of lists, each list contains 5000 HCPs and number of lists is our number of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797aa19-144e-47a3-907b-ba3e5957aad9",
   "metadata": {},
   "source": [
    "Basic logic for the following section -(DOUBLE CLICK FOR MORE READABLITY)\n",
    "So, 'df_sorted' is your base data (lax_n from previous code) with-\n",
    "1)Records with no prod_cd removed\n",
    "2)Redundant columns dropped and a new column 'PROD_WK' containing prod code and week number (from 1 to 105)\n",
    "3)Then sorted on IID to make filtering faster ? (that sort may be reundant actually but doesnt hurt as its pretty fast)\n",
    "\n",
    "We need to transpose it on PROD_WK, but we cant do the operation on the whole dataframe in one go, as it would be too memory\n",
    "intensive for the operations we need to do , hence we break down the task into destinct chunks and export the result part by part.\n",
    "\n",
    "Now when we tranpose a data across a certain column, in our case PROD_WK, and since our resultant data will be unique on IID,\n",
    "i.e, Since IID will be the index, we need to make sure all the records for a given IID are not spread accross multiple chunks,\n",
    "otherwise we will get multiple records with the same IID in the transposed dataset. \n",
    "Hence, \n",
    "1)'df_chunk' - each chunk taken from df_sorted will contain ALL the records for the given 5000 hcps, this is done by a simple is_in() function and\n",
    "the previously made series 'iid_chunks'.\n",
    "2)'df_pivot_chunk' - Used polars libary inbult data transpose function.\n",
    "3)applied a lamba function to rename the columns after tranposing.\n",
    "4)Now Since all the chunks need to eventually fit in one file , we need to make sure their structure is same.\n",
    "Their shapes will be inherently different as each chunk may not have the same number of products or weeks's worth of transactions.\n",
    "to fix this we can use the list of all distinct possible column names we created before 'full_unique_vals' and loop over it\n",
    "to add any column not present in a given chunk.\n",
    "NOTE : This step on its own is pretty time consuming. Things to consider:\n",
    "We are forcefully populating it with series 'None' values of same length as the chunk (5000).\n",
    "Polars stores 'None' values a bit differently, its a distinct datatype holding a non zero amount of space in memory.\n",
    "https://pola-rs.github.io/polars/py-polars/html/reference/datatypes.html\n",
    "This action alone will cause the size of the chunk to jump. Alternative options were explored , like passing blanks [] in the seires\n",
    "but that causes their length to truncate to 0 , and unsuable to be fitted into a column.\n",
    "Using other libraries other than polars could also be a longshot , but probably wont be worth the trade off of speed and stablity.\n",
    "\n",
    "5)A simple column reorder is done to using the full_unique_vals list which is now possible because all of those columns were manually added.\n",
    "Polars inbuilt function 'select' is used to faciliate this.\n",
    "\n",
    "The chunk of data 'df_pivot_chunk' is now fully processed and is ready to be exported.\n",
    "After testing with of variations , I've come to the conclusion that :\n",
    "-Holding 5 chunks worth of data in memory (Faciliated by appending / concatination / (using vstack() function))\n",
    "-And then writing to an external file\n",
    "Will be the most stable and time efficient.\n",
    "Loops that only process a chunk and dont do I/O take about 5 to 7 Seconds, on Loops where 'df_final\" is being exported to an external parquet file\n",
    "take 25 to 30 seconds. (Note : The system can handle upto 10 chunks in memory if background useage was low but that causes a trade-off by causing\n",
    "I/O loops to take longer. Consider Min-Maxing the Iteration Cycle if System memory changes)\n",
    "\n",
    "6)'df_final' holds data 5 chunks, and is exported to curwk location using pyarrow libary.\n",
    "pyarrow seems to be working the most stable as compared to Polars or Pandas related parquet export functions.\n",
    "\n",
    "Future Note :\n",
    "used tqdm library to track progress : First recorded runtime on 13th Dec 2023 was 15~16 Mins.\n",
    "The most time consuming part of this loop is adding Null Values and the Export of chunks.\n",
    "Space inefficieny can also be looked into because of the Null Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4044852c-34ec-4fe4-a9fe-1ab7c54b6338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [02:44<00:00, 14.97s/it]\n"
     ]
    }
   ],
   "source": [
    "writer = None  #this is used by pyarrow to write data in chunks to external file\n",
    "df_final = pl.DataFrame()  #so, df_final will be the object holding final data and being used for exporting\n",
    "loop_counter = 0 #just for utility\n",
    "\n",
    "for iid_chunk in tqdm(iid_chunks):\n",
    "\n",
    "    df_chunk = df_sorted.filter(pl.col('IID').is_in(iid_chunk))\n",
    "    df_pivot_chunk = df_chunk.pivot(values=metrics,index='IID',columns='PROD_WK',maintain_order=True,sort_columns=True)\n",
    "    df_pivot_chunk = df_pivot_chunk.select(pl.all().name.map(lambda col_name: col_name.split('_')[3] + col_name.split('_')[0] + col_name.split('_')[-1] if 'PROD_WK_' in col_name else col_name))\n",
    "\n",
    "    missing_cols = [col for col in full_unique_vals if col not in df_pivot_chunk.columns]\n",
    "    for col in missing_cols: # This Takes 5 Seconds\n",
    "        null_series = pl.Series(col, [None]*len(df_pivot_chunk), dtype=pl.Float64)\n",
    "        df_pivot_chunk = df_pivot_chunk.with_columns(null_series)\n",
    "\n",
    "    df_pivot_chunk = df_pivot_chunk.select(full_unique_vals)\n",
    "\n",
    "    df_final = df_final.vstack(df_pivot_chunk)\n",
    "\n",
    "    if loop_counter % 3 == 0 and loop_counter != 0: # This takes about 25 seconds ? TTT Should be ~ 30 Secs\n",
    "        table = df_final.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(curwk+'\\\\df_final.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{curwk}df_final.parquet',table.schema)\n",
    "        writer.write_table(table)\n",
    "        df_final = pl.DataFrame() # Reset df_final after writing to file\n",
    "\n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(df_final) > 0:\n",
    "    table = df_final.to_arrow()\n",
    "    if writer is None:\n",
    "        #writer = pq.ParquetWriter(curwk+'\\\\df_final.parquet', table.schema)\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{curwk}df_final.parquet',table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27aa659-63b5-4458-8c16-3135d6e5fc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for memory preservation - WIP , TWEAK AS REALTIME RUNS Progress\n",
    "\n",
    "# for obj in gc.get_objects():\n",
    "#     if isinstance(obj, pl.dataframe.frame.DataFrame):\n",
    "#         varnames = [varname for varname, varval in globals().items() if varval is obj]\n",
    "#         size_gb = obj.estimated_size(unit='gb')\n",
    "#         print(f\"Variable names: {varnames}, Estimated size (GB): {size_gb}\")\n",
    "\n",
    "\n",
    "del df_with_new_cols\n",
    "del df_dropped\n",
    "del df_sorted\n",
    "del df_chunk\n",
    "del df_pivot_chunk\n",
    "del df_final\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca1eed-18d7-460d-ae5c-6acd4d35550d",
   "metadata": {},
   "source": [
    "#### May have to add a restart kernel clause here because of sytem stablity\n",
    "##### Followed by re declaring variables and paths "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e5278-7f45-4906-8070-ab7300e059b5",
   "metadata": {},
   "source": [
    "Notes for next section -\n",
    "The previous section processed data at IID level for all the products , metrics for 105 weeks (curwk, df_final)\n",
    "But we still need to add columns for product family and markets.\n",
    "\n",
    "-Groupings for product Family (F) and product markets (M) are hard coded bellow\n",
    "-Using pyarrow lib to read data in chunks from df_final.parquet\n",
    "-Applied same logic as previous code : Read and Process 5 Chunks , Then Export \n",
    "\n",
    "The main function for this is to create all the prod family and prod market columns from 'prod_family_market_buckets'\n",
    "New columns are created by summing the columns present in the corrosponding lists for a prod family/market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e52d693a-7c99-4987-a40e-f35828f5a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_family_market_buckets = {\n",
    "    \"MRXF\" : [\"MRGP\",\"MRBP\",\"GLYP\"],\n",
    "    \"LINF\" : [\"LI1P\",\"LI2P\",\"LI3P\"],\n",
    "    \"LUBF\" : [\"AMTP\",\"LUBP\"],\n",
    "    \"GENM\" : [\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"GLYP\"],\n",
    "    \"BRDM\" : [\"AMTP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"MOTP\",\"ZELP\",\"IRLP\"],\n",
    "    \"LAXM\" : [\"AMTP\",\"FLXP\",\"LACP\",\"LUBP\",\"MRGP\",\"MRBP\",\"LI1P\",\"LI2P\",\"LI3P\",\"TRUP\",\"GLYP\",\"MOTP\",\"ZELP\",\"IRLP\"]\n",
    "}\n",
    "metrics = ['TRX','NRX','TUN','NUN','TUF','NUF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670c4162-c56c-40a8-b0c5-dfdc8f5f2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [10:41, 35.64s/it]\n"
     ]
    }
   ],
   "source": [
    "#parquet_file = pq.ParquetFile(curwk+'\\\\df_final.parquet') \n",
    "parquet_file = pq.ParquetFile(f's3://{bucket}/{curwk}df_final.parquet') #25 minutes for this\n",
    "writer = None\n",
    "wkxpn_LAX_DN = pl.DataFrame()\n",
    "loop_counter = 0\n",
    "\n",
    "for batch in tqdm(parquet_file.iter_batches(batch_size=30000)):\n",
    "    \n",
    "    pl_batch = pl.from_arrow(batch)\n",
    "    all_columns = pl_batch.columns \n",
    "    \n",
    "    for prod_family, prod_codes in prod_family_market_buckets.items():\n",
    "        for metric in metrics:\n",
    "            relevant_columns = [col for col in all_columns if any(prod_code + metric in col for prod_code in prod_codes)]\n",
    "            week_numbers = sorted(set(int(col.split(metric)[-1]) for col in relevant_columns))\n",
    "            for week_number in week_numbers:\n",
    "                new_column = prod_family + metric + str(week_number)\n",
    "                week_columns = [col for col in relevant_columns if col.endswith(metric + str(week_number))]\n",
    "                #pl_batch = pl_batch.with_columns(sum(pl.col(c) for c in week_columns).alias(new_column)) # This Gives Nulls !\n",
    "                pl_batch = pl_batch.with_columns(pl.sum_horizontal(week_columns).alias(new_column))\n",
    "\n",
    "    wkxpn_LAX_DN = wkxpn_LAX_DN.vstack(pl_batch)\n",
    "    \n",
    "    if loop_counter % 3 == 0 and loop_counter != 0:\n",
    "        table = wkxpn_LAX_DN.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(wkxpn+'\\\\LAX_DN.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{wkxpn}LAX_DN.parquet', table.schema)\n",
    "        writer.write_table(table)\n",
    "        wkxpn_LAX_DN = pl.DataFrame()\n",
    "    \n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(wkxpn_LAX_DN) > 0:\n",
    "    table = wkxpn_LAX_DN.to_arrow()\n",
    "    if writer is None:\n",
    "        #writer = pq.ParquetWriter(wkxpn+'\\\\LAX_DN.parquet', table.schema)\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{wkxpn}LAX_DN.parquet', table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "#LAX_DN FOR WEEKLY level is complete !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
