{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74846d4-cf3f-48ab-ba37-8d12215547af",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9001be-bd7c-4339-8ba3-95c0753205e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo\n",
    "\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07671a-fee9-49b2-9ea6-ebba1170ce22",
   "metadata": {},
   "source": [
    "Date Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41fe8a0-8dc9-4105-a94b-c458ace5fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables -\n",
    "this_day = datetime.today()\n",
    "# ### FOR TESTING - REMOVE LATER ### \n",
    "# this_day = this_day - timedelta(days=7)\n",
    "# ####\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")\n",
    "\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a190615d-3b07-44ff-beb0-507d68e33051",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'vortex-staging-a65ced90'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4c754-8da7-40e0-bdf6-f916909c9017",
   "metadata": {},
   "source": [
    "Library Names and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2bf2df-f443-403b-9b8f-c5304a494335",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "prewk = f'PYADM/raw/{PRE_PROC_WK}/dataframes/'\n",
    "wkxpn = 'PYADM/weekly/staging/xponent/'\n",
    "mthxpn = 'PYADM/monthly/staging/xponent/'\n",
    "pwkxpn = f'PYADM/weekly/archive/{PRE_WK}/xponent/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a08a0e9-843c-45a5-9d83-d75ee0bb3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking Up Data from Rx_2\n",
    "WK_TMP1_LAX = pl.read_parquet(f's3://{bucket}/{curwk}temp_WK_TMP1_LAX.parquet') #Source for this is subject to change , may add full version in future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb53fa88-91e2-46a4-8951-cd9baa13c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Weekly \n",
    "group_cols = ['IID','PROD_CD','ChannelID','ChannelName','WK_END_DATE','MKT_CD','MarketName','G_B','RO_TYPE','PFAM_CD','PROD_NAME']\n",
    "sum_cols = ['TRX','NRX','TUN','NUN','TUF','NUF']\n",
    "\n",
    "# #perform groupby and calc function \n",
    "WK_TMP2_LAX = pl.DataFrame()\n",
    "WK_TMP2_LAX = WK_TMP1_LAX.fill_null(\"\").group_by(group_cols).agg(\n",
    "    [pl.col(c).sum() for c in sum_cols]\n",
    ")\n",
    "\n",
    "WK_TMP2_LAX = WK_TMP2_LAX.with_columns(\n",
    "    pl.when(pl.col(\"MKT_CD\") == \"\")\n",
    "    .then(pl.lit(\"\"))\n",
    "    .otherwise(pl.col(\"MarketName\"))\n",
    "    .alias(\"MarketName\")\n",
    ")\n",
    "\n",
    "WK_TMP2_LAX.sort(by=['IID','PROD_CD']) # unsure of use\n",
    "\n",
    "del WK_TMP1_LAX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce570cdf-9168-429c-bdca-577159d9f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking up Date Parm from previous code to get week number and merging it rx data\n",
    "date_parm_wk = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_WK.parquet')\n",
    "date_parm_wk = date_parm_wk.with_columns(pl.col('WK_END_DATE').dt.date()) \n",
    "\n",
    "#fix for dtype fix | join not working | NEW\n",
    "WK_TMP2_LAX = WK_TMP2_LAX.with_columns(pl.col('WK_END_DATE').cast(pl.Date))\n",
    "\n",
    "WK_TMP2_LAX = WK_TMP2_LAX.join(date_parm_wk,on='WK_END_DATE',how='left') # SHOULD I CHANGE THIS TO INNER ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "332ec47f-706a-42bd-83b5-9c0962b21a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chnl_dict = {\"1\":'RTL',\"2\":'MO'} # 1 is RTL, 2 is MO\n",
    "\n",
    "WK_TMP3_LAX = WK_TMP2_LAX.with_columns([\n",
    "    (pl.col('ChannelID').map_elements(lambda x: chnl_dict.get(x,x),return_dtype=pl.String).cast(pl.Utf8) + \"_\" + pl.col(\"I\").cast(pl.Utf8)).alias('CH_WK')\n",
    "])\n",
    "\n",
    "del WK_TMP2_LAX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb18050c-1933-48dc-b350-927175856133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping the columns being used in bit (New step)\n",
    "WK_TMP3_LAX= WK_TMP3_LAX.drop(\n",
    "    ['ChannelID','ChannelName','WK_END_DATE','I','ChannelName','MKT_CD','G_B','RO_TYPE','PROD_NAME','PFAM_CD']\n",
    ")\n",
    "WK_TMP3_LAX = WK_TMP3_LAX.sort('IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aefd46c4-9017-49e5-a6c8-b93c961e04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Weeks of Data for RTL type records :  105\n",
      "Number of Weeks of Data for MO type records :  105\n"
     ]
    }
   ],
   "source": [
    "#QC\n",
    "ucols = list(WK_TMP3_LAX['CH_WK'].unique())\n",
    "tdf = pl.DataFrame({\n",
    "    'chn' : [s.split('_')[0] for s in ucols],\n",
    "    'wk' : [s.split('_')[1] for s in ucols],\n",
    "})\n",
    "tdf.sort(by=['chn'])\n",
    "tdf1 = tdf.filter(pl.col('chn')=='MO').sort(by='wk')\n",
    "tdf2 = tdf.filter(pl.col('chn')=='RTL').sort(by='wk')\n",
    "print(\"Number of Weeks of Data for RTL type records : \",tdf1.shape[0])\n",
    "print(\"Number of Weeks of Data for MO type records : \",tdf2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be41cede-6cba-43f0-9830-0cf717f3e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = list(WK_TMP3_LAX['CH_WK'].unique())\n",
    "metrics = ['TRX','NRX','TUF','NUF','TUN','NUN']\n",
    "full_unique_vals = []\n",
    "def unique_vals_prod_wk(col_name):  \n",
    "    parts = col_name.split('_')\n",
    "    for m in metrics:\n",
    "        full = parts[0]+'_'+m+parts[-1]\n",
    "        full_unique_vals.append(full)\n",
    "for i in unique_vals:\n",
    "    unique_vals_prod_wk(i)\n",
    "full_unique_vals.sort()\n",
    "\n",
    "full_unique_vals = ['IID','PROD_CD','MarketName'] + full_unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42acb408-fb47-41dd-a77e-823144e7fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iids = WK_TMP3_LAX['IID'].unique() \n",
    "chunk_size = 30000 \n",
    "iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5263de96-36e7-4088-b09e-eab8328df246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [02:16<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "writer = None  \n",
    "wkxpn_LAX = pl.DataFrame()  \n",
    "loop_counter = 0 \n",
    "\n",
    "for iid_chunk in tqdm(iid_chunks):\n",
    "\n",
    "    df_chunk = WK_TMP3_LAX.filter(pl.col('IID').is_in(iid_chunk))\n",
    "\n",
    "    df_pivot_chunk = df_chunk.pivot(\n",
    "        values = metrics, \n",
    "        index = ['IID','PROD_CD','MarketName'], \n",
    "        columns = 'CH_WK', \n",
    "        aggregate_function = None, sort_columns = True\n",
    "    )\n",
    "\n",
    "    df_pivot_chunk = df_pivot_chunk.select(pl.all().name.map(\n",
    "        lambda col_name: col_name.split('_')[-2] + '_' + col_name.split('_')[0] + col_name.split('_')[-1] if(('MO' in col_name) or ('RTL' in col_name)) else col_name)\n",
    "    )\n",
    "\n",
    "    #This step might be redundant \n",
    "    missing_cols = [col for col in full_unique_vals if col not in df_pivot_chunk.columns]\n",
    "    for col in missing_cols: # This Takes 5 Seconds\n",
    "        null_series = pl.Series(col, [None]*len(df_pivot_chunk), dtype=pl.Float64)\n",
    "        df_pivot_chunk = df_pivot_chunk.with_columns(null_series)\n",
    "\n",
    "    #This is used to hot fix the column order sequence glitch, useful for later on \n",
    "    code_columns = [col for col in df_pivot_chunk.columns if (('MO' in col) or ('RTL' in col))]\n",
    "    metrics = ['TRX','NRX','TUF','NUF','TUN','NUN']\n",
    "    prefixes = ['MO', 'RTL']\n",
    "    def sort_key(col):\n",
    "        for prefix in prefixes:\n",
    "            for metric in metrics:\n",
    "                if f'{prefix}_{metric}' in col:\n",
    "                    return (prefix, metric, int(col.replace(f'{prefix}_{metric}', '')))\n",
    "\n",
    "    # Sort the columns\n",
    "    code_columns.sort(key=sort_key)\n",
    "    code_columns = ['IID','PROD_CD','MarketName'] + code_columns\n",
    "    # APPLY FIX - \n",
    "    df_pivot_chunk = df_pivot_chunk.select(code_columns)\n",
    "\n",
    "    for metric in metrics:\n",
    "        relevant_columns = [col for col in code_columns if any(prefix+'_'+metric in col for prefix in prefixes)]\n",
    "        week_numbers = sorted(set(int(col.split(metric)[-1]) for col in relevant_columns))\n",
    "        for week_number in week_numbers:\n",
    "            new_column = metric+str(week_number)\n",
    "            week_columns = [col for col in relevant_columns if col.endswith(metric + str(week_number))]\n",
    "            #df_pivot_chunk = df_pivot_chunk.with_columns(sum(pl.col(c) for c in week_columns).alias(new_column))\n",
    "            df_pivot_chunk = df_pivot_chunk.with_columns(pl.sum_horizontal(week_columns).alias(new_column))\n",
    "\n",
    "    # cant combine yet - need to add summs first - \n",
    "    wkxpn_LAX = wkxpn_LAX.vstack(df_pivot_chunk)\n",
    "    \n",
    "    if loop_counter % 10 == 0 and loop_counter != 0: \n",
    "        table = wkxpn_LAX.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(wkxpn+'\\\\LAX.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{wkxpn}LAX.parquet', table.schema)\n",
    "        writer.write_table(table)\n",
    "        wkxpn_LAX = pl.DataFrame() \n",
    "\n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(wkxpn_LAX) > 0:\n",
    "    table = wkxpn_LAX.to_arrow()\n",
    "    if writer is None:\n",
    "        #writer = pq.ParquetWriter(wkxpn+'\\\\LAX.parquet', table.schema)\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{wkxpn}LAX.parquet', table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05271c4-22a1-42cd-81a5-83f4613bb29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del WK_TMP3_LAX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71153a5d-771b-4cb2-98bd-cb7811835ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POTENTIAL SOLLUTION 1 - FOR IW DEV - Read Me ----------------------------\n",
    "# PIVOT THE DATA 2 METRICS AT A TIME AND COMBINE VERTICALLY AT THE END\n",
    "# Test Results - System is able to successfully pivot the data , and the shape of the datasets\n",
    "# after pivoting for TRX,NRX | TUF,NUF | TUN,NUN is the same - So technically they can be combined.\n",
    "# But Practial runs suggests that kernel is unable to allocate any more memory after just 1 pivot operation.\n",
    "# Work Around - store df_pivot_1 externally , and delete , do the same for 2 and 3 then -\n",
    "# read , combine , export in memory \n",
    "# or come up with a way to do the same outside memory . \n",
    "# Time taken for a single pivot action was pretty reasonable , around 30 sec, considering the fact that this is done on data containing\n",
    "# ALL HCPs for ALL products and weeks, POTENTIAL SOLUTION 1 is a good approach for future RAM upgrades.\n",
    "##### pseudo code - \n",
    "# df_pivot_1 = sub_test_1.pivot(values = ['TRX','NRX'], index = ['IID','PROD_CD','MarketName'], columns = 'CH_WK', aggregate_function = None, sort_columns = True)\n",
    "# df_pivot_2 = sub_test_1.pivot(values = ['TUF','NUF'], index = ['IID','PROD_CD','MarketName'], columns = 'CH_WK', aggregate_function = None, sort_columns = True)\n",
    "# df_pivot_3 = sub_test_1.pivot(values = ['TUN','NUN'], index = ['IID','PROD_CD','MarketName'], columns = 'CH_WK', aggregate_function = None, sort_columns = True)\n",
    "# df_final = pd.concat([df_pivot_1, df_pivot_2, df_pivot_3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d09e76-5cd5-498d-8a30-14b1a63f1eea",
   "metadata": {},
   "source": [
    "### Monthly Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72443cd9-fae8-40a3-af85-d592f20a6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WK_TMP1_LAX= pl.read_parquet(curwk+'\\\\temp_WK_TMP1_LAX.parquet')\n",
    "WK_TMP1_LAX= pl.read_parquet(f's3://{bucket}/{curwk}temp_WK_TMP1_LAX.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca3a8903-b93c-4f0a-b2b2-e50e0872abd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #For Monthly\n",
    "\n",
    "group_cols = ['IID','PROD_CD','ChannelID','ChannelName','CCYYMM','MKT_CD','MarketName','G_B','RO_TYPE','PFAM_CD','PROD_NAME']\n",
    "sum_cols = ['TRX','NRX','TUN','NUN','TUF','NUF']\n",
    "\n",
    "# #perform groupby and calc function \n",
    "MTH_TMP2_LAX = pl.DataFrame()\n",
    "MTH_TMP2_LAX = WK_TMP1_LAX.fill_null(\"\").group_by(group_cols).agg(\n",
    "    [pl.col(c).sum() for c in sum_cols]\n",
    ")\n",
    "\n",
    "MTH_TMP2_LAX = MTH_TMP2_LAX.with_columns(\n",
    "    pl.when(pl.col(\"MKT_CD\") == \"\")\n",
    "    .then(pl.lit(\"\"))\n",
    "    .otherwise(pl.col(\"MarketName\"))\n",
    "    .alias(\"MarketName\")\n",
    ")\n",
    "\n",
    "MTH_TMP2_LAX.sort(by=['IID','PROD_CD']) # unsure of use\n",
    "del WK_TMP1_LAX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec37ed1-2689-4394-98fe-ab013da98022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking up Date Parm from previous code to get week number and merging it rx data\n",
    "#date_parm_mt = pl.read_parquet(curwk+\"\\\\curwk_DATE_PARM_MTH.parquet\")\n",
    "date_parm_mt = pl.read_parquet(f's3://{bucket}/{curwk}curwk_DATE_PARM_MTH.parquet')\n",
    "MTH_TMP2_LAX = MTH_TMP2_LAX.join(date_parm_mt,on='CCYYMM',how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaaa0190-a3a2-4072-a658-d27c1bf2452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATE - DATE PARM LIGIC HAD TO BE MADE INNER FOR MONTH LEVEL BUT NOT IN WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98958133-b149-4435-9021-cd4730b20ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chnl_dict = {\"1\":'RTL',\"2\":'MO'} # 1 is RTL , 2 is MO\n",
    "\n",
    "MTH_TMP3_LAX = MTH_TMP2_LAX.with_columns([\n",
    "    (pl.col('ChannelID').map_elements(lambda x: chnl_dict.get(x,x),return_dtype=pl.String).cast(pl.Utf8) + \"_\" + pl.col(\"I\").cast(pl.Utf8)).alias('CH_MT')\n",
    "])\n",
    "del MTH_TMP2_LAX\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9c25fc8-9940-42da-a965-f9a57fee1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping the columns being used in bit (New step)\n",
    "MTH_TMP3_LAX= MTH_TMP3_LAX.drop(\n",
    "    ['ChannelID','ChannelName','CCYYMM','I','ChannelName','MKT_CD','G_B','RO_TYPE','PROD_NAME','PFAM_CD','DATE_AS_OF']\n",
    ")\n",
    "MTH_TMP3_LAX = MTH_TMP3_LAX.sort('IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14433543-f70a-442c-a9ff-e5e570941c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Months of Data for RTL type records :  24\n",
      "Number of Months of Data for MO type records :  24\n"
     ]
    }
   ],
   "source": [
    "#QC\n",
    "ucols = list(MTH_TMP3_LAX['CH_MT'].unique())\n",
    "tdf = pl.DataFrame({\n",
    "    'chn' : [s.split('_')[0] for s in ucols],\n",
    "    'mt' : [s.split('_')[1] for s in ucols],\n",
    "})\n",
    "tdf.sort(by=['chn'])\n",
    "tdf1 = tdf.filter(pl.col('chn')=='MO').sort(by='mt')\n",
    "tdf2 = tdf.filter(pl.col('chn')=='RTL').sort(by='mt')\n",
    "print(\"Number of Months of Data for RTL type records : \",tdf1.shape[0])\n",
    "print(\"Number of Months of Data for MO type records : \",tdf2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d056384-2db3-482c-b9b0-91ed63b9def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = list(MTH_TMP3_LAX['CH_MT'].unique())\n",
    "metrics = ['TRX','NRX','TUF','NUF','TUN','NUN']\n",
    "full_unique_vals = []\n",
    "def unique_vals_prod_wk(col_name):  \n",
    "    parts = col_name.split('_')\n",
    "    for m in metrics:\n",
    "        full = parts[0]+'_'+m+parts[-1]\n",
    "        full_unique_vals.append(full)\n",
    "for i in unique_vals:\n",
    "    unique_vals_prod_wk(i)\n",
    "full_unique_vals.sort()\n",
    "\n",
    "full_unique_vals = ['IID','PROD_CD','MarketName'] + full_unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "319ba8c0-1fbf-4b60-af39-15f4baac390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_iids = MTH_TMP3_LAX['IID'].unique() \n",
    "chunk_size = 30000 \n",
    "iid_chunks = [unique_iids[i:i + chunk_size] for i in range(0, len(unique_iids), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17383536-ccb1-431d-9e49-fb382e93ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:34<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "writer = None  \n",
    "mthxpn_LAX = pl.DataFrame()  \n",
    "loop_counter = 0 \n",
    "\n",
    "for iid_chunk in tqdm(iid_chunks):\n",
    "\n",
    "    df_chunk = MTH_TMP3_LAX.filter(pl.col('IID').is_in(iid_chunk))\n",
    "\n",
    "    df_pivot_chunk = df_chunk.pivot(\n",
    "        values = metrics, \n",
    "        index = ['IID','PROD_CD','MarketName'], \n",
    "        columns = 'CH_MT', \n",
    "        aggregate_function = None, sort_columns = True\n",
    "    )\n",
    "\n",
    "    df_pivot_chunk = df_pivot_chunk.select(pl.all().name.map(\n",
    "        lambda col_name: col_name.split('_')[-2] + '_' + col_name.split('_')[0] + col_name.split('_')[-1] if(('MO' in col_name) or ('RTL' in col_name)) else col_name)\n",
    "    )\n",
    "\n",
    "    #This step might be redundant \n",
    "    missing_cols = [col for col in full_unique_vals if col not in df_pivot_chunk.columns]\n",
    "    for col in missing_cols: # This Takes 5 Seconds\n",
    "        null_series = pl.Series(col, [None]*len(df_pivot_chunk), dtype=pl.Float64)\n",
    "        df_pivot_chunk = df_pivot_chunk.with_columns(null_series)\n",
    "\n",
    "    #This is used to hot fix the column order sequence glitch, useful for later on \n",
    "    code_columns = [col for col in df_pivot_chunk.columns if (('MO' in col) or ('RTL' in col))]\n",
    "    metrics = ['TRX','NRX','TUF','NUF','TUN','NUN']\n",
    "    prefixes = ['MO', 'RTL']\n",
    "    def sort_key(col):\n",
    "        for prefix in prefixes:\n",
    "            for metric in metrics:\n",
    "                if f'{prefix}_{metric}' in col:\n",
    "                    return (prefix, metric, int(col.replace(f'{prefix}_{metric}', '')))\n",
    "\n",
    "    # Sort the columns\n",
    "    code_columns.sort(key=sort_key)\n",
    "    code_columns = ['IID','PROD_CD','MarketName'] + code_columns\n",
    "    # APPLY FIX - \n",
    "    df_pivot_chunk = df_pivot_chunk.select(code_columns)\n",
    "\n",
    "    for metric in metrics:\n",
    "        relevant_columns = [col for col in code_columns if any(prefix+'_'+metric in col for prefix in prefixes)]\n",
    "        month_numbers = sorted(set(int(col.split(metric)[-1]) for col in relevant_columns))\n",
    "        for month_number in month_numbers:\n",
    "            new_column = metric+str(month_number)\n",
    "            month_columns = [col for col in relevant_columns if col.endswith(metric + str(month_number))]\n",
    "            df_pivot_chunk = df_pivot_chunk.with_columns(sum(pl.col(c) for c in month_columns).alias(new_column))\n",
    "\n",
    "    \n",
    "    mthxpn_LAX = mthxpn_LAX.vstack(df_pivot_chunk)\n",
    "    \n",
    "    if loop_counter % 10 == 0 and loop_counter != 0: \n",
    "        table = mthxpn_LAX.to_arrow()\n",
    "        if writer is None:\n",
    "            #writer = pq.ParquetWriter(mthxpn+'\\\\LAX.parquet', table.schema)\n",
    "            writer = pq.ParquetWriter(f's3://{bucket}/{mthxpn}LAX.parquet', table.schema)\n",
    "        writer.write_table(table)\n",
    "        mthxpn_LAX = pl.DataFrame() \n",
    "\n",
    "    loop_counter += 1\n",
    "\n",
    "# Write any remaining chunks to the Parquet file\n",
    "if len(mthxpn_LAX) > 0:\n",
    "    table = mthxpn_LAX.to_arrow()\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(f's3://{bucket}/{mthxpn}LAX.parquet', table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "# Close the ParquetWriter\n",
    "if writer is not None:\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
