{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee148e41-2ca9-4ef0-8ad2-febbf18e5f17",
   "metadata": {},
   "source": [
    "#### IW - ADM MASTER PROFILE and NET NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e91dd63-6045-45e9-abd2-26219186b45e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import s3fs\n",
    "import boto3\n",
    "from io import BytesIO as bo\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010f3eba-02e9-482b-8ec3-7afef22cc3b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Date Variables\n",
    "# Variable Calcuation Section :\n",
    "# Since data is received on Monday #Getting this weeks monday from today's date.\n",
    "##############################\n",
    "#FOR CUR_PROC_WK, TODAY, PRE_PROC_WK,CUR_WK,PRE_WK\n",
    "\n",
    "this_day = datetime.today()\n",
    "\n",
    "####\n",
    "# for testing : [Or running for previous weeks]\n",
    "this_day = this_day - timedelta(days=14) # REMOVE ON REGULAR RUNS\n",
    "####\n",
    "\n",
    "days_to_monday = (this_day.weekday() - 0) % 7\n",
    "monday = this_day - timedelta(days=days_to_monday)\n",
    "CUR_PROC_WK = monday.strftime(\"%Y%m%d\")  #Converting to desired string format\n",
    "TODAY = monday  #Keeping datetime format\n",
    "# Setting pricison to null-\n",
    "TODAY = TODAY.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "PRE_PROC_WK0 = monday - timedelta(days=7)\n",
    "PRE_PROC_WK = str(PRE_PROC_WK0.year) + str(PRE_PROC_WK0.month).zfill(2) + str(PRE_PROC_WK0.day).zfill(2)\n",
    "\n",
    "#xponent data week, even in month closing week, still use FRIDAY date;\n",
    "CUR_WK0 = monday - timedelta(days=17)\n",
    "CUR_WK = str(CUR_WK0.year) + str(CUR_WK0.month).zfill(2) + str(CUR_WK0.day).zfill(2)\n",
    "DATA_DATE_WEEKLY = f\"{CUR_WK0.year}-{CUR_WK0.month:02d}-{CUR_WK0.day:02d}\"\n",
    "YR = CUR_WK0.year\n",
    "\n",
    "#weekly events and calls data date - NO two weeks delay(FRIDAY);\n",
    "EVTS_DATE_WEEKLY0 = monday - timedelta(days=3)\n",
    "EVTS_DATE_WEEKLY = f\"{EVTS_DATE_WEEKLY0.year}-{EVTS_DATE_WEEKLY0.month:02d}-{EVTS_DATE_WEEKLY0.day:02d}\"\n",
    "\n",
    "#previous data week - FRIDAY\n",
    "PRE_WK0 = monday - timedelta(days=24)\n",
    "PRE_WK = PRE_WK0.strftime(\"%Y%m%d\")\n",
    "\n",
    "DATA_DATE_CALENDAR = f\"{monday.year}-{monday.month:02d}-{monday.day:02d}\"\n",
    "RECORD_END_DATE = pd.PeriodIndex([this_day], freq='Q').end_time[0].strftime('%Y-%m-%d')\n",
    "##############################\n",
    "QTR = f'{CUR_WK0.year}Q{(CUR_WK0.month-1)//3+1}'\n",
    "QTR_NET = f\"Q{((CUR_WK0.month - 1) // 3 + 1)}\"\n",
    "CUT_OFF_net = ((pd.PeriodIndex([CUR_WK0], freq='Q')) + 1).start_time[0].strftime('%Y-%m-%d')\n",
    "##############################\n",
    "month_end_date = CUR_WK0.replace(day=1) - timedelta(days=1) #This hold the month ending date for the month before current xpn date\n",
    "# Month closing week\n",
    "if 0 <= (CUR_WK0 - month_end_date).days < 7:\n",
    "    # Xponent data month, in month closing week\n",
    "    # Xponent data date or last day of the previous month (if month closing) - FRIDAY\n",
    "    DATA_DATE_MONTHLY = str(month_end_date.year) + '-' + str(month_end_date.month).zfill(2) + '-' + str(month_end_date.day).zfill(2)\n",
    "else:\n",
    "    # Xponent data month, not in month closing week\n",
    "    # Xponent data date or last day of the previous month (if month closing) - FRIDAY\n",
    "    DATA_DATE_MONTHLY = DATA_DATE_WEEKLY\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c338b83-4721-4d58-b004-25d6baf0aa9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Grabbing IW Release File \n",
    "s3 = s3fs.S3FileSystem()\n",
    "bucket_list = s3.listdir(f'vortex-staging-a65ced90/PYADM/raw/{CUR_PROC_WK}/inbound/')\n",
    "for file in bucket_list:\n",
    "    if (file['Key'].__contains__(f\"IRWD_RELEASE_WKLY_{CUR_PROC_WK}\")):\n",
    "        release_file = file['Key']\n",
    "\n",
    "# Reading Release File -\n",
    "s3_client = boto3.client('s3')\n",
    "bucket = release_file.split('/')[0]\n",
    "file_key = release_file.split('/',1)[1]\n",
    "\n",
    "adm = pd.read_csv(f's3://{bucket}/{file_key}',sep='|')\n",
    "adm1 = adm[CUR_PROC_WK].tolist()\n",
    "\n",
    "for item in adm1:\n",
    "    if item.startswith('P_OUT_110_HCP-ZIP-DEFAULT_'):\n",
    "        HCPZD_TIMESTAMP = item.split('_')[-1].split('.')[0]\n",
    "    elif item.startswith('P_OUT_110_HCP_'):\n",
    "        HCP_TIMESTAMP = item.split('_')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f02764d-c641-48ad-b4fd-3f982c6c8847",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Library Names and File Paths :\n",
    "raw_path = f'PYADM/raw/{CUR_PROC_WK}/inbound/'\n",
    "qtrspec = f'PYADM/quaterly/{QTR}/reference/'\n",
    "curwk = f'PYADM/raw/{CUR_PROC_WK}/dataframes/'\n",
    "ref_pwek = f'PYADM/weekly/archive/{PRE_WK}/reference/'\n",
    "ref_week = 'PYADM/weekly/staging/reference/'\n",
    "ref_month = 'PYADM/monthly/staging/reference/'\n",
    "ZIP = f'PYADM/quaterly/{QTR}/geography/'\n",
    "jami = f'PYADM/reference/{QTR}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632229c-2833-47b1-a3e6-b539ba85981a",
   "metadata": {},
   "source": [
    "#### Importing ZIP Default Files-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "717d517a-4fab-460b-9cc8-d87486cf3611",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 14.7 ms, total: 211 ms\n",
      "Wall time: 787 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_CUSTOMER_ZD = pd.read_csv(\n",
    "    f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT_{HCPZD_TIMESTAMP}.TXT',\n",
    "    delimiter='|',\n",
    "    dtype={1:'Int64',37:'Int64',38:'Int64',32:str,17:str,29:str,20:str,18:str,26:str,21:str,15:str,23:str,25:str,24:str,22:str,14:str},\n",
    "    parse_dates=[31]\n",
    ")\n",
    "temp_CUSTOMER_ZD.rename(columns={'win_i_id': 'IronwoodWinnerID'}, inplace=True)\n",
    "\n",
    "temp_CUSTOMERFLAG_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-FLG_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_ADDRESS_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-ADDR_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype={1:str,11:str,10: str,6:str,12:str})\n",
    "\n",
    "temp_ADDRESSFLAG_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-ADDR-FLG_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "temp_ADDRESSFLAG_ZD.rename(columns={'AddressSourceID': 'AddressSourceId'}, inplace=True)\n",
    "\n",
    "temp_STATELICENSE_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-STATE-LIC_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_COMMUNICATION_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-COMM_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_SPECIALTY_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-SPECIALTIES_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype = {4:str,5:str,1:str})\n",
    "\n",
    "temp_DEA_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-DEA_{HCPZD_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_IDENTIFIER_ZD = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ZIP-DEFAULT-IDENTIFIERS_{HCPZD_TIMESTAMP}.TXT', delimiter='|', dtype={'IID':'Int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b2473-9a54-4af3-ad3a-65fad03e175a",
   "metadata": {},
   "source": [
    "#### Import HCP Files-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68935b50-c129-4ac0-98be-9d6e32e43a9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropped Rows from Dedup of Customer Component :  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:21: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:32: ParserWarning: Both a converter and dtype were specified for column IdentifierValue - only the converter will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 2.66 s, total: 26.4 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GET S3 OBJ function was faster by 10 seconds!!!\n",
    "temp_CUSTOMER1 = pd.read_csv(\n",
    "    f's3://{bucket}/{raw_path}P_OUT_110_HCP_{HCP_TIMESTAMP}.TXT',\n",
    "    delimiter='|',\n",
    "    dtype={1:'Int64',37:'Int64',38:'Int64',\n",
    "          32:str,6:str,17:str,29:str,20:str,18:str,26:str,21:str,15:str,23:str,25:str,24:str,22:str,14:str},\n",
    "    parse_dates=[31]\n",
    ")\n",
    "\n",
    "temp_CUSTOMER1.rename(columns={'win_i_id': 'IronwoodWinnerID'}, inplace=True)\n",
    "qc_before = len(temp_CUSTOMER1)\n",
    "# DEDUP on 'IID'\n",
    "temp_CUSTOMER1.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "qc_after = len(temp_CUSTOMER1)\n",
    "print(\" Dropped Rows from Dedup of Customer Component : \",qc_before-qc_after)\n",
    "\n",
    "temp_CUSTOMERFLAG1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-FLG_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_ADDRESS1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ADDR_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype={1:str,11:str,10: str,6:str,12:str}, on_bad_lines='warn')\n",
    "\n",
    "temp_ADDRESSFLAG1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-ADDR-FLG_{HCP_TIMESTAMP}.TXT', delimiter='|',dtype={1:str})\n",
    "temp_ADDRESSFLAG1.rename(columns={'AddressSourceID': 'AddressSourceId'}, inplace=True)\n",
    "\n",
    "temp_STATELICENSE1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-STATE-LIC_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_COMMUNICATION1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-COMM_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_SPECIALTY1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-SPECIALTIES_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype = {4:str,5:str,1:str})\n",
    "\n",
    "temp_DEA1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-DEA_{HCP_TIMESTAMP}.TXT', delimiter='|')\n",
    "\n",
    "temp_IDENTIFIER1 = pd.read_csv(f's3://{bucket}/{raw_path}P_OUT_110_HCP-IDENTIFIERS_{HCP_TIMESTAMP}.TXT', delimiter='|', dtype={'IID':'Int64',1:str,2:str},\n",
    "                               converters={'IdentifierValue': lambda x: str(x) if x != 'nan' else ''}\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869398e-b84f-477c-8964-55bd7e8ac7fc",
   "metadata": {},
   "source": [
    "FIX FOR MERGING ADDRESS AND FLAG (from both main and ZD sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72d273ea-8269-463b-b097-48f430cd7b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " temp_ADDRESS_ZD dropped :  0\n",
      " temp_ADDRESSFLAG_ZD dropped :  0\n",
      " temp_ADDRESS1 dropped :  0\n",
      " temp_ADDRESSFLAG1 dropped :  0\n",
      "CPU times: user 11.7 s, sys: 233 ms, total: 12 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# *Dedup Address Components first -;\n",
    "# THIS SHOULD HAVE ZERO DUPS\n",
    "\n",
    "qc_before = len(temp_ADDRESS_ZD)\n",
    "temp_ADDRESS_ZD.drop_duplicates(subset=['IID', 'AddressSourceId'], keep='first', inplace=True)\n",
    "# Adding Sorting Step\n",
    "temp_ADDRESS_ZD.sort_values('AddressSourceId',inplace=True,ignore_index=True)\n",
    "qc_after = len(temp_ADDRESS_ZD)\n",
    "print(\" temp_ADDRESS_ZD dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESSFLAG_ZD)\n",
    "temp_ADDRESSFLAG_ZD.drop_duplicates(subset=['IID', 'AddressSourceId','AddressFlagType'], keep='first', inplace=True)\n",
    "qc_after = len(temp_ADDRESSFLAG_ZD)\n",
    "print(\" temp_ADDRESSFLAG_ZD dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESS1)\n",
    "temp_ADDRESS1.drop_duplicates(subset=['IID', 'AddressSourceId'], keep='first', inplace=True)\n",
    "# Adding Sorting Step\n",
    "temp_ADDRESS1.sort_values('AddressSourceId',inplace=True,ignore_index=True)\n",
    "qc_after = len(temp_ADDRESS1)\n",
    "print(\" temp_ADDRESS1 dropped : \",qc_before-qc_after)\n",
    "\n",
    "qc_before = len(temp_ADDRESSFLAG1)\n",
    "temp_ADDRESSFLAG1.drop_duplicates(subset=['IID', 'AddressSourceId','AddressFlagType'], keep='first', inplace=True)\n",
    "qc_after = len(temp_ADDRESSFLAG1)\n",
    "print(\" temp_ADDRESSFLAG1 dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c79f626-b2dc-4140-bb1b-112349799523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Addr with Addr Flag ;\n",
    "temp_ADDRESS_ZD_F = pd.merge(temp_ADDRESS_ZD, temp_ADDRESSFLAG_ZD[['IID', 'AddressSourceId','AddressFlagType']],\n",
    "                             how = 'left',\n",
    "                             left_on = ['IID', 'AddressSourceId'],\n",
    "                             right_on = ['IID', 'AddressSourceId'])\n",
    "temp_ADDRESS_ZD=temp_ADDRESS_ZD_F.copy()\n",
    "\n",
    "temp_ADDRESS1_F = pd.merge(temp_ADDRESS1, temp_ADDRESSFLAG1[['IID', 'AddressSourceId','AddressFlagType']],\n",
    "                             how = 'left',\n",
    "                             left_on = ['IID', 'AddressSourceId'],\n",
    "                             right_on = ['IID', 'AddressSourceId'])\n",
    "temp_ADDRESS1=temp_ADDRESS1_F.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b7a78-d1e5-49be-a1a5-191586bbf787",
   "metadata": {},
   "source": [
    "##### JOIN ALL PARTITIONS AND ZIP DEFAULT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ce29ec1-5d18-45c4-972b-0588d31162c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 916 ms, sys: 329 ms, total: 1.24 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def comb_hcp(DF_HCP,DF_ZD):\n",
    "    combined_df = pd.concat([DF_HCP, DF_ZD], ignore_index=True)\n",
    "    return(combined_df)\n",
    "\n",
    "curwk_CUSTOMER = comb_hcp(temp_CUSTOMER1,temp_CUSTOMER_ZD) \n",
    "curwk_CUSTOMERFLAG = comb_hcp(temp_CUSTOMERFLAG1,temp_CUSTOMERFLAG_ZD) \n",
    "curwk_ADDRESS = comb_hcp(temp_ADDRESS1,temp_ADDRESS_ZD) \n",
    "curwk_STATELICENSE = comb_hcp(temp_STATELICENSE1,temp_STATELICENSE_ZD)\n",
    "curwk_COMMUNICATION = comb_hcp(temp_COMMUNICATION1,temp_COMMUNICATION_ZD) \n",
    "curwk_SPECIALTY = comb_hcp(temp_SPECIALTY1,temp_SPECIALTY_ZD) \n",
    "curwk_DEA = comb_hcp(temp_DEA1,temp_DEA_ZD) \n",
    "curwk_IDENTIFIER = comb_hcp(temp_IDENTIFIER1,temp_IDENTIFIER_ZD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98bb26-63bc-4a61-a785-4a744e4ed6ef",
   "metadata": {},
   "source": [
    "##### CREATE CURRENT WEEK CUSTOMER_MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5cc2bb-246d-4b98-a2ba-d6fd0c90154c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Date related columns from Customer Component need to be changed from text to datetime format so that filtering can be done later\n",
    "# Added this because 9999-12-31 is not supported by the datetime converter\n",
    "# So active Customer will now be designated by 2262-04-11 instead as its winthin supported date range\n",
    "\n",
    "curwk_CUSTOMER.loc[curwk_CUSTOMER['CustomerEffectiveEndDate'] == '9999-12-31', 'CustomerEffectiveEndDate'] = '2262-04-11'\n",
    "\n",
    "# Date Column type change for Identifier -\n",
    "curwk_CUSTOMER['CustomerEffectiveStartDate'] = pd.to_datetime(curwk_CUSTOMER['CustomerEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_CUSTOMER['CustomerEffectiveEndDate'] = pd.to_datetime(curwk_CUSTOMER['CustomerEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fec4dd4-6326-4045-a359-21dea21935eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " curwk_CUSTOMER dropped :  0\n"
     ]
    }
   ],
   "source": [
    "# Dedup CUSTOMER component\n",
    "qc_before = len(curwk_CUSTOMER)\n",
    "temp_CUSTOMER = curwk_CUSTOMER.sort_values(by='IID').drop_duplicates(subset='IID', keep='first')\n",
    "temp_CUSTDUP = curwk_CUSTOMER[curwk_CUSTOMER.duplicated(subset='IID', keep=False)]\n",
    "qc_after = len(temp_CUSTOMER)\n",
    "print(\" curwk_CUSTOMER dropped : \",qc_before-qc_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83ef3736-1d2f-425d-86a2-e8fdf3bd1fc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Date related columns from Address Component need to be changed from text to datetime format so that filtering can be done\n",
    "# Added this because 9999-12-31 is not supported by the datetime converter\n",
    "# So active addresses will now be designated by 2262-04-11 instead as its winthin supported date range\n",
    "curwk_ADDRESS.loc[curwk_ADDRESS['AddressEffectiveEndDate'] == '9999-12-31', 'AddressEffectiveEndDate'] = '2262-04-11'\n",
    "\n",
    "# Date Column type change for address - \n",
    "curwk_ADDRESS['AddressEffectiveStartDate'] = pd.to_datetime(curwk_ADDRESS['AddressEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_ADDRESS['AddressEffectiveEndDate'] = pd.to_datetime(curwk_ADDRESS['AddressEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c30060-62cd-4559-b517-51b5465fa33c",
   "metadata": {},
   "source": [
    "##### Dedup and CLean ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5e8987d-b41e-47fd-9893-c4e55bb74479",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  1864192 Records in curwk_ADDRESS\n",
      "There ARE  1862868 Records in temp_ADDRESS\n",
      "There ARE  1324 Records in temp_ADDRESSEND\n"
     ]
    }
   ],
   "source": [
    "# creating empty dataframes first\n",
    "temp_ADDRESSEND = pd.DataFrame() #this will store all the addresses that failed the filters\n",
    "temp_ADDRESS = pd.DataFrame() #final filterted result\n",
    "\n",
    "# Defining the conditions -\n",
    "condition1 = curwk_ADDRESS['AddressEffectiveEndDate'] < TODAY\n",
    "condition2 = curwk_ADDRESS['AddressStatusCode'] == 'Inactive'\n",
    "condition3 = curwk_ADDRESS['AddressEffectiveEndDate'].isna()  # Additional condition for NaT values\n",
    "\n",
    "# applying filter - \n",
    "temp_ADDRESSEND = curwk_ADDRESS[(condition1 | condition2 | condition3)].copy()\n",
    "temp_ADDRESS = curwk_ADDRESS[~(condition1 | condition2 | condition3)].copy() #ive simply used negagation '~' here to flip the condtions here\n",
    "\n",
    "#qc \n",
    "print(\"There were \",len(curwk_ADDRESS),\"Records in curwk_ADDRESS\")\n",
    "print(\"There ARE \",len(temp_ADDRESS),\"Records in temp_ADDRESS\")\n",
    "print(\"There ARE \",len(temp_ADDRESSEND),\"Records in temp_ADDRESSEND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d6987-9f6f-4baf-9de9-f76d1bfcec73",
   "metadata": {},
   "source": [
    "#### 2.1: Check each IID only has one BOB address -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90ba4765-448e-4019-a97c-218a834ef363",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Table for BOB Count:\n",
      "HCPs\n",
      "0      8618\n",
      "1    253112\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # QC Count the number of addresses with AddressFlagType = 'BOB' for each hcp\n",
    "# this approach is no longer working - \n",
    "# this code is calculating the number of times ‘BOB’ appears in the AddressFlagType column for each unique IID, and then creating a \n",
    "# frequency table of these counts. The frequency table shows how many IIDs have each possible count of ‘BOB’.\n",
    "\n",
    "bob_count = temp_ADDRESS[['IID','AddressFlagType']].copy()\n",
    "bob_count['HCPs'] = bob_count['AddressFlagType'].apply(lambda x: 1 if x == 'BOB' else 0)\n",
    "bob_count_grouped = bob_count.groupby('IID')['HCPs'].sum()\n",
    "\n",
    "# Frequency table\n",
    "frequency_table = bob_count_grouped.value_counts().sort_index()\n",
    "print(\"Frequency Table for BOB Count:\")\n",
    "print(frequency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3dc68c-83fc-43a1-98ac-cbb9812dbac8",
   "metadata": {},
   "source": [
    "### BOB filtering notes : \n",
    "#### from my understanding of the sas codes we have 5 cases to consider - \n",
    "* case 1- Only 1 Address, and Count of BOB flag = 1\n",
    "- case 2- More than 1 Address, and Count of BOB flag = 1 [in this case we only keep the addr with bob flag]\n",
    "- case 3- More than 1 Address, and Count of BOB flag = 0 / Null [I.e no BOB flag] , [ In this case we keep the first obs]\n",
    "- case 4- Only 1 Address, and Count of BOB flag = 0 / Null [I.e no BOB flag], [In this case the assumption is the only avaialble address is the best address so we keep it]\n",
    "- case 5- Count of BOB flag grater than 1 , Outlier case , this implys there are multiple addresses with BOB flag Droppinging this case for now\n",
    "Line  \n",
    "*We were NOT assinging BOB to cases with only one addr and no bob flag , contrary to previous understanding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a0119d2-b9e6-4abc-891b-17f14c991abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOB Clean and Dedup by IID\n",
    "temp_test1 = temp_ADDRESS.copy() #creating a new df to do processing for BOB flag filtering\n",
    "\n",
    "#adding 2 flags to temp_test1 for calculation and filtering - \n",
    "Co_Addr = temp_test1.groupby('IID').size()\n",
    "Co_Addr.name = 'Co_Addr' #count of addresses [ we can use hub_id to count number of addresses for each hub_id]\n",
    "\n",
    "Co_BOB = temp_test1[temp_test1['AddressFlagType'] == 'BOB'].groupby('IID').size()\n",
    "Co_BOB.name = 'Co_BOB'\n",
    "\n",
    "temp_test1 = temp_test1.merge(Co_Addr, on='IID', how='left')\n",
    "temp_test1 = temp_test1.merge(Co_BOB, on='IID', how='left')\n",
    "#qc\n",
    "#temp_test1.shape\n",
    "\n",
    "# following are dataframes made by filtering temp_test1\n",
    "# we filter it to create sub components to cater each condition , then we merge them back together\n",
    "\n",
    "temp_test2 = temp_test1[temp_test1['Co_Addr'] == 1].copy() #case 1 , 4\n",
    "temp_test2.reset_index(drop=True, inplace=True) #as long as there is one address keeping it , BOB or not\n",
    "#print(temp_test2.shape)\n",
    "\n",
    "temp_test3 = temp_test1[(temp_test1['Co_Addr'] > 1) & (temp_test1['Co_BOB'].isnull() | (temp_test1['Co_BOB'] == 0))].copy() #case 3\n",
    "temp_test3.reset_index(drop=True, inplace=True)\n",
    "temp_test3.drop_duplicates(subset='IID', keep='first', inplace=True) #only keeping first occurance\n",
    "#print(temp_test3.shape)\n",
    "\n",
    "temp_test4 = temp_test1[(temp_test1['Co_Addr'] > 1) & (temp_test1['Co_BOB'] == 1)].copy() #case 2\n",
    "temp_test4 = temp_test4[temp_test4['AddressFlagType'] == 'BOB'].copy()\n",
    "#print(temp_test4.shape) # only keeping instances with BOB\n",
    "\n",
    "temp_test5 = temp_test1[temp_test1['Co_BOB'] > 1].copy() #case 5 [outlier scenario]\n",
    "#print(temp_test5.shape)\n",
    "\n",
    "# Combining Components here - \n",
    "# Concatenate temp_test2, temp_test3, and temp_test4 into temp_Addr_all\n",
    "temp_Addr_all = pd.concat([temp_test2, temp_test3, temp_test4], ignore_index=True)\n",
    "#print(temp_Addr_all.shape)\n",
    "\n",
    "#Deduping final Filtered Dataset by IID [or IID in our case] -\n",
    "\n",
    "# Keep only the first occurrence of each IID and drop the duplicates\n",
    "temp_ADDRESSNODUP = temp_Addr_all.drop_duplicates(subset='IID', keep='first')\n",
    "temp_ADDRESSNODUP = temp_ADDRESSNODUP.drop(columns=['Co_Addr', 'Co_BOB']) # FINAL DATASET READY\n",
    "\n",
    "# Save the duplicates in a separate DataFrame [This may be a redundant practice carried over from sas]\n",
    "temp_AddressDup = temp_Addr_all[temp_Addr_all.duplicated(subset='IID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccb8c214-c799-434d-aeeb-a80db9c2ad70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IID</th>\n",
       "      <th>AddressSourceId</th>\n",
       "      <th>AddressType</th>\n",
       "      <th>AddressLine1</th>\n",
       "      <th>AddressLine2</th>\n",
       "      <th>AddressLine3</th>\n",
       "      <th>AddressLine4</th>\n",
       "      <th>CityName</th>\n",
       "      <th>StateCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>...</th>\n",
       "      <th>LongitudeNumber</th>\n",
       "      <th>AddressStatusCode</th>\n",
       "      <th>AddressEffectiveStartDate</th>\n",
       "      <th>AddressEffectiveEndDate</th>\n",
       "      <th>PartnerID</th>\n",
       "      <th>PartnerName</th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>AddressFlagType</th>\n",
       "      <th>AddressFlagType_x</th>\n",
       "      <th>AddressFlagType_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128708</th>\n",
       "      <td>1029</td>\n",
       "      <td>934210356499123054</td>\n",
       "      <td>Professional</td>\n",
       "      <td>820 S McClellan St Ste 300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spokane</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>-117.41511</td>\n",
       "      <td>Active</td>\n",
       "      <td>2012-08-19</td>\n",
       "      <td>2262-04-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KMK</td>\n",
       "      <td>100202410281443371029</td>\n",
       "      <td>BOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         IID     AddressSourceId   AddressType                AddressLine1  \\\n",
       "128708  1029  934210356499123054  Professional  820 S McClellan St Ste 300   \n",
       "\n",
       "       AddressLine2 AddressLine3 AddressLine4 CityName StateCode CountryCode  \\\n",
       "128708          NaN          NaN          NaN  Spokane        WA          US   \n",
       "\n",
       "        ... LongitudeNumber AddressStatusCode AddressEffectiveStartDate  \\\n",
       "128708  ...      -117.41511            Active                2012-08-19   \n",
       "\n",
       "        AddressEffectiveEndDate  PartnerID PartnerName          TransactionID  \\\n",
       "128708               2262-04-11        NaN         KMK  100202410281443371029   \n",
       "\n",
       "       AddressFlagType  AddressFlagType_x AddressFlagType_y  \n",
       "128708             BOB                NaN               NaN  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ADDRESSNODUP[temp_ADDRESSNODUP['IID']==1029]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af71127-982b-4d42-bee6-755e1a1ce729",
   "metadata": {},
   "source": [
    "#### Dedup and Clean SPECIALTY -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cced605-90f6-4070-9bc4-10220cbcfc53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "curwk_SPECIALTY.loc[curwk_SPECIALTY['SpecialtyEffectiveEndDate'] == '9999-12-31', 'SpecialtyEffectiveEndDate'] = '2262-04-11'\n",
    "# Date Column type change for address -\n",
    "curwk_SPECIALTY['SpecialtyEffectiveStartDate'] = pd.to_datetime(curwk_SPECIALTY['SpecialtyEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_SPECIALTY['SpecialtyEffectiveEndDate'] = pd.to_datetime(curwk_SPECIALTY['SpecialtyEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce87530-6aee-485b-b767-8a66aea40881",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IID dropped :  0\n"
     ]
    }
   ],
   "source": [
    "temp_SPECIALTY = curwk_SPECIALTY.copy()\n",
    "qc_before = len(temp_SPECIALTY)\n",
    "temp_SPECSORT = temp_SPECIALTY.sort_values(by = 'IID').drop_duplicates(subset='IID', keep='first')\n",
    "temp_SPECDUP = temp_SPECIALTY[temp_SPECIALTY.duplicated(subset='IID', keep=False)]\n",
    "qc_after = len(temp_SPECSORT)\n",
    "print(\" IID dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1a2eb5-03db-4d5f-a3ea-9ae3a8df4afc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SpecialtyCode dropped :  0\n"
     ]
    }
   ],
   "source": [
    "# new Specialty Check\n",
    "qc_before = len(temp_SPECIALTY)\n",
    "temp_SPECIALTYSORT = temp_SPECIALTY.sort_values(by = 'SpecialtyCode').drop_duplicates(subset='SpecialtyCode', keep='first')\n",
    "temp_SPECIALTYDUP = temp_SPECIALTY[temp_SPECIALTY.duplicated(subset='SpecialtyCode', keep=False)]\n",
    "qc_after = len(temp_SPECSORT)\n",
    "print(\" SpecialtyCode dropped : \",qc_before-qc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f85e02a-cfea-43f2-b6ec-38042411ac4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3 List of Spec_Cds not avialable in qtrspec_SPEC_INCL_LIN : \n",
      "['ASO', 'GLAU', 'TY', 'URPS', 'VEC', 'VEM', 'VEN', 'VES', 'VEU', nan]\n"
     ]
    }
   ],
   "source": [
    "# new spec check\n",
    "# Fetching Data from stored location - \n",
    "qtrspec_SPEC_INCL_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}qtrspec_SPEC_INCL_LIN.parquet')\n",
    "\n",
    "# Merging with qaterly file to find new specs -\n",
    "temp_CHECK_SPECIALTY = pd.merge(temp_SPECIALTYSORT, qtrspec_SPEC_INCL_LIN, how='left', left_on='SpecialtyCode', right_on='SPECIALTY_CD', indicator=True)\n",
    "# Filter the merged_data using conditions A AND NOT B\n",
    "temp_CHECK_SPECIALTY = temp_CHECK_SPECIALTY[(temp_CHECK_SPECIALTY['_merge'] == 'left_only')]\n",
    "# Drop the _merge column as its not needed\n",
    "temp_CHECK_SPECIALTY.drop('_merge', axis=1, inplace=True)\n",
    "temp_CHECK_SPECIALTY = temp_CHECK_SPECIALTY[['SpecialtyCode']]\n",
    "\n",
    "#Print Results -\n",
    "if temp_CHECK_SPECIALTY.empty:\n",
    "    print(\"No new specialty\")\n",
    "else:\n",
    "    print(\"2.3 List of Spec_Cds not avialable in qtrspec_SPEC_INCL_LIN : \")\n",
    "    print(temp_CHECK_SPECIALTY['SpecialtyCode'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1c15b-19a8-4770-8a77-f60776402005",
   "metadata": {},
   "source": [
    "##### process IDENTIFIERS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b45e911-a055-43af-a62e-fcd2e0b194c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#fix max end date -\n",
    "curwk_IDENTIFIER.loc[curwk_IDENTIFIER['IdentifierEffectiveEndDate'] == '9999-12-31', 'IdentifierEffectiveEndDate'] = '2262-04-11'\n",
    "curwk_IDENTIFIER['IdentifierEffectiveStartDate'] = pd.to_datetime(curwk_IDENTIFIER['IdentifierEffectiveStartDate'], format='%Y-%m-%d', errors='coerce')\n",
    "curwk_IDENTIFIER['IdentifierEffectiveEndDate'] = pd.to_datetime(curwk_IDENTIFIER['IdentifierEffectiveEndDate'], format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "874f298c-3070-48a6-8bc3-48827050ac43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#NPI\n",
    "NPI_CUR_PROC_WK = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'NPI'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierValue'] != \"\"\n",
    "condition4 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "\n",
    "NPI_CUR_PROC_WK = curwk_IDENTIFIER[(condition1 & condition2 & condition3 & ~(condition4))].copy()\n",
    "NPI_CUR_PROC_WK.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "\n",
    "NPI_CUR_PROC_WK.rename(columns={'IdentifierValue': 'NPI_ID','IdentifierEffectiveStartDate':'NPI_StartDate','IdentifierEffectiveEndDate':'NPI_EndDate'}, inplace=True)\n",
    "NPI_CUR_PROC_WK = NPI_CUR_PROC_WK[['IID','NPI_ID','NPI_StartDate','NPI_EndDate']]\n",
    "\n",
    "#Note - Historically Npi Datasets from all weeks are combined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c99643e7-c107-4ec3-a8f0-274f0fcfa02f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ME\n",
    "temp_IDENTIFIER_ME = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'ME'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "temp_IDENTIFIER_ME = curwk_IDENTIFIER[(condition1 & condition2 & ~(condition3))].copy()\n",
    "\n",
    "temp_IDENTIFIERSORT = temp_IDENTIFIER_ME.copy()\n",
    "temp_IDENTIFIERSORT.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "temp_IDENTIFIERSORT.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c56c1820-5113-4bf5-a80b-304b3bd78f94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Saw Cases of Records where both ME ID and NPI ID where aviailable for the HCP\n",
    "temp_ME_NPI = temp_IDENTIFIERSORT.merge(NPI_CUR_PROC_WK, how='outer', on='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2ad92-e51e-4d50-9edc-79eb6734fd36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Taxonomy\n",
    "temp_IDENTIFIER_TAXONOMY = pd.DataFrame()\n",
    "condition1 = curwk_IDENTIFIER['IdentifierType'] == 'TAXONOMY'\n",
    "condition2 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'] > TODAY\n",
    "condition3 = curwk_IDENTIFIER['IdentifierEffectiveEndDate'].isna()\n",
    "temp_IDENTIFIER_TAXONOMY = curwk_IDENTIFIER[(condition1 & condition2 & ~(condition3))].copy()\n",
    "\n",
    "temp_IDENTIFIER2SORT = temp_IDENTIFIER_TAXONOMY.copy()\n",
    "temp_IDENTIFIER2SORT.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "temp_IDENTIFIER2SORT.reset_index(drop=True, inplace=True)\n",
    "\n",
    "temp_IDENTIFIER2SORT.rename(columns={'IdentifierValue': 'TAXONOMYID'}, inplace=True)\n",
    "temp_IDENTIFIER2SORT = temp_IDENTIFIER2SORT[['IID','TAXONOMYID']]\n",
    "\n",
    "####\n",
    "temp_ME_NPI_TAX = temp_ME_NPI.merge(temp_IDENTIFIER2SORT, how='outer', on='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09475048-1e13-4b1f-af77-1e7bbfe15baa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#temp_IDENTIFIERS = pd.concat([NPI_CUR_PROC_WK,temp_IDENTIFIERSORT,temp_IDENTIFIER2SORT],axis = 0)\n",
    "#temp_IDENTIFIERS = pd.concat([temp_IDENTIFIERSORT,temp_IDENTIFIER2SORT],axis = 0)\n",
    "temp_IDENTIFIERS=temp_ME_NPI_TAX.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7553211-3a4b-490e-a897-fa03c8722351",
   "metadata": {},
   "source": [
    "### Creating Final Weekly Master Profile Delta -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee06c668-7122-4c1a-b632-6aa86c3b6c75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Create Weekly Delta Customer Master\n",
    "curwk_CUSTOMER_MASTER = pd.DataFrame()\n",
    "\n",
    "# I am assuming that for the case of overlapping columns , we want to keep stuff from Customer Component only\n",
    "# Perform the merge operation ( second line is drop the overlapping columns from right dataset)\n",
    "curwk_CUSTOMER_MASTER = pd.merge(temp_CUSTOMER, temp_ADDRESSNODUP, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]\n",
    "\n",
    "curwk_CUSTOMER_MASTER = pd.merge(curwk_CUSTOMER_MASTER, temp_SPECSORT, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]\n",
    "\n",
    "curwk_CUSTOMER_MASTER = pd.merge(curwk_CUSTOMER_MASTER, temp_IDENTIFIERS, on='IID', how='left',suffixes=(None, '_y'))\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER[[col for col in curwk_CUSTOMER_MASTER.columns if not col.endswith('_y')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143b05b-a3f2-4a59-9989-afc04ea1600a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Final Changes - \n",
    "\n",
    "# Rename columns and change data types\n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER.rename(columns={'PostalCode': 'ZIP', 'SecondaryID': 'IMSID'})\n",
    "curwk_CUSTOMER_MASTER['IMSID'] = curwk_CUSTOMER_MASTER['IMSID'].astype(str)\n",
    "\n",
    "#Coluns that need to be dropped ? :\n",
    "curwk_CUSTOMER_MASTER.drop(columns={'TransactionID' , 'PartnerID' , 'PartnerLoserID' ,'PartnerWinnerID', 'PartnerName'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "curwk_CUSTOMER_MASTER.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004e4ce-9b45-4065-aeeb-af94a121dc36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Temp fix for Identifer \n",
    "curwk_CUSTOMER_MASTER = curwk_CUSTOMER_MASTER.astype({'IdentifierValue': str})\n",
    "#print(curwk_CUSTOMER_MASTER['IdentifierValue'].apply(type).value_counts())\n",
    "\n",
    "curwk_CUSTOMER_MASTER.to_parquet(f's3://{bucket}/{curwk}curwk_CUSTOMER_MASTER.parquet')\n",
    "print('weekly master profile delta ready')\n",
    "curwk_CUSTOMER_MASTER.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3e72a-ad4e-48d0-974a-d0d37c13cd91",
   "metadata": {},
   "source": [
    "### 3 - Profiles Delta- Create HCP Universe, Adds, Updates, Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "439acde1-4444-429e-ab9f-fb1ea4d28557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ZIPL QC : ---\n",
      "Total Number of Records : 234830\n",
      "Number of Records with Invalid Zip :  1414\n",
      "NUmber of Records with Invalid Zip and Active Status : 1234\n",
      "Threshhold check 0.5254865221649704  This value tells us what percentage of active HCPs in the delta are going to whitespace\n"
     ]
    }
   ],
   "source": [
    "#ZIP length check\n",
    "temp_ZIPL_CHECK = curwk_CUSTOMER_MASTER.copy()\n",
    "temp_ZIPL_CHECK['ZIPL'] = temp_ZIPL_CHECK['ZIP'].str.len() #adding new column for zip length\n",
    "all_rec = len(temp_ZIPL_CHECK)\n",
    "temp_ZIPL_CHECK = temp_ZIPL_CHECK[temp_ZIPL_CHECK['ZIPL'] != 5].copy() #filtering records where zip length is not 5 [non compliant zip code]\n",
    "invalid_zip = len(temp_ZIPL_CHECK)\n",
    "temp_ZIPL_CHECK = temp_ZIPL_CHECK[temp_ZIPL_CHECK['CustomerStatusCode'] == 'Active'].copy() #[then filtering out how many of them are for active hcps]\n",
    "act_invalid_zip = len(temp_ZIPL_CHECK)\n",
    "print(\"--- ZIPL QC : ---\")\n",
    "print(\"Total Number of Records :\",all_rec)\n",
    "print(\"Number of Records with Invalid Zip : \",invalid_zip)\n",
    "print(\"NUmber of Records with Invalid Zip and Active Status :\",act_invalid_zip)\n",
    "print(\"Threshhold check\",(act_invalid_zip/all_rec)*100,\" This value tells us what percentage of active HCPs in the delta are going to whitespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7223127a-6bb1-4277-8736-722660957d90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 69)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATE ADDS, UPDATES, AND MERGES DATASETS -\n",
    "temp_ADDS_UPDATES = pd.DataFrame() #This will hold the New HPCs and Updates\n",
    "curwk_MERGES = pd.DataFrame() #This will hold the Winner / Looser ID HCPs\n",
    "enddate_CUST_ENDDATE_CUR_PROC_WK = pd.DataFrame() #This will hold the end date crossed HCPs\n",
    "\n",
    "#defining conditions - \n",
    "cond1 = curwk_CUSTOMER_MASTER['IronwoodWinnerID'].isna() \n",
    "cond2 = curwk_CUSTOMER_MASTER['IronwoodLoserID'].isna()\n",
    "cond3 = curwk_CUSTOMER_MASTER['CustomerEffectiveEndDate'] < TODAY\n",
    "\n",
    "#applying conditions -\n",
    "curwk_MERGES = curwk_CUSTOMER_MASTER[(~(cond1) & ~(cond2) & cond3)].copy()\n",
    "enddate_CUST_ENDDATE_CUR_PROC_WK = curwk_CUSTOMER_MASTER[((cond1 | cond2) & cond3)].copy()\n",
    "temp_ADDS_UPDATES = curwk_CUSTOMER_MASTER[curwk_CUSTOMER_MASTER['CustomerEffectiveEndDate'] >= TODAY]\n",
    "\n",
    "#storing\n",
    "curwk_MERGES.to_parquet(f's3://{bucket}/{curwk}curwk_MERGES.parquet')\n",
    "curwk_MERGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c813abae-f13d-48f0-b24d-d648142d9fa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294578, 69)\n",
      "(1545239, 69)\n",
      "(1554, 69)\n"
     ]
    }
   ],
   "source": [
    "#Separating Addtions and Updates\n",
    "\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = pd.DataFrame() #this will hold updates\n",
    "temp_HCP_ADDS_CUR_PROC_WK = pd.DataFrame() #this will hold new hcps\n",
    "temp_b =pd.DataFrame()\n",
    "\n",
    "MASTER_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet') # fetching previous week's full master profile\n",
    "ref_pwek_MASTER_PROFILE = pd.DataFrame(MASTER_PROFILE['IID'].copy())\n",
    "#only keeping IIDs (reason being, if they are update cases , we want all the columns to be repalced)\n",
    "\n",
    "#merging the datasets , will filter based on indicator (_merge column)\n",
    "merged_df = pd.merge(ref_pwek_MASTER_PROFILE,temp_ADDS_UPDATES,on='IID',how='outer',indicator=True)\n",
    "\n",
    "#separating\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = merged_df[merged_df['_merge'] == 'both']\n",
    "temp_HCP_UPDATES_CUR_PROC_WK = temp_HCP_UPDATES_CUR_PROC_WK.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "temp_b = merged_df[merged_df['_merge'] == 'left_only']\n",
    "temp_b = temp_b.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "temp_HCP_ADDS_CUR_PROC_WK = merged_df[merged_df['_merge'] == 'right_only']\n",
    "temp_HCP_ADDS_CUR_PROC_WK = temp_HCP_ADDS_CUR_PROC_WK.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "#qc -\n",
    "print(temp_HCP_UPDATES_CUR_PROC_WK.shape)\n",
    "print(temp_b.shape)\n",
    "print(temp_HCP_ADDS_CUR_PROC_WK.shape)\n",
    "\n",
    "#Adding Columns -\n",
    "adds_HCP_ADDS_CUR_PROC_WK = temp_HCP_ADDS_CUR_PROC_WK.copy() \\\n",
    ".assign(RECORD_START_DATE=DATA_DATE_CALENDAR, RECORD_END_DATE=RECORD_END_DATE)\n",
    "\n",
    "updates_HCP_UPDATES_CUR_PROC_WK = temp_HCP_UPDATES_CUR_PROC_WK.copy() \\\n",
    ".assign(RECORD_START_DATE=DATA_DATE_CALENDAR, RECORD_END_DATE=RECORD_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a08ca6-8d69-4fbd-b9c4-57148bfac469",
   "metadata": {},
   "source": [
    "#### CREATE WEEKLY FULL UNIVERSE CUSTOMER MASTER    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18f87fc5-e036-46a8-863c-81315e78932e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp_CUSTOMER_MASTER_PRE_PROC_WK = pd.DataFrame() #intermediate dataset[ this has all the exclusions applied -END, UPDATE, AND MERGED IID ]\n",
    "\n",
    "ref_pwek_MASTER_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet') # fetching previous week's full master profile\n",
    "\n",
    "all_exclusions = pd.DataFrame({\n",
    "    'IID': pd.concat([\n",
    "        updates_HCP_UPDATES_CUR_PROC_WK['IID'],\n",
    "        curwk_MERGES['IID'],\n",
    "        enddate_CUST_ENDDATE_CUR_PROC_WK['IID']\n",
    "    ], ignore_index=True).unique()\n",
    "})\n",
    "\n",
    "#ALternative Approach , useing isin() method, but it may not be vaible for very large datasets [DOUBT]\n",
    "#ref_pwek_MASTER_PROFILE = ref_pwek_MASTER_PROFILE[~ref_pwek_MASTER_PROFILE['hub_id'].isin(all_exclusions['hub_id'])]\n",
    "\n",
    "#going with merge + filter for now as it may be more efficient - \n",
    "\n",
    "#merging the datasets , will filter based on indicator (_merge column)\n",
    "merged_df = pd.merge(ref_pwek_MASTER_PROFILE,all_exclusions,on='IID',how='left',indicator=True)\n",
    "temp_CUSTOMER_MASTER_PRE_PROC_WK = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1bef0be-7364-40c8-b852-7c154df3296d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of losers found in the previous week master profile :  2867\n",
      "Instances inisde mergecass where IID = WinnerID\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IID</th>\n",
       "      <th>CustomerContactType</th>\n",
       "      <th>CustomerStatusCode</th>\n",
       "      <th>IMSID</th>\n",
       "      <th>SecondaryIDSourceName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>MiddleName</th>\n",
       "      <th>SuffixName</th>\n",
       "      <th>PrefixName</th>\n",
       "      <th>...</th>\n",
       "      <th>IdentifierType</th>\n",
       "      <th>IdentifierValue</th>\n",
       "      <th>IdentifierStateProvinceCode</th>\n",
       "      <th>IdentifierStatusCode</th>\n",
       "      <th>IdentifierEffectiveStartDate</th>\n",
       "      <th>IdentifierEffectiveEndDate</th>\n",
       "      <th>NPI_ID</th>\n",
       "      <th>NPI_StartDate</th>\n",
       "      <th>NPI_EndDate</th>\n",
       "      <th>TAXONOMYID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [IID, CustomerContactType, CustomerStatusCode, IMSID, SecondaryIDSourceName, FirstName, LastName, MiddleName, SuffixName, PrefixName, DisplayName, PreferredName, FormerName, Gender, BirthYear, DeceasedYear, ProfessionalDesignation, CompanyName, DepartmentName, JobTitle, GraduateSchoolName, GraduationYear, GraduationStateCode, DegreeName, AccountType, AccountSubType, ClassofTradeCode, PDRPOptOutFlag, PDRPOptOutDate, MatchCode, CustomerEffectiveStartDate, CustomerEffectiveEndDate, IronwoodWinnerID, IronwoodLoserID, AddressSourceId, AddressType, AddressLine1, AddressLine2, AddressLine3, AddressLine4, CityName, StateCode, CountryCode, ZIP, AddressSitePhone, AddressSiteEmail, LatitudeNumber, LongitudeNumber, AddressStatusCode, AddressEffectiveStartDate, AddressEffectiveEndDate, AddressFlagType, SpecialtyRank, SpecialtyCode, SpecialtyDescription, SpecialtyGroupCode, SpecialtyGroupDescription, SpecialtyEffectiveStartDate, SpecialtyEffectiveEndDate, IdentifierType, IdentifierValue, IdentifierStateProvinceCode, IdentifierStatusCode, IdentifierEffectiveStartDate, IdentifierEffectiveEndDate, NPI_ID, NPI_StartDate, NPI_EndDate, TAXONOMYID]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 69 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qc -\n",
    "temp_CHECK_MERGE = pd.DataFrame()\n",
    "ref_pwek_MASTER_PROFILE = pd.DataFrame(ref_pwek_MASTER_PROFILE['IID'].copy())\n",
    "\n",
    "temp_CHECK_MERGE = pd.merge(ref_pwek_MASTER_PROFILE,(pd.DataFrame(curwk_MERGES['IID'])),on='IID',how='outer',indicator=True)\n",
    "temp_CHECK_MERGE = temp_CHECK_MERGE[temp_CHECK_MERGE['_merge'] == 'right_only']\n",
    "print(\"# of losers found in the previous week master profile : \",len(temp_CHECK_MERGE))\n",
    "print(\"Instances inisde mergecass where IID = WinnerID\")\n",
    "df_equal  = curwk_MERGES[curwk_MERGES['IID'] == curwk_MERGES['IronwoodWinnerID']]\n",
    "df_equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f34e0b76-4a06-4766-9d90-d52d01c86604",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = pd.DataFrame()\n",
    "\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = pd.concat([adds_HCP_ADDS_CUR_PROC_WK,\n",
    "                                              updates_HCP_UPDATES_CUR_PROC_WK,\n",
    "                                              temp_CUSTOMER_MASTER_PRE_PROC_WK])\n",
    "\n",
    "# Reset the index\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK = temp_CUSTOMER_MASTER_CUR_PROC_WK.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e42c5a28-5134-4dbd-b52c-383a964340a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records dropped: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1839689, 72)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort | Drop Dups | Store Data\n",
    "temp_CUSTOMER_MASTER_CUR_PROC_WK.sort_values('IID',inplace=True)\n",
    "ref_week_MASTER_PROFILE = temp_CUSTOMER_MASTER_CUR_PROC_WK.drop_duplicates('IID')\n",
    "num_duplicates_dropped = len(temp_CUSTOMER_MASTER_CUR_PROC_WK) - len(ref_week_MASTER_PROFILE)\n",
    "print(\"Number of duplicate records dropped:\", num_duplicates_dropped)\n",
    "\n",
    "\n",
    "#File ready -\n",
    "MASTER_PROFILE = ref_week_MASTER_PROFILE.copy()\n",
    "MASTER_PROFILE.to_parquet(f's3://{bucket}/{ref_week}MASTER_PROFILE.parquet')\n",
    "MASTER_PROFILE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9e035-8616-4060-9ab8-f8a562c5918a",
   "metadata": {},
   "source": [
    "#### Duplicate Specialty Descriptions Check (3.2) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4425c8f9-50d3-4c9f-9e29-69b62971dac1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid Specialties: Index(['ABDOMINAL', 'CARDIOVASC', 'CERTIFIED', 'INTERNAL M', 'PEDIATRIC'], dtype='object', name='SpecialtyDescription')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpecialtyCode</th>\n",
       "      <th>SpecialtyDescription</th>\n",
       "      <th>SPECIALTY_CD</th>\n",
       "      <th>SPECIALTY_DESCRIPTION</th>\n",
       "      <th>SPEC_INCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AS</td>\n",
       "      <td>ABDOMINAL</td>\n",
       "      <td>AS</td>\n",
       "      <td>ABDOMINAL SURGERY</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>AR</td>\n",
       "      <td>ABDOMINAL</td>\n",
       "      <td>AR</td>\n",
       "      <td>ABDOMINAL RADIOLOGY</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CD</td>\n",
       "      <td>CARDIOVASC</td>\n",
       "      <td>CD</td>\n",
       "      <td>CARDIOVASCULAR DISEASE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>CDS</td>\n",
       "      <td>CARDIOVASC</td>\n",
       "      <td>CDS</td>\n",
       "      <td>CARDIOVASCULAR SURGERY</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>CNM</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>CNM</td>\n",
       "      <td>CERTIFIED NURSE MIDWIFE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>CNA</td>\n",
       "      <td>CERTIFIED</td>\n",
       "      <td>CNA</td>\n",
       "      <td>CERTIFIED NURSE ANESTHETIST</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MP</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MP</td>\n",
       "      <td>INTERNAL MEDICINE-PSYCHIATRY</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MPD</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MPD</td>\n",
       "      <td>INTERNAL MEDICINE/PEDIATRICS</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>IFP</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IFP</td>\n",
       "      <td>INTERNAL MEDICINE/FAMILY PRACTICE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>MPM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MPM</td>\n",
       "      <td>INTERNAL MEDICINE/REHABILITAION</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>IPM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IPM</td>\n",
       "      <td>INTERNAL MEDICINE-PREVENTIVE MEDICINE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>MEM</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>MEM</td>\n",
       "      <td>INTERNAL MED-EMERGENCY MED</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>IEC</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IEC</td>\n",
       "      <td>INTERNAL MED/EMERGENCY MED/CRITICAL CARE MED (...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>IMD</td>\n",
       "      <td>INTERNAL M</td>\n",
       "      <td>IMD</td>\n",
       "      <td>INTERNAL MEDICINE/DERMATOLOGY (RESIDENTS ONLY)</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCP</td>\n",
       "      <td>PEDIATRIC</td>\n",
       "      <td>CCP</td>\n",
       "      <td>PEDIATRIC CRITICAL CARE MEDICINE</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PDS</td>\n",
       "      <td>PEDIATRIC</td>\n",
       "      <td>PDS</td>\n",
       "      <td>PEDIATRIC SURGERY (SURGERY)</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SpecialtyCode SpecialtyDescription SPECIALTY_CD  \\\n",
       "1              AS            ABDOMINAL           AS   \n",
       "358            AR            ABDOMINAL           AR   \n",
       "6              CD           CARDIOVASC           CD   \n",
       "246           CDS           CARDIOVASC          CDS   \n",
       "317           CNM            CERTIFIED          CNM   \n",
       "318           CNA            CERTIFIED          CNA   \n",
       "247            MP           INTERNAL M           MP   \n",
       "248           MPD           INTERNAL M          MPD   \n",
       "249           IFP           INTERNAL M          IFP   \n",
       "290           MPM           INTERNAL M          MPM   \n",
       "343           IPM           INTERNAL M          IPM   \n",
       "344           MEM           INTERNAL M          MEM   \n",
       "366           IEC           INTERNAL M          IEC   \n",
       "372           IMD           INTERNAL M          IMD   \n",
       "4             CCP            PEDIATRIC          CCP   \n",
       "41            PDS            PEDIATRIC          PDS   \n",
       "\n",
       "                                 SPECIALTY_DESCRIPTION SPEC_INCL  \n",
       "1                                    ABDOMINAL SURGERY         N  \n",
       "358                                ABDOMINAL RADIOLOGY         Y  \n",
       "6                               CARDIOVASCULAR DISEASE         Y  \n",
       "246                             CARDIOVASCULAR SURGERY         N  \n",
       "317                            CERTIFIED NURSE MIDWIFE         Y  \n",
       "318                        CERTIFIED NURSE ANESTHETIST         N  \n",
       "247                       INTERNAL MEDICINE-PSYCHIATRY         Y  \n",
       "248                       INTERNAL MEDICINE/PEDIATRICS         Y  \n",
       "249                  INTERNAL MEDICINE/FAMILY PRACTICE         Y  \n",
       "290                    INTERNAL MEDICINE/REHABILITAION         N  \n",
       "343              INTERNAL MEDICINE-PREVENTIVE MEDICINE         Y  \n",
       "344                         INTERNAL MED-EMERGENCY MED         Y  \n",
       "366  INTERNAL MED/EMERGENCY MED/CRITICAL CARE MED (...         Y  \n",
       "372     INTERNAL MEDICINE/DERMATOLOGY (RESIDENTS ONLY)         Y  \n",
       "4                     PEDIATRIC CRITICAL CARE MEDICINE         Y  \n",
       "41                         PEDIATRIC SURGERY (SURGERY)         N  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_spec_cds = ref_week_MASTER_PROFILE[['SpecialtyCode', 'SpecialtyDescription']].copy()#get the columns i need for master profile\n",
    "unique_spec_cds.drop_duplicates(subset='SpecialtyCode', keep='first', inplace=True)#drop drop all duplicates so i only have unique speciality codes\n",
    "unique_spec_cds.reset_index(drop=True, inplace=True) #reset index\n",
    "unique_spec_cds_incl = pd.merge(unique_spec_cds,qtrspec_SPEC_INCL_LIN,left_on ='SpecialtyCode',right_on='SPECIALTY_CD',how='left') # joining so i get the spec_incl  flag\n",
    "# Group by SPECIALTY_DESCRIPTION and count unique spec_incl values\n",
    "specialty_counts = unique_spec_cds_incl.groupby('SpecialtyDescription')['SPEC_INCL'].nunique()\n",
    "invalid_specialties = specialty_counts[specialty_counts > 1].index\n",
    "# Display the invalid specialties\n",
    "print(\"Invalid Specialties:\",invalid_specialties)\n",
    "qc_df_print = unique_spec_cds_incl[unique_spec_cds_incl['SpecialtyDescription'].isin(invalid_specialties)].copy()\n",
    "qc_df_print.sort_values(by='SpecialtyDescription',inplace=True)\n",
    "qc_df_print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2097701-d3d7-4f16-b255-7ca3f3c3b677",
   "metadata": {},
   "source": [
    "#### REFERENCE DATE PARAMETERS DATA SETS CREATION  -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb1ef857-6439-4190-b708-d22277021fce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>CALENDAR</th>\n",
       "      <th>DATADATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XPONENT</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALLS</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENTS</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MASTER PROFILE</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATA    CALENDAR    DATADATE\n",
       "0         XPONENT  2024-08-05  2024-07-19\n",
       "1           CALLS  2024-08-05  2024-08-02\n",
       "2          EVENTS  2024-08-05  2024-08-02\n",
       "3  MASTER PROFILE  2024-08-05  2024-08-05"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'DATA': ['XPONENT', 'CALLS', 'EVENTS', 'MASTER PROFILE'],\n",
    "    'CALENDAR': [DATA_DATE_CALENDAR] * 4,\n",
    "    'DATADATE': [DATA_DATE_WEEKLY,EVTS_DATE_WEEKLY,EVTS_DATE_WEEKLY,DATA_DATE_CALENDAR]\n",
    "}\n",
    "\n",
    "ref_week_DATE_PARM = pd.DataFrame(data)\n",
    "ref_week_DATE_PARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fec62394-a950-4ed8-848f-9d1a294b758f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATA</th>\n",
       "      <th>CALENDAR</th>\n",
       "      <th>DATADATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XPONENT</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CALLS</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENTS</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MASTER PROFILE</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DATA    CALENDAR    DATADATE\n",
       "0         XPONENT  2024-08-05  2024-07-19\n",
       "1           CALLS  2024-08-05  2024-07-19\n",
       "2          EVENTS  2024-08-05  2024-07-19\n",
       "3  MASTER PROFILE  2024-08-05  2024-08-05"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'DATA': ['XPONENT', 'CALLS', 'EVENTS', 'MASTER PROFILE'],\n",
    "    'CALENDAR': [DATA_DATE_CALENDAR] * 4,\n",
    "    'DATADATE': [DATA_DATE_MONTHLY,DATA_DATE_MONTHLY,DATA_DATE_MONTHLY,DATA_DATE_CALENDAR]\n",
    "}\n",
    "\n",
    "ref_mnth_DATE_PARM = pd.DataFrame(data)\n",
    "ref_mnth_DATE_PARM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bda311a-c8f0-4f5c-8cb4-184300b064d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Exporting Date Param\n",
    "ref_week_DATE_PARM.to_parquet(f's3://{bucket}/{ref_week}DATE_PARM.parquet')\n",
    "ref_mnth_DATE_PARM.to_parquet(f's3://{bucket}/{ref_month}DATE_PARM.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc6c2097-1961-4d33-a5dc-619c2e7a5ddc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DELETEING non used Dfs?\n",
    "del temp_CUSTOMER_MASTER_CUR_PROC_WK\n",
    "del ref_week_MASTER_PROFILE\n",
    "del merged_df\n",
    "del temp_CUSTOMER_MASTER_PRE_PROC_WK\n",
    "del temp_ADDRESS1_F\n",
    "del temp_ADDRESS1\n",
    "del temp_test1\n",
    "del temp_ADDRESS\n",
    "del curwk_ADDRESS\n",
    "del temp_b\n",
    "del curwk_CUSTOMER_MASTER\n",
    "del updates_HCP_UPDATES_CUR_PROC_WK\n",
    "del temp_ADDS_UPDATES\n",
    "del temp_HCP_UPDATES_CUR_PROC_WK\n",
    "del temp_ADDRESSFLAG1\n",
    "del curwk_COMMUNICATION\n",
    "del temp_COMMUNICATION1\n",
    "del temp_CUSTOMER1\n",
    "del temp_CUSTOMER\n",
    "del curwk_CUSTOMER\n",
    "del curwk_DEA\n",
    "del temp_Addr_all\n",
    "del temp_DEA1\n",
    "del temp_ADDRESSNODUP\n",
    "del temp_test4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cda2b-9f13-414e-9c4f-349c5ef84c6e",
   "metadata": {},
   "source": [
    "### Net New HCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4be4011b-f2a2-4905-be6f-182adab2c89f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Current Week Full Master Profile - MASTER_PROFILE (Already Loaded in memory)\n",
    "CUR_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_week}MASTER_PROFILE.parquet')\n",
    "# Previous Week Master Profile\n",
    "PRE_PROFILE = pd.read_parquet(f's3://{bucket}/{ref_pwek}MASTER_PROFILE.parquet')\n",
    "\n",
    "# Perform the merge using pd.merge\n",
    "NET_NEW = pd.merge(CUR_PROFILE, PRE_PROFILE['IID'],on='IID', how='left', indicator=True)\n",
    "NET_NEW = NET_NEW[NET_NEW['_merge'] == 'left_only']\n",
    "NET_NEW = NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)\n",
    "\n",
    "#adding week timestamp \n",
    "NET_NEW['SOURCE'] = CUR_WK  #using cur_wk_xpn_date but in code  -\"&CUR_WK.\" is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f1325-3509-4600-b3fe-86499933f2e9",
   "metadata": {},
   "source": [
    "### Appending net new to create rolling file\n",
    "- the week after frozen week , the net new file is reset\n",
    "- otherwise, net new files gets accumilated by using previous week net new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "490850d8-0fe1-4c31-b66a-9c448f9bd095",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#if WEEK1 == 1 then dont append to previous week net new , else append.\n",
    "WEEK1 = 0 #DONT CHANGE THIS  , You have to manually reset net net for a new Qtr in a separate code , reuse the code there\n",
    "NET_NEW_ACCU = pd.DataFrame()\n",
    "\n",
    "net_new_varname = \"NET_NEW_{}\".format(QTR_NET)\n",
    "\n",
    "if WEEK1 == 1:\n",
    "    NET_NEW_ACCU = NET_NEW.copy()\n",
    "elif WEEK1 == 0:\n",
    "    globals()[net_new_varname] = pd.read_parquet(f's3://{bucket}/{ref_pwek}{net_new_varname}.parquet')\n",
    "    columns_to_drop = ['Territory_IW1', 'Territory_Name_IW1','Region', \n",
    "                   'Region_Name', 'Area','Area_Name','SPECIALTY_DESCRIPTION',\n",
    "                   'SPEC_INCL_LIN','IC_INCL_LIN'] \n",
    "    globals()[net_new_varname] = globals()[net_new_varname].drop(columns=columns_to_drop)\n",
    "    NET_NEW_ACCU = pd.concat([NET_NEW, globals()[net_new_varname]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbee48-4bb7-462f-8c33-5e79291ae73b",
   "metadata": {},
   "source": [
    "#### Assigning Terr information from Zip to Terr\n",
    "- Addtionally, using the quarterly Speciality Inclusion Exclusion list to get inclusion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075f8b69-6025-4986-a2e1-2d14f8f37b6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ZIP_TO_TERR = pd.read_parquet(f's3://{bucket}/{ZIP}zip_to_terr.parquet')\n",
    "ZIP_TO_TERR.rename(columns={'Territory':'Territory_IW1','Territory_Name':'Territory_Name_IW1'},inplace=True) #DOUBT not sure why we do this\n",
    "SPEC_INCL_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}qtrspec_SPEC_INCL_LIN.parquet')\n",
    "#Extra\n",
    "SPEC_INCL_LIN.rename(columns={'SPECIALTY_CD':'SpecialtyCode'},inplace=True)\n",
    "SPEC_INCL_LIN.set_index('SpecialtyCode', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4da13148-887e-43ac-b66b-b8b16e610b93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#getting terr name, region name, area name from zip to terr\n",
    "NET_NEW_FN = pd.merge(NET_NEW_ACCU,ZIP_TO_TERR,left_on='ZIP',right_on='Zip',how='left')\n",
    "whitespace_values = {\n",
    "    'Territory_IW1': \"1111-99999-99\",\n",
    "    'Territory_Name_IW1': \"\",\n",
    "    'Region': \"1111-999\",\n",
    "    'Region_Name': \"\",\n",
    "    'Area': \"1111-9\",\n",
    "    'Area_Name': \"\"\n",
    "}\n",
    "# Fill whitespace_values with the default values\n",
    "NET_NEW_FN.fillna(whitespace_values, inplace=True)\n",
    "\n",
    "#getting speciality flags from spec incl lin quaterly file -[NOTE : BUG FIX SUMMARY - sas did not perform a left join, revised code to perfrom mapping strategy] \n",
    "NET_NEW_FN['SPECIALTY_DESCRIPTION'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPECIALTY_DESCRIPTION'])\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = NET_NEW_FN['SpecialtyCode'].map(SPEC_INCL_LIN['SPEC_INCL'])\n",
    "NET_NEW_FN.fillna({'SPECIALTY_DESCRIPTION':''},inplace=True)\n",
    "\n",
    "#adding new column SPEC_INCL_LIN (this is based on match code, so ill have to wait for it to come.- NOTE  Condition is only applied on failed left matches\n",
    "#NET_NEW_FN['SPEC_INCL_LIN'] = ['N' if x == '01' else 'Y' for x in NET_NEW_FN['MatchCode']] - This is wrong\n",
    "NET_NEW_FN['SPEC_INCL_LIN'] = np.where(NET_NEW_FN['SPEC_INCL_LIN'].isna(),\n",
    "                                        np.where(NET_NEW_FN['MatchCode'] == '01', 'N', 'Y'),\n",
    "                                        NET_NEW_FN['SPEC_INCL_LIN'])\n",
    "\n",
    "#dropping extra columns (different name , same data)\n",
    "NET_NEW_FN.drop(['Zip'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198694e-b48a-4ebb-b4ce-32e015a3d847",
   "metadata": {},
   "source": [
    "### Filtering for any legal removal and unknown address HCPs\n",
    "- 'Jami' file is used here (double check import location and check for file updates on quarter change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3329662e-3c26-4f48-b174-24ed14c8d3e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# applying exclusions\n",
    "JAMI_INCLEXCL = pd.read_parquet(f's3://{bucket}/{jami}jami_inclexcl.parquet')\n",
    "jami_exclusions = pd.DataFrame()\n",
    "jami_exclusions = JAMI_INCLEXCL[JAMI_INCLEXCL['TYPE'].isin(['Legal Removals', 'Unknown Address'])]\n",
    "jami_exclusions = jami_exclusions[['IID']].copy()\n",
    "jami_exclusions.drop_duplicates(subset='IID', inplace=True)\n",
    "jami_exclusions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ref_week_NET_NEW = pd.merge(NET_NEW_FN,jami_exclusions,on = 'IID',how='left',indicator=True)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[ref_week_NET_NEW['_merge'] == 'left_only']\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.drop(['_merge'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bab82-b747-496f-beac-59527f08776a",
   "metadata": {},
   "source": [
    "#### IC flag creation\n",
    "- IC_INCL_LIN is a an important flag which decides if a hcp should be considered for IC or not\n",
    "- CustomerEffectiveStartDate , SPEC_INCL_LIN, CustomerStatusCode, MatchCode decide its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b5cbb81-20ff-4057-8f4b-76644bf9fe05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19824, 82)\n"
     ]
    }
   ],
   "source": [
    "ref_week_NET_NEW['IC_INCL_LIN'] = 0  # Initialize the column with 0\n",
    "\n",
    "condition = (\n",
    "    (ref_week_NET_NEW['CustomerEffectiveStartDate'] < CUT_OFF_net) &\n",
    "    (ref_week_NET_NEW['SPEC_INCL_LIN'] == \"Y\") &\n",
    "    (ref_week_NET_NEW['CustomerStatusCode'] == \"Active\") &\n",
    "    (ref_week_NET_NEW['MatchCode'] != '01')\n",
    ")\n",
    "\n",
    "ref_week_NET_NEW.loc[condition, 'IC_INCL_LIN'] = 1\n",
    "print(ref_week_NET_NEW.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0f319-f684-4bdb-8fa9-9c1c381d1bd3",
   "metadata": {},
   "source": [
    "#### Importing IIDs from frozen IC universe , and deleting any occurances of those in net new as an added QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b019038-d6a8-4ae1-acd0-a4f0e87d4806",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped from ic uni:  0\n"
     ]
    }
   ],
   "source": [
    "CUSTOMER_MASTER_IC_LIN = pd.read_parquet(f's3://{bucket}/{qtrspec}CUSTOMER_MASTER_IC_LIN.parquet')\n",
    "CUSTOMER_MASTER_IC_LIN = pd.DataFrame(CUSTOMER_MASTER_IC_LIN['IID'])\n",
    "#Faulty Approach - needs checking into : Try using - ref_week_NET_NEW = merged_df[merged_df['_merge'] == 'left_only']\n",
    "# incase of memory issues ?\n",
    "# merged_df = pd.merge(ref_week_NET_NEW, CUSTOMER_MASTER_IC_LIN[['IID']], on='IID', how='inner')\n",
    "# ref_week_NET_NEW = ref_week_NET_NEW.drop(merged_df.index)\n",
    "#updated approach - \n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW = ref_week_NET_NEW[~ref_week_NET_NEW['IID'].isin(CUSTOMER_MASTER_IC_LIN['IID'])]\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped from ic uni: \",qc_bef-qc_af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63813f12-3928-42c9-accb-2f3e65a89b92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records Dropped :  0\n"
     ]
    }
   ],
   "source": [
    "#Only Keep the first Instance of IID Source.\n",
    "qc_bef = len(ref_week_NET_NEW)\n",
    "ref_week_NET_NEW.sort_values(by=['IID', 'SOURCE'], inplace=True)\n",
    "ref_week_NET_NEW.drop_duplicates(subset='IID', keep='first', inplace=True)\n",
    "qc_af = len(ref_week_NET_NEW)\n",
    "print(\"Records Dropped : \",qc_bef-qc_af)\n",
    "\n",
    "ref_week_NET_NEW = ref_week_NET_NEW.sort_values(['IID'])\n",
    "ref_week_NET_NEW.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#sdap here\n",
    "ref_week_NET_NEW.to_parquet(f's3://{bucket}/{ref_week}{net_new_varname}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e223e3b-7a8a-470c-b247-38b1615ac869",
   "metadata": {},
   "source": [
    "#### QC | Frequency Table to QC number of records , OC flags , date of addition etc\n",
    "- Carry over from sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69c5fa8d-7ff1-48f7-a7d9-5de4ba1b2250",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SOURCE  COUNT\n",
      "8   20240503   1503\n",
      "6   20240510   1523\n",
      "2   20240517   1650\n",
      "3   20240524   1626\n",
      "1   20240531   1895\n",
      "10  20240607   1330\n",
      "4   20240614   1561\n",
      "7   20240621   1519\n",
      "9   20240628   1385\n",
      "11  20240705   1313\n",
      "0   20240712   2965\n",
      "5   20240719   1554\n"
     ]
    }
   ],
   "source": [
    "#17.1: Check current week's net_new source\n",
    "# Perform the frequency count\n",
    "freq_counts = ref_week_NET_NEW['SOURCE'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['SOURCE', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('SOURCE')\n",
    "\n",
    "# Print the frequency counts\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab9f8092-623a-4d41-b2e6-adc297d977ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IC_INCL_LIN  COUNT\n",
      "0            1  13248\n",
      "1            0   6576\n"
     ]
    }
   ],
   "source": [
    "#17.2: Check IC_INCL_LIN - this wont work yet because flag is not there (match code not avilable)\n",
    "freq_counts = ref_week_NET_NEW['IC_INCL_LIN'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "freq_counts.columns = ['IC_INCL_LIN', 'COUNT']\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "freq_counts = freq_counts.sort_values('COUNT', ascending=False)\n",
    "\n",
    "# Print the frequency counts #NOTE : last check on 27-12-2023 , 16 people with IC flag 0 are lesser in python compared to sas presumably Jami ppl\n",
    "print(freq_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff87fc41-674b-4b44-adb7-6d8f80b565db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcp_count 19824\n",
      "distinct_hcp_count 19824\n"
     ]
    }
   ],
   "source": [
    "# 17.3: Check Net New for Dups\n",
    "print(\"hcp_count\",ref_week_NET_NEW['IID'].count())\n",
    "print(\"distinct_hcp_count\",ref_week_NET_NEW['IID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "176c1f15-af1d-417f-89d0-3bb61ecc4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO/\n",
    "# Check for Net New NOBS COUNT - DONE\n",
    "# Impliment SAS method to only keep Oldest Record - DONE\n",
    "# Apply QC that checks against frozen MP - DONE\n",
    "# Export Dataframe - DONE\n",
    "# Remove Redundant Cells and Time - DONE\n",
    "# check why speciality rank has 2 columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
